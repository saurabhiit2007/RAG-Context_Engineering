### 1. Overview

BM25 (Best Matching 25) is a probabilistic ranking function used in information retrieval to score and rank documents given a query. It can be seen as a principled improvement over TF IDF that better handles term frequency saturation and document length normalization. BM25 is widely used in search engines and remains a strong lexical baseline in modern retrieval systems.

---

### 2. Intuition Behind BM25

TF-IDF assumes that term importance grows linearly with frequency and applies only coarse normalization for document length. In practice, these assumptions are often suboptimal.

BM25 improves on this by:
- Saturating the contribution of repeated terms
- Explicitly normalizing for document length
- Grounding the scoring function in probabilistic retrieval theory

The result is a ranking function that aligns better with human relevance judgments.

---

### 3. BM25 Scoring Formula

The BM25 score for a document *d* and query *q* is:

$$
\text{BM25}(d, q) =
\sum_{t \in q}
\text{IDF}(t)
\cdot
\frac{
    tf(t, d) \cdot (k_1 + 1)
}{
    tf(t, d)
    + k_1 \cdot
    \left(
        1 - b + b \cdot \frac{|d|}{\text{avgdl}}
    \right)
}
$$

Where:

- $t$ is a query term  
- $tf(t, d)$ is the term frequency of $t$ in document $d$  
- $|d|$ is the length of document $d$  
- $\text{avgdl}$ is the average document length in the corpus  
- $k_1$ controls term frequency saturation  
- $b$ controls document length normalization  
- $\text{IDF}(t)$ is the inverse document frequency of term $t$

---

### 3.1 Key Components Explained

#### Term Frequency Saturation

Unlike TF IDF, BM25 assumes diminishing returns for repeated terms. The first few occurrences of a term matter most, while additional repetitions contribute progressively less to the score.

The parameter $k_1$ controls how quickly this saturation happens.

---

#### Document Length Normalization

Longer documents naturally contain more terms and higher raw term counts. BM25 explicitly normalizes scores based on document length to avoid favoring longer documents unfairly.

The parameter *b* controls how strongly document length affects the score:
- b = 0 disables length normalization
- b = 1 applies full normalization

---

#### Inverse Document Frequency

BM25 uses a slightly modified IDF formulation that is more robust for rare and common terms:

$$IDF(t) = log((N âˆ’ df(t) + 0.5) / (df(t) + 0.5))$$

This formulation stabilizes scores and improves ranking quality.

---

### 4. Where BM25 is Used

BM25 is commonly used in:

- Search engines
- Document retrieval systems
- Question answering pipelines
- First stage retrievers in RAG systems
- Enterprise and web scale search applications

It is often the default choice when dense embeddings are not available or too expensive.

---

### 5. Strengths of BM25

- Strong lexical matching performance
- Better handling of term frequency than TF IDF
- Explicit document length normalization
- No training required
- Robust across different domains

BM25 often outperforms TF IDF with minimal tuning.

---

### 6. Limitations and Caveats

- Cannot capture semantic similarity
- Still relies on exact term overlap
- Sensitive to tokenization and preprocessing
- Query term independence assumption
- Requires tuning of k1 and b for optimal performance

BM25 remains a lexical method and inherits the fundamental limitations of bag of words models.

---

### 7. Practical Considerations

- Typical values: k1 in [1.2, 2.0], b around 0.75
- Stopword handling significantly affects results
- Works best with cosine or BM25 specific scoring, not raw dot products
- Often paired with inverted indexes for efficient retrieval

BM25 is computationally efficient and scales well to large corpora.

---

### 8. BM25 vs TF IDF

| Aspect | TF IDF | BM25 |
|------|-------|------|
| Term frequency | Linear | Saturated |
| Length normalization | Weak | Explicit |
| Tuning parameters | None | k1, b |
| Ranking quality | Good | Better |
| Complexity | Low | Moderate |

BM25 can be seen as a more refined and robust version of TF IDF.

### 8.1 An Example

Consider a small corpus of three documents and a simple query.

#### Corpus

- **D1**: "deep learning deep learning deep learning tutorial"
- **D2**: "deep learning tutorial"
- **D3**: "deep learning introduction overview"

#### Query : deep learning tutorial

#### How TF IDF Scores These Documents

    Key Observation

    TF IDF increases linearly with term frequency. Repeating the same term multiple times keeps increasing the score.

    ### Approximate Behavior

    - D1
        - Very high TF for "deep" and "learning"
        - High TF IDF score due to repetition
    - D2
        - Moderate TF for all query terms
        - Lower score than D1
    - D3
        - Missing "tutorial"
        - Lowest score

    TF IDF Ranking: D1 > D2 > D3

    Issue
    D1 is ranked highest mainly because it repeats "deep learning" many times, even though it is not necessarily more relevant to the query than D2.

    TF IDF assumes that more repetitions always mean higher relevance.


#### How BM25 Scores These Documents

    Key Observation

    BM25 applies **term frequency saturation** and **length normalization**.

    - The first few occurrences of a term matter most
    - Extra repetitions contribute less
    - Longer documents are penalized

    Approximate Behavior

    - D1
        - TF contribution saturates quickly
        - Longer document length reduces score
    - D2
        - Balanced term coverage
        - Shorter document length boosts score
    - D3
        - Still missing "tutorial"
        - Lowest score

    BM25 Ranking: D2 > D1 > D3


---

### 8.2 Why BM25 Is Better Than TF IDF in This Example

#### 8.2.1 Term Frequency Saturation

TF IDF: Repeats always increase score

BM25: Repeats beyond a point add little value

This prevents keyword stuffing from dominating rankings.

#### 8.2.2 Document Length Normalization

TF IDF: Weak or implicit normalization

BM25: Explicitly penalizes longer documents

This avoids favoring verbose documents.

#### 8.2.3 Ranking Quality

BM25 aligns better with human intuition:

- D2 is more concise and directly matches the query
- D1 is repetitive but not necessarily more informative

> Note: BM25 improves over TF IDF by introducing realistic assumptions about term importance. It limits the impact of repeated terms and corrects for document length bias, leading to more reliable and intuitive document rankings in retrieval systems.


---

### 9. BM25 in Modern Retrieval Systems

BM25 is frequently used as:

- A fast first stage retriever
- A baseline for evaluating dense retrievers
- A component in hybrid retrieval systems
- A fallback when embeddings fail

Many state of the art systems combine BM25 with dense retrieval to balance recall and precision.

---

### 10. Interview Perspective

For interviews, emphasize:

- Why BM25 improves over TF IDF
- The role of term frequency saturation
- The importance of document length normalization
- Typical parameter choices and their effects
- How BM25 fits into RAG and hybrid retrieval pipelines

This demonstrates both theoretical understanding and practical retrieval intuition.

---