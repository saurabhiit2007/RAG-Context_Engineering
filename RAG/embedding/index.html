
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference Retrieval Augmented Generation And Context Engineering.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../chunking/">
      
      
        <link rel="next" href="../retrieval_methods/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Embedding - RAG & Context Engineering</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="RAG &amp; Context Engineering" class="md-header__button md-logo" aria-label="RAG & Context Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            RAG & Context Engineering
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Embedding
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/RAG_And_Context_Engineering" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="RAG &amp; Context Engineering" class="md-nav__button md-logo" aria-label="RAG & Context Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    RAG & Context Engineering
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/RAG_And_Context_Engineering" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Retrival Augmented Generation (RAG)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Retrival Augmented Generation (RAG)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chunking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chunking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Embedding
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-dense-sparse-and-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Dense, Sparse, and Hybrid Embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Dense, Sparse, and Hybrid Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dense-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dense Embeddings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sparse Embeddings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid Embeddings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-bi-encoders-vs-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Bi-Encoders vs. Cross-Encoders
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Bi-Encoders vs. Cross-Encoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bi-encoders-dual-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bi-Encoders (Dual Encoders)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cross-Encoders
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#late-interaction-colbert" class="md-nav__link">
    <span class="md-ellipsis">
      
        Late Interaction — ColBERT
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-embedding-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Embedding Training Objectives
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Embedding Training Objectives">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-learning-infonce-mnr-loss" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contrastive Learning (InfoNCE / MNR Loss)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-retrieval-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Supervised Retrieval Fine-tuning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matryoshka-representation-learning-mrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Matryoshka Representation Learning (MRL)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instruction-tuned-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Instruction-Tuned Embeddings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-domain-adaptation-the-cold-start-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Domain Adaptation (The "Cold Start" Problem)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-distance-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Distance Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-common-failure-modes" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Common Failure Modes
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-evaluation-of-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Evaluation of Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Interview Questions
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../retrieval_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Retrieval Methods
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../indexing_and_vector_database/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Indexing Strategies, Vector Databases, and Retrieval Systems for RAG
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Re-Ranking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../advanced_rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced RAG
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_reference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quick Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_10" >
        
          
          <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Supporting Concepts
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10">
            <span class="md-nav__icon md-icon"></span>
            
  
    Supporting Concepts
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/tf_idf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TF_IDF
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/bm25/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BM25
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/splade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Splade
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/sentence_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sentence Transformer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    References
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-dense-sparse-and-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Dense, Sparse, and Hybrid Embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Dense, Sparse, and Hybrid Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dense-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dense Embeddings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sparse Embeddings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid Embeddings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-bi-encoders-vs-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Bi-Encoders vs. Cross-Encoders
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Bi-Encoders vs. Cross-Encoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bi-encoders-dual-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bi-Encoders (Dual Encoders)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cross-Encoders
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#late-interaction-colbert" class="md-nav__link">
    <span class="md-ellipsis">
      
        Late Interaction — ColBERT
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-embedding-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Embedding Training Objectives
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Embedding Training Objectives">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-learning-infonce-mnr-loss" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contrastive Learning (InfoNCE / MNR Loss)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-retrieval-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Supervised Retrieval Fine-tuning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matryoshka-representation-learning-mrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Matryoshka Representation Learning (MRL)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instruction-tuned-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Instruction-Tuned Embeddings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-domain-adaptation-the-cold-start-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Domain Adaptation (The "Cold Start" Problem)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-distance-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Distance Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-common-failure-modes" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Common Failure Modes
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-evaluation-of-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Evaluation of Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Interview Questions
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Embedding</h1>

<h2 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h2>
<p>Embeddings are fixed-length dense vectors that encode the semantic meaning of text. In RAG, embeddings are generated for both document chunks (at index time) and user queries (at retrieval time), and similarity between those vectors determines what gets retrieved.</p>
<hr />
<hr />
<h2 id="2-dense-sparse-and-hybrid-embeddings">2. Dense, Sparse, and Hybrid Embeddings<a class="headerlink" href="#2-dense-sparse-and-hybrid-embeddings" title="Permanent link">&para;</a></h2>
<h3 id="dense-embeddings">Dense Embeddings<a class="headerlink" href="#dense-embeddings" title="Permanent link">&para;</a></h3>
<p>Text is mapped into a continuous low-dimensional vector space (typically 256–4096 dimensions) using a neural encoder. Similar texts end up near each other regardless of exact word overlap.</p>
<ul>
<li><strong>Pros:</strong> Captures paraphrases and semantic equivalence; efficient ANN search; handles natural language queries well.</li>
<li><strong>Cons:</strong> Weak at exact keyword matching; sensitive to domain shift; less interpretable.</li>
<li><strong>Examples:</strong> Sentence-BERT, E5, GTE, OpenAI text-embedding models.</li>
</ul>
<hr />
<h3 id="sparse-embeddings">Sparse Embeddings<a class="headerlink" href="#sparse-embeddings" title="Permanent link">&para;</a></h3>
<p>Text is represented as a high-dimensional sparse vector over a vocabulary (most dimensions are zero). Non-zero weights correspond to terms that appear in the document or query.</p>
<ul>
<li><strong>Pros:</strong> Excellent exact-match recall for keywords, IDs, product codes; interpretable; robust for rare terms.</li>
<li><strong>Cons:</strong> No semantic generalisation; fails on paraphrases; vocabulary-dependent.</li>
<li><strong>Examples:</strong> TF-IDF, BM25, SPLADE.</li>
</ul>
<hr />
<h3 id="hybrid-embeddings">Hybrid Embeddings<a class="headerlink" href="#hybrid-embeddings" title="Permanent link">&para;</a></h3>
<p>Hybrid approaches combine both signals. Three common strategies:</p>
<ul>
<li><strong>Late fusion (RRF):</strong> Run dense and sparse search independently, then merge ranked lists using Reciprocal Rank Fusion. Called "late" because merging happens after both retrievals complete.</li>
<li><strong>Two-stage retrieval:</strong> Sparse search narrows millions of documents to a few hundred candidates; a dense model re-scores that shortlist. Balances efficiency and accuracy.</li>
<li><strong>Joint sparse-dense (SPLADE-style):</strong> A single model produces vectors containing both semantic signals and lexical importance weights — one unified search.</li>
</ul>
<hr />
<hr />
<h2 id="3-bi-encoders-vs-cross-encoders">3. Bi-Encoders vs. Cross-Encoders<a class="headerlink" href="#3-bi-encoders-vs-cross-encoders" title="Permanent link">&para;</a></h2>
<p>This is the most important architectural distinction in embedding-based retrieval and a near-universal interview topic.</p>
<h3 id="bi-encoders-dual-encoders">Bi-Encoders (Dual Encoders)<a class="headerlink" href="#bi-encoders-dual-encoders" title="Permanent link">&para;</a></h3>
<p>Query and document are encoded <strong>independently</strong> into vectors. Similarity is a dot product or cosine distance. Because document embeddings are pre-computed and stored, retrieval is O(log N) with ANN indexes.</p>
<div class="highlight"><pre><span></span><code>s(q, d) = ⟨f(q), g(d)⟩
</code></pre></div>
<ul>
<li><strong>Strengths:</strong> Extremely fast at scale; indexable; supports millions of documents.</li>
<li><strong>Weakness:</strong> No cross-attention between query and document — limited relevance modelling.</li>
<li><strong>Used for:</strong> First-stage retrieval (recall-optimised).</li>
</ul>
<hr />
<h3 id="cross-encoders">Cross-Encoders<a class="headerlink" href="#cross-encoders" title="Permanent link">&para;</a></h3>
<p>Query and document are <strong>concatenated</strong> and passed through a transformer together. Full self-attention across both texts allows every query token to attend to every document token.</p>
<div class="highlight"><pre><span></span><code>s(q, d) = h([q ; d])
</code></pre></div>
<ul>
<li><strong>Strengths:</strong> Rich token-level interactions; substantially more accurate relevance scoring.</li>
<li><strong>Weakness:</strong> Cannot be pre-computed or indexed — one full forward pass per query-document pair. Too slow for large corpora.</li>
<li><strong>Used for:</strong> Second-stage reranking over a small candidate set (precision-optimised).</li>
</ul>
<blockquote>
<p><strong>Bi-encoders maximise recall at scale. Cross-encoders maximise precision on a shortlist. Production systems use both in sequence.</strong></p>
</blockquote>
<hr />
<h3 id="late-interaction-colbert">Late Interaction — ColBERT<a class="headerlink" href="#late-interaction-colbert" title="Permanent link">&para;</a></h3>
<p>A middle ground between bi- and cross-encoders:</p>
<ul>
<li>Stores a vector per <strong>token</strong> in each document (more memory than bi-encoders).</li>
<li>Uses a <strong>MaxSim</strong> operation at retrieval: the score for a query token is the maximum similarity across all document token vectors.</li>
<li>Achieves near cross-encoder accuracy at bi-encoder speed.</li>
</ul>
<hr />
<hr />
<h2 id="4-embedding-training-objectives">4. Embedding Training Objectives<a class="headerlink" href="#4-embedding-training-objectives" title="Permanent link">&para;</a></h2>
<h3 id="contrastive-learning-infonce-mnr-loss">Contrastive Learning (InfoNCE / MNR Loss)<a class="headerlink" href="#contrastive-learning-infonce-mnr-loss" title="Permanent link">&para;</a></h3>
<p>Positive pairs (query + relevant document) are pulled closer in vector space; negative examples (other documents in the batch) are pushed apart.</p>
<p><strong>InfoNCE Loss:</strong></p>
<div class="highlight"><pre><span></span><code>L = -log [ exp(sim(q, d+)) / (exp(sim(q, d+)) + Σ exp(sim(q, d-))) ]
</code></pre></div>
<ul>
<li><strong>Temperature τ:</strong> Scales similarity scores before softmax. Low τ → model focuses heavily on hardest negatives. High τ → smooths the distribution.</li>
</ul>
<p><strong>Multiple Negatives Ranking (MNR) Loss:</strong> The standard in Sentence-Transformers. In a batch of K pairs, each document serves as a negative for all other queries in the batch — providing K–1 negatives per query "for free" without manual labelling.</p>
<hr />
<h3 id="supervised-retrieval-fine-tuning">Supervised Retrieval Fine-tuning<a class="headerlink" href="#supervised-retrieval-fine-tuning" title="Permanent link">&para;</a></h3>
<p>Models are fine-tuned on human-labelled query-document relevance datasets.</p>
<ul>
<li><strong>MS MARCO:</strong> The gold standard — real Bing queries paired with human-judged relevant passages. Most production embedding models (BGE, GTE, E5) are trained on MS MARCO.</li>
<li><strong>Pairwise loss:</strong> Model is given (q, d+, d-) and penalised if d- scores higher than d+.</li>
<li><strong>Listwise loss:</strong> Model optimises the entire ranked list at once. More accurate for ranking (directly optimises nDCG) but more expensive to train.</li>
</ul>
<hr />
<h3 id="matryoshka-representation-learning-mrl">Matryoshka Representation Learning (MRL)<a class="headerlink" href="#matryoshka-representation-learning-mrl" title="Permanent link">&para;</a></h3>
<p>Training embeds information hierarchically so the first N dimensions contain the most important features. This enables <strong>vector truncation</strong> — you can store 1536-dimensional vectors but query only the first 256 dimensions to save storage and compute with minimal accuracy loss.</p>
<hr />
<h3 id="instruction-tuned-embeddings">Instruction-Tuned Embeddings<a class="headerlink" href="#instruction-tuned-embeddings" title="Permanent link">&para;</a></h3>
<p>Models like Instructor and BGE accept a natural-language task prefix:</p>
<blockquote>
<p>"Represent this query for retrieving legal documents"</p>
</blockquote>
<p>This lets a single model adapt its embedding space to different tasks — retrieval vs. clustering vs. classification — without separate models.</p>
<hr />
<hr />
<h2 id="5-domain-adaptation-the-cold-start-problem">5. Domain Adaptation (The "Cold Start" Problem)<a class="headerlink" href="#5-domain-adaptation-the-cold-start-problem" title="Permanent link">&para;</a></h2>
<p>Generic embeddings underperform on specialist domains (medical, legal, code). Adaptation options in increasing cost order:</p>
<ol>
<li><strong>Continued pre-training:</strong> Run Masked Language Modeling (MLM) on your private corpus to teach the model domain vocabulary.</li>
<li><strong>Contrastive fine-tuning:</strong> Use domain-specific query-document pairs with contrastive loss to pull relevant items closer.</li>
<li><strong>Generative Pseudo-Labeling (GPL):</strong> Use an LLM to generate synthetic questions for each unlabeled document, then train the embedding model on these synthetic pairs. Effective with zero labeled data.</li>
<li><strong>Adapter-based tuning:</strong> Insert lightweight adapter layers and fine-tune only those, leaving base model weights frozen.</li>
</ol>
<blockquote>
<p><strong>Critical:</strong> If you change the embedding model, you must re-index the entire corpus. Vectors from different models are not comparable.</p>
</blockquote>
<hr />
<hr />
<h2 id="6-distance-metrics">6. Distance Metrics<a class="headerlink" href="#6-distance-metrics" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>What It Measures</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cosine similarity</td>
<td>Angle between vectors (ignores magnitude)</td>
<td>Most common for text; equivalent to dot product on L2-normalised vectors</td>
</tr>
<tr>
<td>Dot product</td>
<td>Projection of one vector onto another</td>
<td>Faster; rewards both direction and magnitude</td>
</tr>
<tr>
<td>Euclidean distance</td>
<td>Straight-line distance in vector space</td>
<td>Sensitive to vector magnitude; less common for text</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> Cosine similarity with L2-normalised vectors is equivalent to dot product. L2 normalisation stabilises similarity scores and improves ANN search behaviour by projecting all vectors onto a unit hypersphere.</p>
<hr />
<hr />
<h2 id="7-common-failure-modes">7. Common Failure Modes<a class="headerlink" href="#7-common-failure-modes" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Failure Mode</th>
<th>Root Cause</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Semantic drift</td>
<td>Retrieved chunks are topically related but not relevant</td>
<td>Add cross-encoder reranker</td>
</tr>
<tr>
<td>Out-of-vocabulary</td>
<td>Search for product IDs or rare part numbers fails</td>
<td>Add hybrid search (BM25 or SPLADE)</td>
</tr>
<tr>
<td>Intent mismatch</td>
<td>Procedural query retrieves descriptive content</td>
<td>Use HyDE (hypothetical document embeddings)</td>
</tr>
<tr>
<td>Domain mismatch</td>
<td>Generic model underperforms on specialist text</td>
<td>Fine-tune or domain-adapt the embedding model</td>
</tr>
<tr>
<td>Lost in the middle</td>
<td>LLM ignores context deep in the prompt</td>
<td>Parent-Document Retrieval; reorder chunks</td>
</tr>
</tbody>
</table>
<hr />
<hr />
<h2 id="8-evaluation-of-embeddings">8. Evaluation of Embeddings<a class="headerlink" href="#8-evaluation-of-embeddings" title="Permanent link">&para;</a></h2>
<p><strong>Offline (retrieval-level):</strong></p>
<ul>
<li>Recall@k — is the relevant document in the top k?</li>
<li>MRR — how early does the first relevant document appear?</li>
<li>nDCG — quality of the full ranked list using graded relevance.</li>
</ul>
<p><strong>End-to-end:</strong></p>
<ul>
<li>Answer correctness, faithfulness, latency, and cost.</li>
</ul>
<blockquote>
<p>Better retrieval does not always lead to better generation without proper prompting and context selection.</p>
</blockquote>
<hr />
<hr />
<h2 id="9-interview-questions">9. Interview Questions<a class="headerlink" href="#9-interview-questions" title="Permanent link">&para;</a></h2>
<p><strong>Q: Why can't you use an LLM directly for retrieval over millions of documents?</strong></p>
<p>A: Two reasons: (1) LLMs have a fixed context window and cannot "read" millions of documents in one pass; (2) attention is O(N²) in sequence length, making it computationally infeasible. Embeddings provide O(log N) search via ANN indexes.</p>
<hr />
<p><strong>Q: What is HyDE and when would you use it?</strong></p>
<p>A: Hypothetical Document Embeddings — use an LLM to generate a "fake" ideal answer to the query, embed that answer, and retrieve real documents similar to it. This bridges the vocabulary gap between a short query and longer document text. Use it when standard retrieval misses relevant documents because the query uses different wording than the documents. Risk: if the LLM hallucinates, the hallucinated text is embedded and may retrieve wrong documents.</p>
<hr />
<p><strong>Q: When do dense embeddings outperform BM25, and when does BM25 win?</strong></p>
<p>A: Dense embeddings win on semantic search, paraphrase matching, and natural language queries where exact document wording differs from the query. BM25 wins when queries contain rare terms, product codes, identifiers, or exact strings where lexical matching is what matters. Hybrid systems combine both to cover both failure modes.</p>
<hr />
<p><strong>Q: Does increasing embedding dimensionality always improve performance?</strong></p>
<p>A: No. Higher dimensions increase memory, index size, and ANN search latency. Beyond a point, the "curse of dimensionality" makes distance metrics less meaningful. MRL allows trading off dimensionality versus accuracy dynamically at query time.</p>
<hr />
<p><strong>Q: When should you re-index your vector database?</strong></p>
<p>A: Whenever you change the embedding model — you cannot compare vectors generated by Model A with those from Model B. Also re-index when the corpus changes significantly, when chunk size or preprocessing changes, or when switching to an index type that requires a full rebuild (like IVF).</p>
<hr />
<p><strong>Q: How do bi-encoders and cross-encoders differ in how they handle relevance?</strong></p>
<p>A: Bi-encoders encode query and document independently so they cannot model interaction between the two at encoding time — they rely on global semantic similarity. Cross-encoders concatenate query and document and process them jointly, so every token in the query can attend to every token in the document. This produces much richer relevance signals but cannot be pre-computed, making it unsuitable for large-scale first-stage retrieval.</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>