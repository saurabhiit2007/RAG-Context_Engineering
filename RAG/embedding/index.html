
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference Retrieval Augmented Generation And Context Engineering.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../chunking/">
      
      
        <link rel="next" href="../indexing_and_vector_database/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Embedding - RAG & Context Engineering</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-what-are-embeddings-in-rag" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="RAG &amp; Context Engineering" class="md-header__button md-logo" aria-label="RAG & Context Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            RAG & Context Engineering
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Embedding
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/RAG_And_Context_Engineering" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="RAG &amp; Context Engineering" class="md-nav__button md-logo" aria-label="RAG & Context Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    RAG & Context Engineering
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/RAG_And_Context_Engineering" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Retrival Augmented Generation (RAG)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Retrival Augmented Generation (RAG)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chunking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chunking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Embedding
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-are-embeddings-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. What Are Embeddings in RAG
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-dense-vs-sparse-vs-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Dense vs Sparse vs Hybrid Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-dense-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Dense Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-sparse-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Sparse Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Hybrid Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-sentence-document-and-chunk-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Sentence, Document, and Chunk Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-sentence-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Sentence Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-document-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Document Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-chunk-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Chunk Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-bi-encoders-vs-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Bi Encoders vs Cross Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#41-bi-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 Bi Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#42-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 Cross Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#43-late-interaction-eg-colbert" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Late Interaction (e.g., ColBERT)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-embedding-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Embedding Training Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-contrastive-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Contrastive Learning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-supervised-retrieval-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Supervised Retrieval Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#53-in-batch-negatives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3 In Batch Negatives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#54-instruction-tuning-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.4 Instruction Tuning for Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#55-matryoshka-representation-learning-mrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.5 Matryoshka Representation Learning (MRL)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#56-instruction-tuned-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.6 Instruction-Tuned Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-domain-adaptation-the-cold-start-problem-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Domain Adaptation (The "Cold Start" Problem) for Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-distance-metrics-and-vector-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Distance Metrics and Vector Normalization
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-multilingual-and-cross-lingual-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Multilingual and Cross Lingual Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-common-failure-modes-mitigations" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Common Failure Modes &amp; Mitigations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-evaluation-of-embeddings-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. Evaluation of Embeddings in RAG
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#offline-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Offline Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        End to End Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-high-frequency-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        11. High-Frequency Questions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-practical-design-pattern-the-gold-standard-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        12. Practical Design Pattern: The "Gold Standard" Pipeline
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../indexing_and_vector_database/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Indexing & Retrieval
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Supporting Concepts
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Supporting Concepts
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/tf_idf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    TF_IDF
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/bm25/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BM25
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/splade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Splade
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supporting_topics/sentence_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sentence Transformer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Context Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Context Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../context_engineering/ce/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Context Engineering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    References
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-are-embeddings-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. What Are Embeddings in RAG
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-dense-vs-sparse-vs-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Dense vs Sparse vs Hybrid Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-dense-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Dense Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-sparse-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Sparse Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-hybrid-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Hybrid Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-sentence-document-and-chunk-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Sentence, Document, and Chunk Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-sentence-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Sentence Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-document-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Document Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-chunk-level-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Chunk Level Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-bi-encoders-vs-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Bi Encoders vs Cross Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#41-bi-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 Bi Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#42-cross-encoders" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 Cross Encoders
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#43-late-interaction-eg-colbert" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Late Interaction (e.g., ColBERT)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-embedding-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Embedding Training Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#51-contrastive-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Contrastive Learning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-supervised-retrieval-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Supervised Retrieval Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#53-in-batch-negatives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3 In Batch Negatives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#54-instruction-tuning-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.4 Instruction Tuning for Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#55-matryoshka-representation-learning-mrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.5 Matryoshka Representation Learning (MRL)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#56-instruction-tuned-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.6 Instruction-Tuned Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-domain-adaptation-the-cold-start-problem-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Domain Adaptation (The "Cold Start" Problem) for Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-distance-metrics-and-vector-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Distance Metrics and Vector Normalization
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-multilingual-and-cross-lingual-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Multilingual and Cross Lingual Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-common-failure-modes-mitigations" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Common Failure Modes &amp; Mitigations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-evaluation-of-embeddings-in-rag" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. Evaluation of Embeddings in RAG
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#offline-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Offline Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        End to End Metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-high-frequency-questions" class="md-nav__link">
    <span class="md-ellipsis">
      
        11. High-Frequency Questions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-practical-design-pattern-the-gold-standard-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        12. Practical Design Pattern: The "Gold Standard" Pipeline
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Embedding</h1>

<h3 id="1-what-are-embeddings-in-rag">1. What Are Embeddings in RAG<a class="headerlink" href="#1-what-are-embeddings-in-rag" title="Permanent link">&para;</a></h3>
<p>Embeddings are fixed length vector representations of text that encode semantic meaning.</p>
<p>In a RAG pipeline, embeddings are used to:
- Represent user queries
- Represent documents or chunks stored in a vector index
- Enable similarity based retrieval using cosine similarity, dot product, or Euclidean distance</p>
<hr />
<h3 id="2-dense-vs-sparse-vs-hybrid-embeddings">2. Dense vs Sparse vs Hybrid Embeddings<a class="headerlink" href="#2-dense-vs-sparse-vs-hybrid-embeddings" title="Permanent link">&para;</a></h3>
<h3 id="21-dense-embeddings">2.1 Dense Embeddings<a class="headerlink" href="#21-dense-embeddings" title="Permanent link">&para;</a></h3>
<p>Dense embeddings map text into low dimensional continuous vectors, typically 256 to 4096 dimensions.</p>
<p><strong>Examples</strong></p>
<ul>
<li>Sentence BERT</li>
<li>E5, GTE, Instructor models</li>
<li>OpenAI text embedding models</li>
</ul>
<p><strong>Pros</strong></p>
<ul>
<li>Capture semantic similarity beyond exact token overlap</li>
<li>Handle paraphrases and natural language queries well</li>
<li>Efficient approximate nearest neighbor search</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Weak at exact keyword matching</li>
<li>Less interpretable</li>
<li>Sensitive to domain shift</li>
</ul>
<hr />
<h3 id="22-sparse-embeddings">2.2 Sparse Embeddings<a class="headerlink" href="#22-sparse-embeddings" title="Permanent link">&para;</a></h3>
<p>Sparse embeddings represent text as high dimensional sparse vectors aligned with vocabulary terms.</p>
<p><strong>Examples</strong></p>
<ul>
<li>TF-IDF</li>
<li>BM25</li>
<li>SPLADE</li>
</ul>
<p><strong>Pros</strong></p>
<ul>
<li>Strong lexical matching</li>
<li>Interpretable scores</li>
<li>Robust for rare terms, IDs, and numbers</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Poor semantic generalization</li>
<li>Vocabulary dependent</li>
<li>Large memory footprint</li>
</ul>
<hr />
<h3 id="23-hybrid-embeddings">2.3 Hybrid Embeddings<a class="headerlink" href="#23-hybrid-embeddings" title="Permanent link">&para;</a></h3>
<p>Hybrid approaches combine dense and sparse signals.</p>
<p><strong>Common strategies</strong></p>
<ul>
<li><strong>Late fusion of dense and BM25 scores:</strong> In this approach, you run two separate searches—one using semantic vectors (dense) and one using keyword matching (BM25)—and then combine their results using a ranking algorithm like Reciprocal Rank Fusion (RRF). It is called "late" fusion because the merging happens only after both independent retrieval processes are complete.</li>
<li><strong>Two stage retrieval with sparse recall and dense reranking:</strong> This strategy uses a fast, keyword-based search (sparse) to quickly narrow down millions of documents to a few hundred candidates, which are then re-scored by a more expensive semantic model (dense). This balances efficiency and accuracy by using the dense model only on a small, pre-filtered subset of data.</li>
<li><strong>Joint dense sparse representations:</strong> This involves using a single model or index that generates vectors containing both semantic signals and lexical "importance" weights (like SPLADE). Instead of running two separate searches, you perform one "unified" search that recognizes both the meaning of the sentence and the specific importance of the words within it.</li>
</ul>
<hr />
<h3 id="3-sentence-document-and-chunk-level-embeddings">3. Sentence, Document, and Chunk Level Embeddings<a class="headerlink" href="#3-sentence-document-and-chunk-level-embeddings" title="Permanent link">&para;</a></h3>
<h3 id="31-sentence-level-embeddings">3.1 Sentence Level Embeddings<a class="headerlink" href="#31-sentence-level-embeddings" title="Permanent link">&para;</a></h3>
<p>Each sentence is embedded independently.</p>
<p><strong>Use cases</strong></p>
<ul>
<li>FAQ style retrieval</li>
<li>Short factual queries</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Loses broader context</li>
<li>Sensitive to sentence segmentation errors</li>
</ul>
<hr />
<h3 id="32-document-level-embeddings">3.2 Document Level Embeddings<a class="headerlink" href="#32-document-level-embeddings" title="Permanent link">&para;</a></h3>
<p>Entire documents are embedded as a single vector.</p>
<p><strong>Use cases</strong></p>
<ul>
<li>Small documents</li>
<li>Metadata driven retrieval</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Poor recall for long documents</li>
<li>Information dilution</li>
</ul>
<hr />
<h3 id="33-chunk-level-embeddings">3.3 Chunk Level Embeddings<a class="headerlink" href="#33-chunk-level-embeddings" title="Permanent link">&para;</a></h3>
<p>Documents are split into chunks, often with overlap, and each chunk is embedded.</p>
<p><strong>Why chunking dominates in RAG</strong></p>
<ul>
<li>Preserves local context</li>
<li>Improves recall</li>
<li>Scales to long documents</li>
</ul>
<blockquote>
<p>Note: Chunking strategy and embedding strategy must be designed together.</p>
</blockquote>
<hr />
<h3 id="4-bi-encoders-vs-cross-encoders">4. Bi Encoders vs Cross Encoders<a class="headerlink" href="#4-bi-encoders-vs-cross-encoders" title="Permanent link">&para;</a></h3>
<h3 id="41-bi-encoders">4.1 Bi Encoders<a class="headerlink" href="#41-bi-encoders" title="Permanent link">&para;</a></h3>
<p>Query and document are encoded independently.</p>
<div class="arithmatex">\[
s(q, d) = \langle f(q), g(d) \rangle
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(s(q, d)\)</span>: The similarity score between a Query (<span class="arithmatex">\(q\)</span>) and a Document (<span class="arithmatex">\(d\)</span>).</li>
<li><span class="arithmatex">\(f(q)\)</span>: The embedding vector of the Query, produced by an encoder model <span class="arithmatex">\(f\)</span>.</li>
<li><span class="arithmatex">\(g(d)\)</span>: The embedding vector of the Document, produced by an encoder model <span class="arithmatex">\(g\)</span> (often the same model as <span class="arithmatex">\(f\)</span>).</li>
<li><span class="arithmatex">\(\langle \dots, \dots \rangle\)</span>: This symbol denotes the Dot Product (or inner product) between the two vectors.</li>
</ul>
<p>This formula highlights the independence of the encoding process. Because <span class="arithmatex">\(f(q)\)</span> and <span class="arithmatex">\(g(d)\)</span> are calculated separately, you can pre-calculate <span class="arithmatex">\(g(d)\)</span> for millions of documents and store them in a database. When a user asks a question, you only need to calculate <span class="arithmatex">\(f(q)\)</span> once and then perform a fast matrix multiplication to find the best match.</p>
<p><strong>Pros</strong></p>
<ul>
<li><strong>Fast retrieval:</strong> Pre-computed document embeddings allow query matching via simple math (dot product) rather than a full model pass.</li>
<li><strong>Scales to millions of documents:</strong> Retrieval time grows logarithmically rather than linearly because you only encode the query once.</li>
<li><strong>Enables vector indexing:</strong> Compatibility with Approximate Nearest Neighbor (ANN) algorithms allows for high-speed searching across massive datasets.</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><strong>Limited query document interaction:</strong> Because query and document are encoded in isolation, the model cannot perform "cross-attention" to see how specific words in the query relate to specific words in the document.</li>
</ul>
<p><strong>Used in</strong></p>
<ul>
<li><strong>First-stage retrieval:</strong> Acts as a high-speed "filter" to quickly narrow down a massive library of documents to a small candidate set (e.g., the top 100).</li>
</ul>
<hr />
<h3 id="42-cross-encoders">4.2 Cross Encoders<a class="headerlink" href="#42-cross-encoders" title="Permanent link">&para;</a></h3>
<p>Query and document are encoded jointly.</p>
<div class="arithmatex">\[
s(q, d) = h([q; d])
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\([q; d]\)</span>: This represents concatenation. The Query and Document are joined together into a single long sequence of text (usually separated by a special token like [SEP]).</li>
<li><span class="arithmatex">\(h(\dots)\)</span>: This is the Transformer model. The entire joined sequence is passed through all layers of the model at once.</li>
<li><span class="arithmatex">\(s(q, d)\)</span>: The final similarity score is typically the value of the "Classification" head (the [CLS] token) at the very end of the model.</li>
</ul>
<p><strong>Pros</strong></p>
<ul>
<li><strong>Strong relevance modeling:</strong> The model views the query and document simultaneously, allowing it to capture the complex relationship between the user's intent and the content.</li>
<li><strong>Fine-grained token interactions:</strong> Every word in the query can directly compare itself to every word in the document via the Transformer's self-attention mechanism.</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li><strong>Computationally expensive:</strong> Every query-document pair requires a full forward pass through a deep Transformer, leading to high latency and high GPU costs.</li>
<li><strong>Cannot be indexed:</strong> Because the score depends on the specific combination of query and document, you cannot pre-calculate or store results in a vector database for fast lookup.</li>
</ul>
<p><strong>Used in</strong></p>
<ul>
<li><strong>Reranking top-K retrieved chunks:</strong> Cross-Encoders are virtually never used for initial search, it acts as a "high-precision filter" that re-evaluates a small number of candidates (usually 10–100) provided by a faster first-stage retriever.</li>
</ul>
<blockquote>
<p>Note: Bi encoders maximize recall, cross encoders maximize precision.</p>
</blockquote>
<h3 id="43-late-interaction-eg-colbert">4.3 Late Interaction (e.g., ColBERT)<a class="headerlink" href="#43-late-interaction-eg-colbert" title="Permanent link">&para;</a></h3>
<p>The "middle ground" between Bi and Cross encoders.</p>
<ul>
<li><strong>Mechanism:</strong> Stores a vector for <em>every token</em> in a document. Retrieval uses a "MaxSim" operation.</li>
<li><strong>Benefit:</strong> Achieves Cross-encoder accuracy while remaining much faster for search.</li>
</ul>
<hr />
<h3 id="5-embedding-training-objectives">5. Embedding Training Objectives<a class="headerlink" href="#5-embedding-training-objectives" title="Permanent link">&para;</a></h3>
<p>Embedding Training Objectives refers to the mathematical strategy used to "teach" a model how to place similar items close together and dissimilar items far apart in a vector space.</p>
<h3 id="51-contrastive-learning">5.1 Contrastive Learning<a class="headerlink" href="#51-contrastive-learning" title="Permanent link">&para;</a></h3>
<p>Positive pairs are pulled closer while negatives are pushed apart.</p>
<div class="arithmatex">\[
\mathcal{L} = -\log \frac{\exp(sim(q, d^+))}{\exp(sim(q, d^+)) + \sum \exp(sim(q, d^-))}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{L}\)</span>: The InfoNCE Loss value. The InfoNCE loss is the negative log-likelihood of the model correctly identifying the single positive document among a set of negatives; minimizing this forces the model to pull relevant pairs together and push irrelevant pairs apart in the vector space."</li>
<li><span class="arithmatex">\(q\)</span>: The Query (or Anchor) vector.</li>
<li><span class="arithmatex">\(d^+\)</span>: The Positive Document vector (the ground-truth relevant document).</li>
<li><span class="arithmatex">\(d^-\)</span>: The Negative Document vectors (irrelevant documents in the batch).</li>
<li><span class="arithmatex">\(sim(u, v)\)</span>: The Similarity Function, usually Cosine Similarity or Dot Product, which measures how close two vectors are.</li>
<li><span class="arithmatex">\(\exp(\dots)\)</span>: The Exponential Function, used to ensure all similarity scores are positive and to amplify the difference between the highest and lowest scores.</li>
<li><span class="arithmatex">\(\sum\)</span>: The Summation over all negative samples in the batch.</li>
</ul>
<blockquote>
<p>Note: In practice, you will often see a <span class="arithmatex">\(\tau\)</span> (tau) symbol in this equation: <span class="arithmatex">\(\exp(sim(q, d) / \tau)\)</span>.
- <span class="arithmatex">\(\tau\)</span> (Temperature): A hyperparameter that scales the similarity scores.
- Why it matters: A low temperature makes the model more "opinionated," focusing heavily on the hardest negatives, while a high temperature smooths the distribution.</p>
</blockquote>
<p><strong>Example Losses used</strong></p>
<ul>
<li><strong>Information Noise-Contrastive Estimation (InfoNCE)</strong>: Described above</li>
<li><strong>Multiple negatives ranking (MNR) loss</strong>: MNR Loss is a specific implementation of contrastive learning that is the "bread and butter" of the Sentence-Transformers library.<ul>
<li>What it means: It is a framework designed for efficient training. Instead of manually finding <span class="arithmatex">\(100\)</span> "wrong" documents for every question, it uses In-Batch Negatives.</li>
<li>How it works: In a batch of <span class="arithmatex">\(K\)</span> pairs <span class="arithmatex">\(\{(q_1, d_1), (q_2, d_2), \dots, (q_K, d_K)\}\)</span>, the model assumes <span class="arithmatex">\(d_1\)</span> is the only correct answer for <span class="arithmatex">\(q_1\)</span>. It then treats all other documents in that same batch (7<span class="arithmatex">\(d_2, d_3, \dots, d_K\)</span>) as negative examples for 8<span class="arithmatex">\(q_1\)</span>.</li>
<li>Why it's popular: It allows you to train on millions of pairs without ever needing to label "negative" data. You only need <span class="arithmatex">\((query, +ve\_document)\)</span> pairs.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="52-supervised-retrieval-objectives">5.2 Supervised Retrieval Objectives<a class="headerlink" href="#52-supervised-retrieval-objectives" title="Permanent link">&para;</a></h3>
<p>Refers to the final stage of training where you move beyond general semantic similarity and "teach" the model to follow human-labeled preferences for specific queries.
Uses labeled query document relevance data.</p>
<p><strong>Examples</strong></p>
<ul>
<li><strong>MS MARCO style datasets:</strong> Named after Microsoft’s MAchine Reading COmprehension dataset, these are the "gold standard" for training RAG retrievers.</li>
<li>The Structure: It consists of real-world anonymized Bing queries paired with web passages that were manually marked as "relevant" or "irrelevant" by human judges.</li>
<li>
<p>Why it matters: Most embedding models (like BGE, GTE, or E5) are fine-tuned on MS MARCO because it teaches the model the "behavior" of a search engine: identifying specific passages that directly answer a natural language question.</p>
</li>
<li>
<p><strong>Pairwise and listwise losses</strong>: Once you have these labeled datasets, you need a mathematical "rule" to update the model. These two methods differ in how many documents they compare at once.</p>
</li>
<li><strong>Pairwise Loss (Comparing Two)</strong><ul>
<li>The Logic: The model is given a query (<span class="arithmatex">\(q\)</span>), a positive document (<span class="arithmatex">\(d^+\)</span>), and a negative document (<span class="arithmatex">\(d^-\)</span>). The loss function penalizes the model if the score for <span class="arithmatex">\(d^-\)</span> is higher than (or too close to) the score for <span class="arithmatex">\(d^+\)</span>.</li>
<li>Analogy: A "Head-to-Head" tournament. The model only needs to know that A is better than B.</li>
<li>Example: RankNet or Triplet Loss.</li>
</ul>
</li>
<li><strong>Listwise Loss (Comparing the Whole List)</strong><ul>
<li>The Logic: The model takes a query and a whole list of <span class="arithmatex">\(N\)</span> documents. It attempts to optimize the entire ranking order simultaneously, rather than just looking at pairs. It is designed to directly improve metrics like NDCG (Normalized Discounted Cumulative Gain).</li>
<li>Analogy: A "Leaderboard." The model tries to get the entire top-10 in the correct order.</li>
<li>Example: ListNet or LambdaMART.</li>
<li>Pros/Cons: Listwise is more accurate for ranking but much more computationally expensive and complex to implement than pairwise.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="53-in-batch-negatives">5.3 In Batch Negatives<a class="headerlink" href="#53-in-batch-negatives" title="Permanent link">&para;</a></h3>
<p>The Concept: For every query-document pair in a training batch, all other documents in that same batch are automatically treated as "negative" examples for that query.</p>
<p><strong>Why it matters</strong></p>
<ul>
<li><strong>Efficient:</strong> It provides a massive number of negative samples "for free" without the need to manually label or load additional data from disk.</li>
<li><strong>Scales well:</strong> Increasing the batch size mathematically increases the number of distractors per query (<span class="arithmatex">\(Batch\ Size - 1\)</span>), directly sharpening the model's discriminative power.</li>
<li><strong>Common in modern embedding training:</strong> It is the standard mechanism for high-performance models (like BGE, E5, and OpenAI) to learn robust representations at a massive scale.</li>
</ul>
<hr />
<h3 id="54-instruction-tuning-for-embeddings">5.4 Instruction Tuning for Embeddings<a class="headerlink" href="#54-instruction-tuning-for-embeddings" title="Permanent link">&para;</a></h3>
<p>Instruction tuning for embeddings involves training models to generate vector representations that dynamically adapt based on natural language task descriptions.</p>
<p><strong>Example</strong></p>
<ul>
<li>"Represent the question for retrieving relevant passages"</li>
</ul>
<p><strong>Benefits</strong></p>
<ul>
<li><strong>Better zero shot generalization:</strong> Enables models to handle novel retrieval or classification tasks they weren't explicitly trained on by interpreting the provided instruction.</li>
<li><strong>Improved alignment across tasks:</strong> Ensures that a single model can produce distinct, context-aware embeddings for the same text depending on whether the goal is clustering, similarity, or domain-specific search.</li>
</ul>
<h3 id="55-matryoshka-representation-learning-mrl">5.5 Matryoshka Representation Learning (MRL)<a class="headerlink" href="#55-matryoshka-representation-learning-mrl" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>What it is:</strong> Nesting information so that the first <span class="arithmatex">\(N\)</span> dimensions of a vector contain the most important features.</li>
<li><strong>Why it matters:</strong> Allows for <strong>vector truncation</strong>. You can store a 1536-dim vector but only query the first 256 dimensions to save on storage and compute with minimal accuracy loss.</li>
</ul>
<h3 id="56-instruction-tuned-embeddings">5.6 Instruction-Tuned Embeddings<a class="headerlink" href="#56-instruction-tuned-embeddings" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept:</strong> Models like <code>Instructor</code> or <code>BGE</code> that take a prefix (e.g., <em>"Represent this query for retrieving medical research papers"</em>).</li>
<li><strong>Benefit:</strong> Allows a single model to behave differently across specialized tasks (Search vs. Clustering vs. Classification).</li>
</ul>
<hr />
<h3 id="6-domain-adaptation-the-cold-start-problem-for-embeddings">6. Domain Adaptation (The "Cold Start" Problem) for Embeddings<a class="headerlink" href="#6-domain-adaptation-the-cold-start-problem-for-embeddings" title="Permanent link">&para;</a></h3>
<p>When generic embeddings (OpenAI/Cohere) fail on specialized data (Legal, Medical, Code):</p>
<ol>
<li><strong>Continued Pre-training:</strong> Run Masked Language Modeling (MLM) on your private corpus.</li>
<li><strong>Fine-tuning (Contrastive Loss):</strong> Use query-document pairs to "pull" relevant items closer.</li>
<li><strong>GPL (Generative Pseudo-Labeling):</strong> Use an LLM to generate synthetic questions for your unlabeled documents, then train the embedding model on these synthetic pairs.</li>
<li><strong>Adapter based tuning</strong></li>
</ol>
<hr />
<h3 id="7-distance-metrics-and-vector-normalization">7. Distance Metrics and Vector Normalization<a class="headerlink" href="#7-distance-metrics-and-vector-normalization" title="Permanent link">&para;</a></h3>
<h3 id="common-metrics">Common Metrics<a class="headerlink" href="#common-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cosine similarity:</strong> Measures the cosine of the angle between two vectors, focusing on directional orientation rather than magnitude.</li>
<li><strong>Dot product:</strong> Calculates the sum of the products of corresponding components, reflecting both the angle and the combined magnitudes of the vectors.</li>
<li><strong>Euclidean distance:</strong> Computes the straight-line distance between two points in space, sensitive to absolute coordinate values.</li>
</ul>
<blockquote>
<p>Note:
1. Cosine similarity with L2 normalized vectors is equivalent to dot product.
Since L2 normalization sets all vector magnitudes to 1, the denominator in the cosine formula (<span class="arithmatex">\(||A|| \times ||B||\)</span>) becomes 1, leaving only the dot product.
2. L2 normalization stabilizes similarity scores and improves ANN search behavior. By projecting all vectors onto a unit hypersphere, normalization removes length-based bias and ensures that approximate search algorithms (like HNSW or LSH) focus purely on semantic direction.</p>
</blockquote>
<hr />
<h3 id="8-multilingual-and-cross-lingual-embeddings">8. Multilingual and Cross Lingual Embeddings<a class="headerlink" href="#8-multilingual-and-cross-lingual-embeddings" title="Permanent link">&para;</a></h3>
<p><strong>Goal</strong>
Retrieve documents written in a different language than the query.</p>
<p><strong>Approaches</strong>
- Joint multilingual training
- Translation based supervision</p>
<p><strong>Challenges</strong>
- Language imbalance
- Script level differences</p>
<hr />
<h3 id="9-common-failure-modes-mitigations">9. Common Failure Modes &amp; Mitigations<a class="headerlink" href="#9-common-failure-modes-mitigations" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Failure Mode</th>
<th style="text-align: left;">Root Cause</th>
<th style="text-align: left;">Mitigation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Semantic Drift</strong></td>
<td style="text-align: left;">Retrieved chunks are topically related but irrelevant.</td>
<td style="text-align: left;">Use a Cross-encoder reranker.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Lost in the Middle</strong></td>
<td style="text-align: left;">LLM ignores context in long prompts.</td>
<td style="text-align: left;">Parent-Document Retrieval (retrieve small chunks, provide large context).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Out-of-Vocabulary</strong></td>
<td style="text-align: left;">Search for product IDs or rare part numbers.</td>
<td style="text-align: left;">Implement Hybrid Search (BM25).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Intent Mismatch</strong></td>
<td style="text-align: left;">Procedural query retrieves descriptive content.</td>
<td style="text-align: left;">Use HyDE (Hypothetical Document Embeddings).</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="10-evaluation-of-embeddings-in-rag">10. Evaluation of Embeddings in RAG<a class="headerlink" href="#10-evaluation-of-embeddings-in-rag" title="Permanent link">&para;</a></h3>
<h3 id="offline-metrics">Offline Metrics<a class="headerlink" href="#offline-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li>Recall at K</li>
<li>Mean reciprocal rank</li>
<li>nDCG</li>
</ul>
<hr />
<h3 id="end-to-end-metrics">End to End Metrics<a class="headerlink" href="#end-to-end-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li>Answer correctness</li>
<li>Faithfulness</li>
<li>Latency and cost</li>
</ul>
<p><strong>Key insight</strong>
Better retrieval does not always lead to better generation without proper prompting and context selection.</p>
<hr />
<h3 id="11-high-frequency-questions">11. High-Frequency Questions<a class="headerlink" href="#11-high-frequency-questions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Q: Why not use an LLM for retrieval directly?</strong><ul>
<li><em>A:</em> Context window limits and <span class="arithmatex">\(O(N^2)\)</span> attention complexity make it impossible to "read" millions of docs per query. Embeddings provide <span class="arithmatex">\(O(\log N)\)</span> search.</li>
</ul>
</li>
<li><strong>Q: Does increasing embedding dimensions always improve performance?</strong><ul>
<li><em>A:</em> Not necessarily. It can lead to the "Curse of Dimensionality" where distance metrics become less meaningful and latency increases.</li>
</ul>
</li>
<li><strong>Q: What is HyDE?</strong><ul>
<li><em>A:</em> Hypothetical Document Embeddings. You use an LLM to write a "fake" answer to the query, then embed that fake answer to find real documents. This aligns the query more closely with the document's vector space.</li>
</ul>
</li>
<li>
<p><strong>Q: When should you re-index your Vector DB?</strong></p>
<ul>
<li><em>A:</em> Any time you change the <strong>Embedding Model</strong>. You cannot compare vectors generated by Model A with those from Model B.</li>
</ul>
</li>
<li>
<p>When do dense embeddings outperform BM25</p>
</li>
<li>When should sparse retrieval be preferred</li>
<li>Why use bi encoders instead of cross encoders for retrieval</li>
<li>How would you adapt embeddings to a new domain with no labels</li>
<li>How do you debug poor retrieval in a RAG system</li>
</ol>
<hr />
<h3 id="12-practical-design-pattern-the-gold-standard-pipeline">12. Practical Design Pattern: The "Gold Standard" Pipeline<a class="headerlink" href="#12-practical-design-pattern-the-gold-standard-pipeline" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Query Expansion:</strong> Use an LLM to rewrite the query or generate a HyDE response.</li>
<li><strong>Hybrid Retrieval:</strong> Parallel search using Dense (Vector) and Sparse (BM25).</li>
<li><strong>Reciprocal Rank Fusion (RRF):</strong> Combine scores from different search methods.</li>
<li><strong>Reranking:</strong> Pass top 50 results through a Cross-encoder.</li>
<li><strong>Context Selection:</strong> Pass top 5-10 results to the LLM for final generation.</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>