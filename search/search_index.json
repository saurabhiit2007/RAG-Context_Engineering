{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAG &amp; Context Engineering concepts","text":""},{"location":"references/","title":"RAG &amp; Context Engineering concepts","text":""},{"location":"RAG/chunking/","title":"Chunking","text":""},{"location":"RAG/chunking/#1-overview","title":"1. Overview","text":"<p>Chunking is the process of splitting documents into smaller units before embedding and indexing. It is a critical design choice in RAG systems because it directly impacts retrieval quality, context relevance, latency, and cost.</p> <p>Long documents must be broken down into chunks to:</p> <ul> <li>Fit model token limits</li> <li>Improve retrieval granularity</li> <li>Reduce semantic dilution</li> <li>Control prompt length and cost</li> </ul> <p>Poor chunking can cause:</p> <ul> <li>Retrieval failure</li> <li>Loss of context</li> <li>Fragmented or incomplete answers</li> </ul>"},{"location":"RAG/chunking/#2-chunking-strategies","title":"2. Chunking Strategies","text":""},{"location":"RAG/chunking/#21-fixed-size-chunking","title":"2.1 Fixed-Size Chunking","text":"<p>Documents are split into chunks of a fixed token or character length, often with optional overlap.</p> <p>Example:</p> <ul> <li>Chunk size: 512 tokens</li> <li>Overlap: 50 tokens</li> </ul>"},{"location":"RAG/chunking/#algorithm","title":"Algorithm","text":"<ol> <li>Tokenize the document</li> <li>Split tokens into consecutive windows of size <code>N</code></li> <li>Optionally overlap adjacent windows by <code>M</code> tokens</li> </ol>"},{"location":"RAG/chunking/#pros","title":"Pros","text":"<ul> <li>Simple to implement</li> <li>Fast and scalable</li> <li>Works reasonably well as a baseline</li> </ul>"},{"location":"RAG/chunking/#cons","title":"Cons","text":"<ul> <li>Ignores semantic boundaries</li> <li>May split sentences or paragraphs</li> <li>Important information can be fragmented across chunks</li> </ul>"},{"location":"RAG/chunking/#when-to-use","title":"When to Use","text":"<ul> <li>Baseline RAG systems</li> <li>Uniform document formats</li> <li>Large-scale indexing where simplicity matters</li> </ul>"},{"location":"RAG/chunking/#22-sentence-based-chunking","title":"2.2 Sentence-Based Chunking","text":"<p>Documents are split at sentence boundaries, often grouping multiple sentences into a single chunk until a token limit is reached.</p>"},{"location":"RAG/chunking/#algorithm_1","title":"Algorithm","text":"<ol> <li>Sentence tokenize the document</li> <li>Accumulate sentences until the chunk reaches a size threshold</li> <li>Start a new chunk when the threshold is exceeded</li> </ol>"},{"location":"RAG/chunking/#pros_1","title":"Pros","text":"<ul> <li>Preserves sentence semantics</li> <li>Reduces mid-sentence splits</li> <li>Better coherence than fixed-size chunking</li> </ul>"},{"location":"RAG/chunking/#cons_1","title":"Cons","text":"<ul> <li>Sentence lengths vary significantly</li> <li>Long sentences can exceed limits</li> <li>Still ignores higher-level structure</li> </ul>"},{"location":"RAG/chunking/#when-to-use_1","title":"When to Use","text":"<ul> <li>Narrative text</li> <li>QA over articles or reports</li> <li>Medium-length documents</li> </ul>"},{"location":"RAG/chunking/#23-paragraph-based-chunking","title":"2.3 Paragraph-Based Chunking","text":"<p>Chunks are formed using paragraph boundaries, typically defined by newline separation.</p>"},{"location":"RAG/chunking/#algorithm_2","title":"Algorithm","text":"<ol> <li>Split document by paragraph delimiters</li> <li>Merge small paragraphs until size threshold is met</li> <li>Split large paragraphs if needed</li> </ol>"},{"location":"RAG/chunking/#pros_2","title":"Pros","text":"<ul> <li>Preserves local topical coherence</li> <li>Aligns well with human-written structure</li> <li>Better retrieval relevance for explanatory text</li> </ul>"},{"location":"RAG/chunking/#cons_2","title":"Cons","text":"<ul> <li>Paragraph length is highly inconsistent</li> <li>Large paragraphs may require further splitting</li> <li>Formatting noise can affect quality</li> </ul>"},{"location":"RAG/chunking/#when-to-use_2","title":"When to Use","text":"<ul> <li>Well-structured documents</li> <li>Technical blogs and documentation</li> <li>Markdown or HTML content</li> </ul>"},{"location":"RAG/chunking/#24-recursive-chunking","title":"2.4 Recursive Chunking","text":"<p>Recursive chunking applies a hierarchy of splitting rules, starting with coarse semantic units and progressively falling back to finer ones if size constraints are violated.</p> <p>Typical split order:</p> <ul> <li>Sections</li> <li>Subsections</li> <li>Paragraphs</li> <li>Sentences</li> <li>Fixed-size fallback</li> </ul>"},{"location":"RAG/chunking/#algorithm_3","title":"Algorithm","text":"<ol> <li>Attempt to split by highest-level boundary</li> <li>If chunk exceeds size limit, split using next level</li> <li>Continue recursively until constraints are satisfied</li> </ol>"},{"location":"RAG/chunking/#pros_3","title":"Pros","text":"<ul> <li>Preserves document structure</li> <li>Produces semantically meaningful chunks</li> <li>Handles diverse document formats well</li> </ul>"},{"location":"RAG/chunking/#cons_3","title":"Cons","text":"<ul> <li>More complex to implement</li> <li>Requires reliable document parsing</li> <li>Slightly higher preprocessing cost</li> </ul>"},{"location":"RAG/chunking/#when-to-use_3","title":"When to Use","text":"<ul> <li>Enterprise documents</li> <li>PDFs with headings</li> <li>Mixed-format content</li> </ul>"},{"location":"RAG/chunking/#25-context-aware-chunking","title":"2.5 Context-Aware Chunking","text":"<p>Chunks are formed based on semantic similarity rather than fixed boundaries. Adjacent text segments are grouped if they share high semantic coherence.</p>"},{"location":"RAG/chunking/#algorithm_4","title":"Algorithm","text":"<ol> <li>Compute embeddings for small text units</li> <li>Measure semantic similarity between adjacent units</li> <li>Merge units until similarity drops or size limit is reached</li> </ol>"},{"location":"RAG/chunking/#pros_4","title":"Pros","text":"<ul> <li>High semantic coherence</li> <li>Reduces context fragmentation</li> <li>Improves retrieval precision</li> </ul>"},{"location":"RAG/chunking/#cons_4","title":"Cons","text":"<ul> <li>Computationally expensive</li> <li>Requires embedding during preprocessing</li> <li>Sensitive to similarity thresholds</li> </ul>"},{"location":"RAG/chunking/#when-to-use_4","title":"When to Use","text":"<ul> <li>High-accuracy RAG systems</li> <li>Knowledge-intensive QA</li> <li>Smaller corpora where quality matters</li> </ul>"},{"location":"RAG/chunking/#26-sliding-window-chunking","title":"2.6 Sliding Window Chunking","text":"<p>Chunks are generated using overlapping windows that slide across the document.</p> <p>Example:</p> <ul> <li>Window size: 512 tokens</li> <li>Stride: 256 tokens</li> </ul>"},{"location":"RAG/chunking/#pros_5","title":"Pros","text":"<ul> <li>Preserves cross-boundary context</li> <li>Reduces information loss at chunk edges</li> <li>Improves recall</li> </ul>"},{"location":"RAG/chunking/#cons_5","title":"Cons","text":"<ul> <li>Increased index size</li> <li>Higher storage and retrieval cost</li> <li>More redundant embeddings</li> </ul>"},{"location":"RAG/chunking/#when-to-use_5","title":"When to Use","text":"<ul> <li>Long-form documents</li> <li>Cases where boundary loss is critical</li> <li>Multi-hop reasoning tasks</li> </ul>"},{"location":"RAG/chunking/#chunking-strategy-comparison","title":"Chunking Strategy Comparison","text":"Strategy Semantic Coherence Complexity Index Size Common Use Fixed-size Low Low Medium Baselines Sentence-based Medium Low Medium Articles Paragraph-based Medium Low Medium Documentation Recursive High Medium Medium Enterprise RAG Context-aware High High Low to Medium High-precision RAG Sliding window Medium Low High Long documents"},{"location":"RAG/chunking/#3-other-concepts","title":"3. Other Concepts","text":""},{"location":"RAG/chunking/#31-chunk-size-vs-top-k-tradeoffs","title":"3.1 Chunk Size vs Top-k Tradeoffs","text":"<p>Chunk size and top-k retrieval are tightly coupled design parameters in RAG systems. Changing one almost always requires adjusting the other.</p>"},{"location":"RAG/chunking/#key-intuition","title":"Key Intuition","text":"<ul> <li>Smaller chunks increase retrieval granularity but reduce context per chunk</li> <li>Larger chunks provide more local context but reduce retrieval precision</li> </ul>"},{"location":"RAG/chunking/#tradeoff-matrix","title":"Tradeoff Matrix","text":"Chunk Size Typical Top-k Behavior Small (100\u2013300 tokens) High (10\u201320) High recall, lower precision Medium (300\u2013700 tokens) Medium (4\u20138) Balanced retrieval Large (700\u20131500 tokens) Low (1\u20133) High precision, risk of misses"},{"location":"RAG/chunking/#failure-patterns","title":"Failure Patterns","text":"<ul> <li>Small chunks + low top-k \u2192 missing required information</li> <li>Large chunks + high top-k \u2192 context overload</li> <li>Large chunks + low top-k \u2192 partial coverage</li> </ul>"},{"location":"RAG/chunking/#32-chunk-overlap-selection","title":"3.2 Chunk Overlap Selection","text":"<p>Overlap determines how much content is shared between adjacent chunks.</p>"},{"location":"RAG/chunking/#why-overlap-matters","title":"Why Overlap Matters","text":"<ul> <li>Prevents information loss at chunk boundaries</li> <li>Preserves cross-sentence and cross-paragraph context</li> </ul>"},{"location":"RAG/chunking/#typical-settings","title":"Typical Settings","text":"<ul> <li>Fixed-size chunking: 10 to 20 percent overlap</li> <li>Sliding window chunking: stride equals 50 percent of window size</li> <li>Recursive chunking: overlap often unnecessary</li> </ul>"},{"location":"RAG/chunking/#tradeoffs","title":"Tradeoffs","text":"<p>Benefits - Improved recall - Reduced boundary effects</p> <p>Costs - Larger index size - Higher storage and retrieval cost - More redundant embeddings</p>"},{"location":"RAG/chunking/#insight","title":"Insight","text":"<p>Overlap helps recall but should be used sparingly. Overlap is a mitigation strategy, not a substitute for good chunking.</p>"},{"location":"RAG/chunking/#33-chunk-metadata-and-filtering","title":"3.3 Chunk Metadata and Filtering","text":""},{"location":"RAG/chunking/#what-is-chunk-metadata","title":"What Is Chunk Metadata?","text":"<p>Metadata is structured information attached to each chunk that enables filtering and ranking during retrieval.</p> <p>Common metadata fields</p> <ul> <li>Document ID</li> <li>Section or heading</li> <li>Timestamp or version</li> <li>Author or source</li> <li>Content type</li> </ul>"},{"location":"RAG/chunking/#how-metadata-is-used","title":"How Metadata Is Used","text":"<ul> <li>Filter chunks before similarity search</li> <li>Re-rank retrieved chunks</li> <li>Restrict retrieval to specific document subsets</li> </ul>"},{"location":"RAG/chunking/#benefits","title":"Benefits","text":"<ul> <li>Improves retrieval precision</li> <li>Reduces noise</li> <li>Enables structured queries</li> </ul>"},{"location":"RAG/chunking/#example","title":"Example","text":"<p>Retrieve only:</p> <ul> <li>Chunks from a specific product version</li> <li>Chunks created after a certain date</li> <li>Chunks from a trusted source</li> </ul>"},{"location":"RAG/chunking/#insight_1","title":"Insight","text":"<p>Metadata filtering is one of the most effective ways to improve vanilla RAG without changing models.</p>"},{"location":"RAG/chunking/#34-chunking-for-tables-and-code","title":"3.4 Chunking for Tables and Code","text":"<p>Text-centric chunking often fails for structured content such as tables and source code.</p>"},{"location":"RAG/chunking/#341-chunking-tables","title":"3.4.1 Chunking Tables","text":"<p>Challenges</p> <ul> <li>Rows and columns encode relationships</li> <li>Linear text chunking destroys structure</li> </ul> <p>Common strategies</p> <ul> <li>Row-based chunking</li> <li>Column-wise chunking for analytical queries</li> <li>Table-to-text serialization with schema preservation</li> </ul> <p>Best practice</p> <p>Attach table schema and headers as metadata to every chunk.</p>"},{"location":"RAG/chunking/#342-chunking-code","title":"3.4.2 Chunking Code","text":"<p>Challenges</p> <ul> <li>Long-range dependencies</li> <li>Functions and classes are semantic units</li> </ul> <p>Common strategies</p> <ul> <li>Function-level chunking</li> <li>Class-level chunking</li> <li>File-level chunking for small files</li> </ul> <p>Best practice</p> <p>Never split a function or class across chunks.</p>"},{"location":"RAG/chunking/#35-adaptive-chunking-based-on-query-type","title":"3.5 Adaptive Chunking Based on Query Type","text":""},{"location":"RAG/chunking/#motivation","title":"Motivation","text":"<p>Different queries require different chunking granularity. Static chunking cannot optimally serve all query types.</p>"},{"location":"RAG/chunking/#common-query-types","title":"Common Query Types","text":"Query Type Preferred Chunking Fact lookup Small chunks Concept explanation Medium chunks Procedural steps Large chunks Multi-hop reasoning Overlapping or sliding window chunks"},{"location":"RAG/chunking/#adaptive-strategies","title":"Adaptive Strategies","text":"<ul> <li>Maintain multiple indexes with different chunk sizes</li> <li>Dynamically select top-k based on query intent</li> <li>Use query classification to select chunking policy</li> </ul>"},{"location":"RAG/chunking/#pros_6","title":"Pros","text":"<ul> <li>Higher retrieval accuracy</li> <li>Better context utilization</li> <li>Reduced hallucinations</li> </ul>"},{"location":"RAG/chunking/#cons_6","title":"Cons","text":"<ul> <li>Increased system complexity</li> <li>Higher indexing and storage cost</li> <li>Requires query understanding</li> </ul>"},{"location":"RAG/chunking/#4-takeaways","title":"4. Takeaways","text":"<ul> <li>How do you choose the right chunk size?</li> <li>The Embedding Model: Some models perform better with short sentences; others can handle long paragraphs.</li> <li>User Query Style: If users ask short, specific questions, small chunks are better. If they ask for summaries, larger chunks are needed.</li> <li>The \"Lost in the Middle\" Phenomenon: LLMs struggle to find information buried in the middle of a massive chunk.</li> <li>Chunking directly impacts retrieval quality and hallucination rates</li> <li>Smaller chunks improve recall, larger chunks improve coherence</li> <li>There is no universally optimal chunking strategy</li> <li>Recursive and context-aware chunking are commonly used in production</li> <li>Chunking should be evaluated jointly with retrieval and prompting</li> <li>Chunk size, overlap, and top-k must be tuned jointly</li> <li>Metadata filtering often yields large quality gains</li> <li>Tables and code require specialized chunking</li> <li>Adaptive chunking improves robustness but adds complexity</li> </ul>"},{"location":"RAG/embedding/","title":"Embedding","text":""},{"location":"RAG/embedding/#1-what-are-embeddings-in-rag","title":"1. What Are Embeddings in RAG","text":"<p>Embeddings are fixed length vector representations of text that encode semantic meaning.</p> <p>In a RAG pipeline, embeddings are used to: - Represent user queries - Represent documents or chunks stored in a vector index - Enable similarity based retrieval using cosine similarity, dot product, or Euclidean distance</p>"},{"location":"RAG/embedding/#2-dense-vs-sparse-vs-hybrid-embeddings","title":"2. Dense vs Sparse vs Hybrid Embeddings","text":""},{"location":"RAG/embedding/#21-dense-embeddings","title":"2.1 Dense Embeddings","text":"<p>Dense embeddings map text into low dimensional continuous vectors, typically 256 to 4096 dimensions.</p> <p>Examples</p> <ul> <li>Sentence BERT</li> <li>E5, GTE, Instructor models</li> <li>OpenAI text embedding models</li> </ul> <p>Pros</p> <ul> <li>Capture semantic similarity beyond exact token overlap</li> <li>Handle paraphrases and natural language queries well</li> <li>Efficient approximate nearest neighbor search</li> </ul> <p>Cons</p> <ul> <li>Weak at exact keyword matching</li> <li>Less interpretable</li> <li>Sensitive to domain shift</li> </ul>"},{"location":"RAG/embedding/#22-sparse-embeddings","title":"2.2 Sparse Embeddings","text":"<p>Sparse embeddings represent text as high dimensional sparse vectors aligned with vocabulary terms.</p> <p>Examples</p> <ul> <li>TF-IDF</li> <li>BM25</li> <li>SPLADE</li> </ul> <p>Pros</p> <ul> <li>Strong lexical matching</li> <li>Interpretable scores</li> <li>Robust for rare terms, IDs, and numbers</li> </ul> <p>Cons</p> <ul> <li>Poor semantic generalization</li> <li>Vocabulary dependent</li> <li>Large memory footprint</li> </ul>"},{"location":"RAG/embedding/#23-hybrid-embeddings","title":"2.3 Hybrid Embeddings","text":"<p>Hybrid approaches combine dense and sparse signals.</p> <p>Common strategies</p> <ul> <li>Late fusion of dense and BM25 scores: In this approach, you run two separate searches\u2014one using semantic vectors (dense) and one using keyword matching (BM25)\u2014and then combine their results using a ranking algorithm like Reciprocal Rank Fusion (RRF). It is called \"late\" fusion because the merging happens only after both independent retrieval processes are complete.</li> <li>Two stage retrieval with sparse recall and dense reranking: This strategy uses a fast, keyword-based search (sparse) to quickly narrow down millions of documents to a few hundred candidates, which are then re-scored by a more expensive semantic model (dense). This balances efficiency and accuracy by using the dense model only on a small, pre-filtered subset of data.</li> <li>Joint dense sparse representations: This involves using a single model or index that generates vectors containing both semantic signals and lexical \"importance\" weights (like SPLADE). Instead of running two separate searches, you perform one \"unified\" search that recognizes both the meaning of the sentence and the specific importance of the words within it.</li> </ul>"},{"location":"RAG/embedding/#3-sentence-document-and-chunk-level-embeddings","title":"3. Sentence, Document, and Chunk Level Embeddings","text":""},{"location":"RAG/embedding/#31-sentence-level-embeddings","title":"3.1 Sentence Level Embeddings","text":"<p>Each sentence is embedded independently.</p> <p>Use cases</p> <ul> <li>FAQ style retrieval</li> <li>Short factual queries</li> </ul> <p>Limitations</p> <ul> <li>Loses broader context</li> <li>Sensitive to sentence segmentation errors</li> </ul>"},{"location":"RAG/embedding/#32-document-level-embeddings","title":"3.2 Document Level Embeddings","text":"<p>Entire documents are embedded as a single vector.</p> <p>Use cases</p> <ul> <li>Small documents</li> <li>Metadata driven retrieval</li> </ul> <p>Limitations</p> <ul> <li>Poor recall for long documents</li> <li>Information dilution</li> </ul>"},{"location":"RAG/embedding/#33-chunk-level-embeddings","title":"3.3 Chunk Level Embeddings","text":"<p>Documents are split into chunks, often with overlap, and each chunk is embedded.</p> <p>Why chunking dominates in RAG</p> <ul> <li>Preserves local context</li> <li>Improves recall</li> <li>Scales to long documents</li> </ul> <p>Note: Chunking strategy and embedding strategy must be designed together.</p>"},{"location":"RAG/embedding/#4-bi-encoders-vs-cross-encoders","title":"4. Bi Encoders vs Cross Encoders","text":""},{"location":"RAG/embedding/#41-bi-encoders","title":"4.1 Bi Encoders","text":"<p>Query and document are encoded independently.</p> \\[ s(q, d) = \\langle f(q), g(d) \\rangle \\] <p>where:</p> <ul> <li>\\(s(q, d)\\): The similarity score between a Query (\\(q\\)) and a Document (\\(d\\)).</li> <li>\\(f(q)\\): The embedding vector of the Query, produced by an encoder model \\(f\\).</li> <li>\\(g(d)\\): The embedding vector of the Document, produced by an encoder model \\(g\\) (often the same model as \\(f\\)).</li> <li>\\(\\langle \\dots, \\dots \\rangle\\): This symbol denotes the Dot Product (or inner product) between the two vectors.</li> </ul> <p>This formula highlights the independence of the encoding process. Because \\(f(q)\\) and \\(g(d)\\) are calculated separately, you can pre-calculate \\(g(d)\\) for millions of documents and store them in a database. When a user asks a question, you only need to calculate \\(f(q)\\) once and then perform a fast matrix multiplication to find the best match.</p> <p>Pros</p> <ul> <li>Fast retrieval: Pre-computed document embeddings allow query matching via simple math (dot product) rather than a full model pass.</li> <li>Scales to millions of documents: Retrieval time grows logarithmically rather than linearly because you only encode the query once.</li> <li>Enables vector indexing: Compatibility with Approximate Nearest Neighbor (ANN) algorithms allows for high-speed searching across massive datasets.</li> </ul> <p>Cons</p> <ul> <li>Limited query document interaction: Because query and document are encoded in isolation, the model cannot perform \"cross-attention\" to see how specific words in the query relate to specific words in the document.</li> </ul> <p>Used in</p> <ul> <li>First-stage retrieval: Acts as a high-speed \"filter\" to quickly narrow down a massive library of documents to a small candidate set (e.g., the top 100).</li> </ul>"},{"location":"RAG/embedding/#42-cross-encoders","title":"4.2 Cross Encoders","text":"<p>Query and document are encoded jointly.</p> \\[ s(q, d) = h([q; d]) \\] <p>where:</p> <ul> <li>\\([q; d]\\): This represents concatenation. The Query and Document are joined together into a single long sequence of text (usually separated by a special token like [SEP]).</li> <li>\\(h(\\dots)\\): This is the Transformer model. The entire joined sequence is passed through all layers of the model at once.</li> <li>\\(s(q, d)\\): The final similarity score is typically the value of the \"Classification\" head (the [CLS] token) at the very end of the model.</li> </ul> <p>Pros</p> <ul> <li>Strong relevance modeling: The model views the query and document simultaneously, allowing it to capture the complex relationship between the user's intent and the content.</li> <li>Fine-grained token interactions: Every word in the query can directly compare itself to every word in the document via the Transformer's self-attention mechanism.</li> </ul> <p>Cons</p> <ul> <li>Computationally expensive: Every query-document pair requires a full forward pass through a deep Transformer, leading to high latency and high GPU costs.</li> <li>Cannot be indexed: Because the score depends on the specific combination of query and document, you cannot pre-calculate or store results in a vector database for fast lookup.</li> </ul> <p>Used in</p> <ul> <li>Reranking top-K retrieved chunks: Cross-Encoders are virtually never used for initial search, it acts as a \"high-precision filter\" that re-evaluates a small number of candidates (usually 10\u2013100) provided by a faster first-stage retriever.</li> </ul> <p>Note: Bi encoders maximize recall, cross encoders maximize precision.</p>"},{"location":"RAG/embedding/#43-late-interaction-eg-colbert","title":"4.3 Late Interaction (e.g., ColBERT)","text":"<p>The \"middle ground\" between Bi and Cross encoders.</p> <ul> <li>Mechanism: Stores a vector for every token in a document. Retrieval uses a \"MaxSim\" operation.</li> <li>Benefit: Achieves Cross-encoder accuracy while remaining much faster for search.</li> </ul>"},{"location":"RAG/embedding/#5-embedding-training-objectives","title":"5. Embedding Training Objectives","text":"<p>Embedding Training Objectives refers to the mathematical strategy used to \"teach\" a model how to place similar items close together and dissimilar items far apart in a vector space.</p>"},{"location":"RAG/embedding/#51-contrastive-learning","title":"5.1 Contrastive Learning","text":"<p>Positive pairs are pulled closer while negatives are pushed apart.</p> \\[ \\mathcal{L} = -\\log \\frac{\\exp(sim(q, d^+))}{\\exp(sim(q, d^+)) + \\sum \\exp(sim(q, d^-))} \\] <p>where:</p> <ul> <li>\\(\\mathcal{L}\\): The InfoNCE Loss value. The InfoNCE loss is the negative log-likelihood of the model correctly identifying the single positive document among a set of negatives; minimizing this forces the model to pull relevant pairs together and push irrelevant pairs apart in the vector space.\"</li> <li>\\(q\\): The Query (or Anchor) vector.</li> <li>\\(d^+\\): The Positive Document vector (the ground-truth relevant document).</li> <li>\\(d^-\\): The Negative Document vectors (irrelevant documents in the batch).</li> <li>\\(sim(u, v)\\): The Similarity Function, usually Cosine Similarity or Dot Product, which measures how close two vectors are.</li> <li>\\(\\exp(\\dots)\\): The Exponential Function, used to ensure all similarity scores are positive and to amplify the difference between the highest and lowest scores.</li> <li>\\(\\sum\\): The Summation over all negative samples in the batch.</li> </ul> <p>Note: In practice, you will often see a \\(\\tau\\) (tau) symbol in this equation: \\(\\exp(sim(q, d) / \\tau)\\). - \\(\\tau\\) (Temperature): A hyperparameter that scales the similarity scores. - Why it matters: A low temperature makes the model more \"opinionated,\" focusing heavily on the hardest negatives, while a high temperature smooths the distribution.</p> <p>Example Losses used</p> <ul> <li>Information Noise-Contrastive Estimation (InfoNCE): Described above</li> <li>Multiple negatives ranking (MNR) loss: MNR Loss is a specific implementation of contrastive learning that is the \"bread and butter\" of the Sentence-Transformers library.<ul> <li>What it means: It is a framework designed for efficient training. Instead of manually finding \\(100\\) \"wrong\" documents for every question, it uses In-Batch Negatives.</li> <li>How it works: In a batch of \\(K\\) pairs \\(\\{(q_1, d_1), (q_2, d_2), \\dots, (q_K, d_K)\\}\\), the model assumes \\(d_1\\) is the only correct answer for \\(q_1\\). It then treats all other documents in that same batch (7\\(d_2, d_3, \\dots, d_K\\)) as negative examples for 8\\(q_1\\).</li> <li>Why it's popular: It allows you to train on millions of pairs without ever needing to label \"negative\" data. You only need \\((query, +ve\\_document)\\) pairs.</li> </ul> </li> </ul>"},{"location":"RAG/embedding/#52-supervised-retrieval-objectives","title":"5.2 Supervised Retrieval Objectives","text":"<p>Refers to the final stage of training where you move beyond general semantic similarity and \"teach\" the model to follow human-labeled preferences for specific queries. Uses labeled query document relevance data.</p> <p>Examples</p> <ul> <li>MS MARCO style datasets: Named after Microsoft\u2019s MAchine Reading COmprehension dataset, these are the \"gold standard\" for training RAG retrievers.</li> <li>The Structure: It consists of real-world anonymized Bing queries paired with web passages that were manually marked as \"relevant\" or \"irrelevant\" by human judges.</li> <li> <p>Why it matters: Most embedding models (like BGE, GTE, or E5) are fine-tuned on MS MARCO because it teaches the model the \"behavior\" of a search engine: identifying specific passages that directly answer a natural language question.</p> </li> <li> <p>Pairwise and listwise losses: Once you have these labeled datasets, you need a mathematical \"rule\" to update the model. These two methods differ in how many documents they compare at once.</p> </li> <li>Pairwise Loss (Comparing Two)<ul> <li>The Logic: The model is given a query (\\(q\\)), a positive document (\\(d^+\\)), and a negative document (\\(d^-\\)). The loss function penalizes the model if the score for \\(d^-\\) is higher than (or too close to) the score for \\(d^+\\).</li> <li>Analogy: A \"Head-to-Head\" tournament. The model only needs to know that A is better than B.</li> <li>Example: RankNet or Triplet Loss.</li> </ul> </li> <li>Listwise Loss (Comparing the Whole List)<ul> <li>The Logic: The model takes a query and a whole list of \\(N\\) documents. It attempts to optimize the entire ranking order simultaneously, rather than just looking at pairs. It is designed to directly improve metrics like NDCG (Normalized Discounted Cumulative Gain).</li> <li>Analogy: A \"Leaderboard.\" The model tries to get the entire top-10 in the correct order.</li> <li>Example: ListNet or LambdaMART.</li> <li>Pros/Cons: Listwise is more accurate for ranking but much more computationally expensive and complex to implement than pairwise.</li> </ul> </li> </ul>"},{"location":"RAG/embedding/#53-in-batch-negatives","title":"5.3 In Batch Negatives","text":"<p>The Concept: For every query-document pair in a training batch, all other documents in that same batch are automatically treated as \"negative\" examples for that query.</p> <p>Why it matters</p> <ul> <li>Efficient: It provides a massive number of negative samples \"for free\" without the need to manually label or load additional data from disk.</li> <li>Scales well: Increasing the batch size mathematically increases the number of distractors per query (\\(Batch\\ Size - 1\\)), directly sharpening the model's discriminative power.</li> <li>Common in modern embedding training: It is the standard mechanism for high-performance models (like BGE, E5, and OpenAI) to learn robust representations at a massive scale.</li> </ul>"},{"location":"RAG/embedding/#54-instruction-tuning-for-embeddings","title":"5.4 Instruction Tuning for Embeddings","text":"<p>Instruction tuning for embeddings involves training models to generate vector representations that dynamically adapt based on natural language task descriptions.</p> <p>Example</p> <ul> <li>\"Represent the question for retrieving relevant passages\"</li> </ul> <p>Benefits</p> <ul> <li>Better zero shot generalization: Enables models to handle novel retrieval or classification tasks they weren't explicitly trained on by interpreting the provided instruction.</li> <li>Improved alignment across tasks: Ensures that a single model can produce distinct, context-aware embeddings for the same text depending on whether the goal is clustering, similarity, or domain-specific search.</li> </ul>"},{"location":"RAG/embedding/#55-matryoshka-representation-learning-mrl","title":"5.5 Matryoshka Representation Learning (MRL)","text":"<ul> <li>What it is: Nesting information so that the first \\(N\\) dimensions of a vector contain the most important features.</li> <li>Why it matters: Allows for vector truncation. You can store a 1536-dim vector but only query the first 256 dimensions to save on storage and compute with minimal accuracy loss.</li> </ul>"},{"location":"RAG/embedding/#56-instruction-tuned-embeddings","title":"5.6 Instruction-Tuned Embeddings","text":"<ul> <li>Concept: Models like <code>Instructor</code> or <code>BGE</code> that take a prefix (e.g., \"Represent this query for retrieving medical research papers\").</li> <li>Benefit: Allows a single model to behave differently across specialized tasks (Search vs. Clustering vs. Classification).</li> </ul>"},{"location":"RAG/embedding/#6-domain-adaptation-the-cold-start-problem-for-embeddings","title":"6. Domain Adaptation (The \"Cold Start\" Problem) for Embeddings","text":"<p>When generic embeddings (OpenAI/Cohere) fail on specialized data (Legal, Medical, Code):</p> <ol> <li>Continued Pre-training: Run Masked Language Modeling (MLM) on your private corpus.</li> <li>Fine-tuning (Contrastive Loss): Use query-document pairs to \"pull\" relevant items closer.</li> <li>GPL (Generative Pseudo-Labeling): Use an LLM to generate synthetic questions for your unlabeled documents, then train the embedding model on these synthetic pairs.</li> <li>Adapter based tuning</li> </ol>"},{"location":"RAG/embedding/#7-distance-metrics-and-vector-normalization","title":"7. Distance Metrics and Vector Normalization","text":""},{"location":"RAG/embedding/#common-metrics","title":"Common Metrics","text":"<ul> <li>Cosine similarity: Measures the cosine of the angle between two vectors, focusing on directional orientation rather than magnitude.</li> <li>Dot product: Calculates the sum of the products of corresponding components, reflecting both the angle and the combined magnitudes of the vectors.</li> <li>Euclidean distance: Computes the straight-line distance between two points in space, sensitive to absolute coordinate values.</li> </ul> <p>Note: 1. Cosine similarity with L2 normalized vectors is equivalent to dot product. Since L2 normalization sets all vector magnitudes to 1, the denominator in the cosine formula (\\(||A|| \\times ||B||\\)) becomes 1, leaving only the dot product. 2. L2 normalization stabilizes similarity scores and improves ANN search behavior. By projecting all vectors onto a unit hypersphere, normalization removes length-based bias and ensures that approximate search algorithms (like HNSW or LSH) focus purely on semantic direction.</p>"},{"location":"RAG/embedding/#8-multilingual-and-cross-lingual-embeddings","title":"8. Multilingual and Cross Lingual Embeddings","text":"<p>Goal Retrieve documents written in a different language than the query.</p> <p>Approaches - Joint multilingual training - Translation based supervision</p> <p>Challenges - Language imbalance - Script level differences</p>"},{"location":"RAG/embedding/#9-common-failure-modes-mitigations","title":"9. Common Failure Modes &amp; Mitigations","text":"Failure Mode Root Cause Mitigation Semantic Drift Retrieved chunks are topically related but irrelevant. Use a Cross-encoder reranker. Lost in the Middle LLM ignores context in long prompts. Parent-Document Retrieval (retrieve small chunks, provide large context). Out-of-Vocabulary Search for product IDs or rare part numbers. Implement Hybrid Search (BM25). Intent Mismatch Procedural query retrieves descriptive content. Use HyDE (Hypothetical Document Embeddings)."},{"location":"RAG/embedding/#10-evaluation-of-embeddings-in-rag","title":"10. Evaluation of Embeddings in RAG","text":""},{"location":"RAG/embedding/#offline-metrics","title":"Offline Metrics","text":"<ul> <li>Recall at K</li> <li>Mean reciprocal rank</li> <li>nDCG</li> </ul>"},{"location":"RAG/embedding/#end-to-end-metrics","title":"End to End Metrics","text":"<ul> <li>Answer correctness</li> <li>Faithfulness</li> <li>Latency and cost</li> </ul> <p>Key insight Better retrieval does not always lead to better generation without proper prompting and context selection.</p>"},{"location":"RAG/embedding/#11-high-frequency-questions","title":"11. High-Frequency Questions","text":"<ol> <li>Q: Why not use an LLM for retrieval directly?<ul> <li>A: Context window limits and \\(O(N^2)\\) attention complexity make it impossible to \"read\" millions of docs per query. Embeddings provide \\(O(\\log N)\\) search.</li> </ul> </li> <li>Q: Does increasing embedding dimensions always improve performance?<ul> <li>A: Not necessarily. It can lead to the \"Curse of Dimensionality\" where distance metrics become less meaningful and latency increases.</li> </ul> </li> <li>Q: What is HyDE?<ul> <li>A: Hypothetical Document Embeddings. You use an LLM to write a \"fake\" answer to the query, then embed that fake answer to find real documents. This aligns the query more closely with the document's vector space.</li> </ul> </li> <li> <p>Q: When should you re-index your Vector DB?</p> <ul> <li>A: Any time you change the Embedding Model. You cannot compare vectors generated by Model A with those from Model B.</li> </ul> </li> <li> <p>When do dense embeddings outperform BM25</p> </li> <li>When should sparse retrieval be preferred</li> <li>Why use bi encoders instead of cross encoders for retrieval</li> <li>How would you adapt embeddings to a new domain with no labels</li> <li>How do you debug poor retrieval in a RAG system</li> </ol>"},{"location":"RAG/embedding/#12-practical-design-pattern-the-gold-standard-pipeline","title":"12. Practical Design Pattern: The \"Gold Standard\" Pipeline","text":"<ol> <li>Query Expansion: Use an LLM to rewrite the query or generate a HyDE response.</li> <li>Hybrid Retrieval: Parallel search using Dense (Vector) and Sparse (BM25).</li> <li>Reciprocal Rank Fusion (RRF): Combine scores from different search methods.</li> <li>Reranking: Pass top 50 results through a Cross-encoder.</li> <li>Context Selection: Pass top 5-10 results to the LLM for final generation.</li> </ol>"},{"location":"RAG/evaluation/","title":"Evaluation","text":""},{"location":"RAG/evaluation/#evaluation-of-retrieval-augmented-generation-rag-systems","title":"Evaluation of Retrieval-Augmented Generation (RAG) Systems","text":"<p>Evaluation is one of the hardest and most important aspects of Retrieval-Augmented Generation systems. A RAG pipeline combines multiple components such as retrieval, reranking, and generation, each of which can fail in different ways. As a result, evaluation must be multi-dimensional, covering both component-level and end-to-end behavior.</p>"},{"location":"RAG/evaluation/#1-why-rag-evaluation-is-hard","title":"1. Why RAG Evaluation Is Hard","text":"<p>Unlike traditional NLP tasks, RAG systems: - Do not have a single ground-truth output - Depend on external knowledge sources - Can fail silently by generating fluent but incorrect answers - Have multiple interacting components</p> <p>Because of this, no single metric is sufficient. Effective evaluation requires layered evaluation across retrieval quality, generation quality, and faithfulness to sources.</p>"},{"location":"RAG/evaluation/#2-component-level-vs-end-to-end-evaluation","title":"2. Component-Level vs End-to-End Evaluation","text":""},{"location":"RAG/evaluation/#component-level-evaluation","title":"Component-Level Evaluation","text":"<p>Each module is evaluated independently.</p> <p>Retrieval</p> <ul> <li>Are relevant documents retrieved?</li> <li>Are they ranked correctly?</li> </ul> <p>Generation</p> <ul> <li>Given perfect context, can the model answer correctly?</li> </ul> <p>Pros</p> <ul> <li>Easier to debug failures</li> <li>Clear attribution of errors</li> <li>Allows offline benchmarking</li> </ul> <p>Cons</p> <ul> <li>Does not capture compounding errors</li> <li>May overestimate real-world performance</li> </ul>"},{"location":"RAG/evaluation/#end-to-end-evaluation","title":"End-to-End Evaluation","text":"<p>The full pipeline is evaluated from user query to final answer.</p> <p>Pros</p> <ul> <li>Reflects real user experience</li> <li>Captures interaction effects between components</li> </ul> <p>Cons</p> <ul> <li>Hard to diagnose root causes</li> <li>More expensive and noisy</li> </ul> <p>Note: Strong systems use both, starting with component-level evaluation during development and end-to-end evaluation before deployment.</p>"},{"location":"RAG/evaluation/#3-retrieval-evaluation-metrics","title":"3. Retrieval Evaluation Metrics","text":"<p>Retrieval metrics measure whether the system is able to fetch relevant context.</p>"},{"location":"RAG/evaluation/#31-recallk","title":"3.1 Recall@k","text":"<p>Fraction of queries for which at least one relevant document appears in the top-k retrieved results.</p> <p>Why it matters</p> <ul> <li>If recall is low, generation cannot recover</li> <li>Especially critical for factual question answering</li> </ul> <p>Limitations</p> <ul> <li>Does not consider ranking within top-k</li> <li>Binary notion of relevance</li> </ul>"},{"location":"RAG/evaluation/#32-mean-reciprocal-rank-mrr","title":"3.2 Mean Reciprocal Rank (MRR)","text":"<p>MRR measures how early the first relevant document appears in the ranked list, giving higher scores when the system retrieves a useful document closer to the top.</p> <p>Why it matters</p> <ul> <li>Rewards systems that rank relevant documents earlier</li> <li>Useful when only one document is needed</li> </ul> <p>Limitations</p> <ul> <li>Ignores multiple relevant documents</li> </ul>"},{"location":"RAG/evaluation/#33-normalized-discounted-cumulative-gain-ndcg","title":"3.3 Normalized Discounted Cumulative Gain (nDCG)","text":"<p>nDCG measures how well the system orders documents by relevance, rewarding highly relevant documents appearing early in the ranking and smoothly penalizing them as they appear lower.</p> <p>Why it matters</p> <ul> <li>More realistic for multi-document relevance</li> <li>Penalizes relevant documents appearing lower in the list</li> </ul> <p>Limitations</p> <ul> <li>Requires graded relevance labels</li> <li>More complex to compute and interpret</li> </ul>"},{"location":"RAG/evaluation/#4-generation-quality-metrics","title":"4. Generation Quality Metrics","text":"<p>Generation metrics evaluate the textual quality of the final answer.</p>"},{"location":"RAG/evaluation/#41-exact-match-em","title":"4.1 Exact Match (EM)","text":"<p>Checks if the generated answer exactly matches the ground truth.</p> <p>Pros</p> <ul> <li>Simple and interpretable</li> <li>Useful for factoid questions</li> </ul> <p>Cons</p> <ul> <li>Too strict for natural language generation</li> <li>Sensitive to paraphrasing</li> </ul>"},{"location":"RAG/evaluation/#42-f1-score","title":"4.2 F1 Score","text":"<p>Token-level overlap between generated answer and reference.</p> <p>Pros</p> <ul> <li>More flexible than Exact Match</li> <li>Widely used in question answering benchmarks</li> </ul> <p>Cons</p> <ul> <li>Still surface-level</li> <li>Does not capture semantic correctness fully</li> </ul>"},{"location":"RAG/evaluation/#43-bleu-and-rouge","title":"4.3 BLEU and ROUGE","text":"<p>Use case</p> <ul> <li>Longer-form or summarization-style answers</li> </ul> <p>Limitations</p> <ul> <li>Correlate poorly with factual correctness</li> <li>Can reward fluent but incorrect outputs</li> </ul> <p>Note: These metrics primarily measure fluency, not truthfulness.</p>"},{"location":"RAG/evaluation/#5-faithfulness-and-groundedness-evaluation","title":"5. Faithfulness and Groundedness Evaluation","text":"<p>A core risk in RAG is hallucination, where the model generates content not supported by retrieved documents.</p>"},{"location":"RAG/evaluation/#51-faithfulness","title":"5.1 Faithfulness","text":"<p>Question answered </p> <p>Is the generated answer supported by the retrieved context?</p> <p>Common approaches</p> <ul> <li>LLM-as-a-judge prompting</li> <li>Sentence-level entailment checks</li> <li>Context to answer consistency scoring</li> </ul>"},{"location":"RAG/evaluation/#52-groundedness","title":"5.2 Groundedness","text":"<p>Question answered </p> <p>Does every claim in the answer trace back to a retrieved source?</p> <p>Techniques</p> <ul> <li>Claim extraction followed by source matching</li> <li>Evidence coverage metrics</li> <li>Citation validation</li> </ul> <p>Failure mode</p> <ul> <li>Answers that are correct but unsupported by retrieved documents</li> </ul>"},{"location":"RAG/evaluation/#6-llm-based-evaluation","title":"6. LLM-Based Evaluation","text":"<p>Large language models are increasingly used as evaluators.</p>"},{"location":"RAG/evaluation/#use-cases","title":"Use Cases","text":"<ul> <li>Faithfulness scoring</li> <li>Answer correctness evaluation</li> <li>Relevance of retrieved passages</li> <li>Pairwise comparison of answers</li> </ul>"},{"location":"RAG/evaluation/#benefits","title":"Benefits","text":"<ul> <li>Scales without human labels</li> <li>Captures semantic nuance beyond lexical overlap</li> </ul>"},{"location":"RAG/evaluation/#risks","title":"Risks","text":"<ul> <li>Bias toward fluent answers</li> <li>Sensitivity to prompt design</li> <li>Self-preference when evaluating the same model family</li> </ul> <p>Best practice</p> <ul> <li>Use structured rubrics</li> <li>Validate against human judgments on a held-out set</li> </ul>"},{"location":"RAG/evaluation/#7-human-evaluation-protocols","title":"7. Human Evaluation Protocols","text":"<p>Human evaluation remains the gold standard for RAG systems.</p>"},{"location":"RAG/evaluation/#common-criteria","title":"Common Criteria","text":"<ul> <li>Correctness</li> <li>Completeness</li> <li>Faithfulness</li> <li>Clarity</li> <li>Usefulness</li> </ul>"},{"location":"RAG/evaluation/#protocol-design","title":"Protocol Design","text":"<ul> <li>Blind evaluation</li> <li>Multiple annotators per example</li> <li>Measurement of inter-annotator agreement</li> </ul>"},{"location":"RAG/evaluation/#tradeoffs","title":"Tradeoffs","text":"<ul> <li>High cost</li> <li>Low scalability</li> <li>Slow iteration cycles</li> </ul>"},{"location":"RAG/evaluation/#8-error-analysis-and-failure-modes","title":"8. Error Analysis and Failure Modes","text":"<p>Effective evaluation includes systematic error analysis.</p>"},{"location":"RAG/evaluation/#common-failure-types","title":"Common Failure Types","text":"<ul> <li>Relevant document not retrieved</li> <li>Relevant document retrieved but ignored by the generator</li> <li>Partial hallucinations mixed with correct facts</li> <li>Over-reliance on parametric knowledge</li> <li>Stale, contradictory, or low-quality sources</li> </ul> <p>Practical tip </p> <p>Track failures across queries and cluster them by root cause rather than by metric alone.</p>"},{"location":"RAG/evaluation/#9-dataset-construction-for-rag-evaluation","title":"9. Dataset Construction for RAG Evaluation","text":""},{"location":"RAG/evaluation/#key-challenges","title":"Key Challenges","text":"<ul> <li>Creating reliable relevance labels</li> <li>Defining ground truth answers</li> <li>Handling ambiguous or multi-hop queries</li> </ul>"},{"location":"RAG/evaluation/#dataset-types","title":"Dataset Types","text":"<ul> <li>Synthetic question answering generated from documents</li> <li>Human-authored question answer pairs</li> <li>Real user queries from production logs</li> </ul>"},{"location":"RAG/evaluation/#10-tradeoffs-and-design-choices","title":"10. Tradeoffs and Design Choices","text":"Design Choice Impact High recall retrieval Improves answer coverage but increases noise Aggressive reranking Improves precision but adds latency Strict faithfulness constraints Reduces hallucinations but may lower answer recall"},{"location":"RAG/evaluation/#11-what-interviewers-look-for","title":"11. What Interviewers Look For","text":"<p>Strong candidates can:</p> <ul> <li>Explain why no single metric is sufficient</li> <li>Justify metric choices based on application requirements</li> <li>Discuss tradeoffs between faithfulness and usefulness</li> <li>Describe how evaluation informs system design decisions</li> </ul>"},{"location":"RAG/fundamentals/","title":"Fundamentals","text":""},{"location":"RAG/fundamentals/#retrival-augmented-generation-rag","title":"Retrival Augmented Generation (RAG)","text":""},{"location":"RAG/fundamentals/#1-overview","title":"1. Overview","text":""},{"location":"RAG/fundamentals/#what-problem-does-rag-solve","title":"What Problem Does RAG Solve?","text":"<p>Large Language Models are trained on a fixed snapshot of data and store knowledge implicitly in their parameters. This leads to several limitations:</p>"},{"location":"RAG/fundamentals/#key-limitations-of-vanilla-llms","title":"Key Limitations of Vanilla LLMs","text":"<ol> <li> <p>Knowledge cutoff    Models cannot natively access information created after training.</p> </li> <li> <p>Hallucinations    When the model lacks factual knowledge, it may generate fluent but incorrect answers.</p> </li> <li> <p>Poor domain specificity    General models struggle with enterprise, proprietary, or niche domain data.</p> </li> <li> <p>Cost of knowledge updates    Updating knowledge via retraining or fine-tuning is expensive and slow.</p> </li> </ol>"},{"location":"RAG/fundamentals/#how-rag-addresses-these-issues","title":"How RAG Addresses These Issues","text":"<p>RAG decouples knowledge storage from language generation. Instead of forcing the model to memorize facts, it retrieves relevant external information at inference time and conditions generation on that retrieved context.</p> <p>This allows: - Up-to-date knowledge access - Reduced hallucinations - Grounded and explainable outputs - Faster and cheaper iteration cycles</p>"},{"location":"RAG/fundamentals/#2-rag-vs-fine-tuning-when-does-rag-help","title":"2. RAG vs Fine-Tuning: When Does RAG Help?","text":""},{"location":"RAG/fundamentals/#when-rag-is-the-better-choice","title":"When RAG Is the Better Choice","text":"<p>RAG is preferred when:</p> <ul> <li>Knowledge changes frequently such as policies, documentation, or news.</li> <li>Data is large, unstructured, or proprietary.</li> <li>Source grounding and explainability are important.</li> <li>You want to avoid frequent retraining costs.</li> </ul> <p>Common use cases - Enterprise knowledge assistants - Question answering over PDFs, wikis, and tickets - Customer support bots grounded in internal documents</p>"},{"location":"RAG/fundamentals/#when-fine-tuning-is-the-better-choice","title":"When Fine-Tuning Is the Better Choice","text":"<p>Fine-tuning is preferred when:</p> <ul> <li>You want to change model behavior, not just factual knowledge.</li> <li>The task requires consistent style, tone, or reasoning patterns.</li> <li>The knowledge is stable and relatively small.</li> </ul> <p>Common use cases - Instruction following improvements - Code style adaptation - Domain-specific reasoning patterns</p> <p>In practice, RAG and fine-tuning are often combined. A model may be fine-tuned for instruction following and reasoning, while RAG provides factual grounding.</p>"},{"location":"RAG/fundamentals/#3-high-level-rag-pipeline","title":"3. High-Level RAG Pipeline","text":"<p>A standard RAG system consists of four main stages:</p> <ol> <li>Indexing  </li> <li>Retrieval  </li> <li>Augmentation  </li> <li>Generation  </li> </ol>"},{"location":"RAG/fundamentals/#31-indexing","title":"3.1 Indexing","text":"<p>Indexing prepares external knowledge for efficient retrieval at inference time.</p> <p>Steps in Indexing</p> <ol> <li> <p>Document Ingestion:  Load raw documents such as PDFs, HTML, markdown files, logs, or database records.</p> </li> <li> <p>Chunking: Split documents into smaller passages, typically 200 to 1000 tokens. Chunking is necessary because:</p> </li> <li>Embedding models have fixed input limits</li> <li> <p>Smaller chunks improve retrieval granularity</p> </li> <li> <p>Embedding: Each chunk is converted into a dense vector using an embedding model. For each document chunk \\(d_i\\): $$ \\mathbf{e}i = f{\\text{embed}}(d_i) $$</p> </li> <li> <p>Storage: All embeddings are stored in a vector database or approximate nearest neighbor index.</p> </li> </ol>"},{"location":"RAG/fundamentals/#32-retrieval","title":"3.2 Retrieval","text":"<p>Retrieval happens at query time and selects the most relevant chunks.</p>"},{"location":"RAG/fundamentals/#retrieval-process","title":"Retrieval Process","text":"<ol> <li>Embed the user query \\(q\\) into a vector \\(\\mathbf{e}_q\\)</li> <li>Compute similarity between the query vector and stored document vectors</li> <li>Select the top-k most similar chunks</li> </ol> <p>A common similarity metric is cosine similarity: $$ \\text{score}(q, d_i) = \\frac{\\mathbf{e}_q \\cdot \\mathbf{e}_i}{|\\mathbf{e}_q||\\mathbf{e}_i|} $$</p> <p>Retrieval quality is often the single most important factor in RAG system performance.</p>"},{"location":"RAG/fundamentals/#33-augmentation","title":"3.3 Augmentation","text":"<p>Augmentation injects retrieved content into the model input.</p>"},{"location":"RAG/fundamentals/#common-augmentation-strategies","title":"Common Augmentation Strategies","text":"<ul> <li>Concatenating retrieved chunks before the question</li> <li>Adding separators, titles, or metadata</li> <li>Structuring context using bullet points or citations</li> </ul>"},{"location":"RAG/fundamentals/#34-generation","title":"3.4 Generation","text":"<p>The LLM generates the final response conditioned on both:</p> <ul> <li>The user query</li> <li>The retrieved external context</li> </ul> <p>Formally: $$ P(y \\mid x) = P(y \\mid q, d_{1:k}) $$</p> <p>The model reasons over provided evidence rather than relying solely on its internal parameters.</p>"},{"location":"RAG/fundamentals/#4-failure-modes-of-vanilla-rag","title":"4. Failure Modes of Vanilla RAG","text":""},{"location":"RAG/fundamentals/#41-retrieval-failures","title":"4.1 Retrieval Failures","text":""},{"location":"RAG/fundamentals/#poor-recall","title":"Poor Recall","text":"<p>Relevant documents exist in the corpus but are not retrieved.</p> <p>Common causes - Weak or misaligned embedding model - Poor chunking strategy - Query embedding mismatch with document embeddings</p> <p>Impact - The model generates answers without the required evidence - Hallucinations reappear despite using RAG</p>"},{"location":"RAG/fundamentals/#poor-precision","title":"Poor Precision","text":"<p>Retrieved chunks are irrelevant or only loosely related to the query.</p> <p>Common causes - Overly large chunks - Ambiguous user queries - High top-k retrieval without filtering</p> <p>Impact - Model is distracted by irrelevant context - Answers become vague or incorrect</p>"},{"location":"RAG/fundamentals/#42-chunking-related-failures","title":"4.2 Chunking-Related Failures","text":""},{"location":"RAG/fundamentals/#over-chunking","title":"Over-Chunking","text":"<p>Chunks are too small and lose semantic meaning.</p> <p>Impact - Missing context - Incomplete or misleading information</p>"},{"location":"RAG/fundamentals/#under-chunking","title":"Under-Chunking","text":"<p>Chunks are too large and contain multiple unrelated topics.</p> <p>Impact - Lower retrieval accuracy - Context window pollution</p> <p>Chunk size is a critical hyperparameter and strongly influences retrieval quality.</p>"},{"location":"RAG/fundamentals/#43-context-window-limitations","title":"4.3 Context Window Limitations","text":"<p>Vanilla RAG typically concatenates retrieved chunks directly into the prompt.</p> <p>Failure cases - Retrieved context exceeds the model context window - Important chunks are truncated - Ordering of chunks affects answer quality</p> <p>Impact - Loss of critical evidence - Inconsistent or incomplete answers</p>"},{"location":"RAG/fundamentals/#44-lack-of-reasoning-over-retrieved-evidence","title":"4.4 Lack of Reasoning Over Retrieved Evidence","text":"<p>Vanilla RAG assumes the model will correctly use the retrieved context.</p> <p>Common issues - Model ignores relevant chunks - Model cherry-picks incorrect information - Model blends retrieved facts with hallucinated content</p> <p>Impact - Answers appear grounded but contain subtle errors</p>"},{"location":"RAG/fundamentals/#45-retrieval-augmentation-mismatch","title":"4.5 Retrieval-Augmentation Mismatch","text":"<p>The retrieved information may be correct but poorly integrated into the prompt.</p> <p>Common causes - No clear separation between context and question - Missing instructions to ground answers in retrieved content - No citation or reference constraints</p> <p>Impact - Model treats retrieved text as optional background - Reduced factual consistency</p>"},{"location":"RAG/fundamentals/#46-static-retrieval-strategy","title":"4.6 Static Retrieval Strategy","text":"<p>Vanilla RAG uses a single retrieval step.</p> <p>Limitations - Cannot refine or reformulate queries - No iterative retrieval based on intermediate reasoning - No handling of multi-hop or compositional questions</p> <p>Impact - Poor performance on complex queries - Failure on questions requiring reasoning across documents</p>"},{"location":"RAG/fundamentals/#47-no-verification-or-feedback-loop","title":"4.7 No Verification or Feedback Loop","text":"<p>Vanilla RAG pipelines usually lack validation of retrieved or generated content.</p> <p>Missing components - Answer verification - Retrieval confidence estimation - Self-correction mechanisms</p> <p>Impact - Silent failures - Difficult to debug or evaluate system behavior</p>"},{"location":"RAG/fundamentals/#5-key-takeaways","title":"5. Key Takeaways","text":"<ul> <li>RAG reduces hallucinations but does not eliminate them</li> <li>Retrieval quality is the dominant failure point</li> <li>Chunking and prompt structure are first-order design choices</li> <li>Vanilla RAG struggles with complex, multi-hop reasoning</li> <li>Advanced RAG systems add reranking, query rewriting, verification, and feedback loops</li> </ul> <p>Most real-world RAG systems extend vanilla RAG to explicitly address these failure modes.</p>"},{"location":"RAG/indexing_and_vector_database/","title":"Indexing & Retrieval","text":""},{"location":"RAG/indexing_and_vector_database/#indexing-strategies-vector-databases-and-retrieval-systems-for-rag","title":"Indexing Strategies, Vector Databases, and Retrieval Systems for RAG","text":""},{"location":"RAG/indexing_and_vector_database/#1-why-indexing-and-retrieval-matter-in-rag","title":"1. Why Indexing and Retrieval Matter in RAG","text":"<p>In RAG systems, retrieval is the bottleneck for answer quality. If the retriever fails, even the strongest LLM cannot recover.</p> <p>Indexing and retrieval determine: - What information is even visible to the generator - Latency and cost per query - Scalability to millions or billions of chunks - Robustness to noisy or ambiguous queries</p> <p>A typical RAG retrieval pipeline: 1. Chunk documents 2. Generate embeddings 3. Build indexes 4. Retrieve candidates 5. Re rank or filter 6. Construct final context</p>"},{"location":"RAG/indexing_and_vector_database/#2-indexing-strategies","title":"2. Indexing Strategies","text":"<p>Indexing defines how embeddings or tokens are organized to support fast similarity search.</p>"},{"location":"RAG/indexing_and_vector_database/#21-flat-index","title":"2.1 Flat Index","text":"<p>A flat index stores all vectors and computes similarity against every vector during search.</p>"},{"location":"RAG/indexing_and_vector_database/#pros","title":"Pros","text":"<ul> <li>Exact results: No approximation is used, so recall is perfect. This makes it a reliable reference for evaluating other indexes.</li> <li>Simple implementation: No clustering, graph construction, or parameter tuning is required.</li> <li>Deterministic behavior: Results are stable across runs, which simplifies debugging.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons","title":"Cons","text":"<ul> <li>Does not scale: Search time grows linearly with dataset size, making it unusable beyond small corpora.</li> <li>High latency: Even moderate sized datasets cause unacceptable response times.</li> <li>High compute cost: Requires full distance computation for every query.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#when-to-use","title":"When to use","text":"<ul> <li>Small datasets</li> <li>Offline evaluation</li> <li>Ground truth recall benchmarking</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#22-approximate-nearest-neighbor-indexes","title":"2.2 Approximate Nearest Neighbor Indexes","text":"<p>Approximate methods trade small accuracy loss for large performance gains.</p>"},{"location":"RAG/indexing_and_vector_database/#221-hnsw-hierarchical-navigable-small-world","title":"2.2.1 HNSW (Hierarchical Navigable Small World)","text":"<p>HNSW builds a multi layer graph where each node connects to similar vectors. Search starts at higher layers and progressively refines.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_1","title":"Pros","text":"<ul> <li>Very high recall at low latency: HNSW often achieves near flat index recall with orders of magnitude faster search.</li> <li>Fast query performance: Graph traversal limits the number of distance computations.</li> <li>Supports dynamic updates: New vectors can be inserted without rebuilding the index.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_1","title":"Cons","text":"<ul> <li>High memory usage: Graph edges add significant overhead compared to raw vectors.</li> <li>Slow index construction: Building the graph is expensive for very large datasets.</li> <li>Parameter sensitivity: Poor tuning can increase memory usage or degrade recall.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#best-for","title":"Best for","text":"<ul> <li>Latency sensitive RAG systems</li> <li>Medium to large scale corpora</li> <li>Online and frequently updated data</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#222-ivf-inverted-file-index","title":"2.2.2 IVF (Inverted File Index)","text":"<p>IVF clusters vectors into centroids and searches only within the closest clusters.</p> <p>Note:     In traditional text retrieval, an inverted index maps:     (term \u2192 list of documents containing that term).     This is the opposite of storing documents as sequences of terms, hence the word inverted. At query time, only documents associated with the query terms are examined.</p> <pre><code>In IVF, the mapping becomes:\n- centroid ID \u2192 list of vectors assigned to that centroid\nInstead of scanning all vectors, the system:\n1. Assigns each vector to its nearest centroid during indexing\n2. At query time, finds the closest centroids\n3. Searches only the vectors stored in those centroid \u201clists\u201d\nEach centroid acts like a term in a classical inverted index, and each posting list contains the vectors that belong to that region of the vector space.\n</code></pre>"},{"location":"RAG/indexing_and_vector_database/#pros_2","title":"Pros","text":"<ul> <li>Lower memory overhead: Does not store large graphs, making it more memory efficient.</li> <li>Faster index build: Clustering is cheaper than graph construction.</li> <li>Works well with disk based search: Suitable for large datasets that do not fit fully in memory.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_2","title":"Cons","text":"<ul> <li>Lower recall than HNSW: If relevant vectors fall outside probed clusters, they are missed.</li> <li>Sensitive to clustering quality: Poor centroids lead to degraded retrieval quality.</li> <li>Requires careful tuning: Number of clusters and probes strongly affect performance.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#best-for_1","title":"Best for","text":"<ul> <li>Very large scale datasets</li> <li>Cost constrained systems</li> <li>Disk backed vector stores</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#223-product-quantization-pq","title":"2.2.3 Product Quantization (PQ)","text":"<p>PQ compresses vectors into compact codes and computes approximate distances.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_3","title":"Pros","text":"<ul> <li>Massive memory reduction: Enables storage of billions of vectors.</li> <li>Lower IO cost: Smaller representations reduce disk access.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_3","title":"Cons","text":"<ul> <li>Lossy compression: Precision drops due to quantization errors.</li> <li>Reduced recall: Fine grained similarity distinctions are lost.</li> <li>Harder to debug: Errors are harder to attribute to specific vectors.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#typically-used-with","title":"Typically used with","text":"<ul> <li>IVF + PQ for extreme scale search</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#23-sparse-indexes","title":"2.3 Sparse Indexes","text":"<p>Sparse indexes represent documents using term based features such as TF IDF or BM25. Each document is a high dimensional vector over a vocabulary, where most dimensions are zero.</p> <p>They are implemented using an inverted index, which maps:</p> <p>term \u2192 list of documents containing that term</p> <p>At query time, only documents that share terms with the query are considered, and they are scored using functions like BM25 that account for term frequency, document frequency, and document length.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_4","title":"Pros","text":"<ul> <li>Strong lexical precision: Exact matches for keywords, IDs, and entities.</li> <li>Interpretable scoring: Scores can be explained via term frequency and document frequency.</li> <li>Handles rare terms well: Especially important for names, error codes, and identifiers.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_4","title":"Cons","text":"<ul> <li>Weak semantic understanding: Cannot match paraphrases or conceptual similarity.</li> <li>Vocabulary dependent: Performance degrades when query wording differs from documents.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#common-systems","title":"Common systems","text":"<ul> <li>Elasticsearch</li> <li>OpenSearch</li> <li>Lucene</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#24-hybrid-indexing","title":"2.4 Hybrid Indexing","text":"<p>Hybrid systems combine dense and sparse indexes to exploit complementary strengths.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_5","title":"Pros","text":"<ul> <li>Improved recall and precision: Dense captures semantics, sparse captures exact matches.</li> <li>Robust to query variation: Handles both natural language and keyword heavy queries.</li> <li>Production proven: Widely used in enterprise and legal search.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_5","title":"Cons","text":"<ul> <li>Increased system complexity: Requires managing multiple indexes and score fusion.</li> <li>Higher latency: Multiple retrieval paths increase query cost.</li> <li>Tuning complexity: Weighting dense vs sparse scores is non trivial.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#3-vector-databases","title":"3. Vector Databases","text":"<p>Vector databases manage embedding storage, indexing, and retrieval at scale.</p>"},{"location":"RAG/indexing_and_vector_database/#31-core-capabilities","title":"3.1 Core Capabilities","text":"<p>A production ready vector database supports:</p> <ul> <li>Approximate nearest neighbor search</li> <li>Metadata filtering</li> <li>Index persistence</li> <li>Horizontal scaling</li> <li>Online updates</li> <li>Observability and monitoring</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#32-popular-vector-databases","title":"3.2 Popular Vector Databases","text":""},{"location":"RAG/indexing_and_vector_database/#faiss","title":"FAISS","text":"<p>Pros</p> <ul> <li>Extremely flexible</li> <li>High performance</li> <li>Ideal for research and custom pipelines</li> </ul> <p>Cons</p> <ul> <li>Not a full database</li> <li>Requires significant engineering for production use</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#milvus","title":"Milvus","text":"<p>Pros</p> <ul> <li>Distributed and scalable</li> <li>Supports multiple index types</li> <li>Strong ecosystem</li> </ul> <p>Cons</p> <ul> <li>Operational complexity</li> <li>Requires careful resource management</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#qdrant","title":"Qdrant","text":"<p>Pros</p> <ul> <li>Strong metadata filtering</li> <li>Simple operational model</li> <li>Optimized for RAG workloads</li> </ul> <p>Cons</p> <ul> <li>Less flexible index customization</li> <li>Smaller ecosystem compared to FAISS</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#pinecone","title":"Pinecone","text":"<p>Pros</p> <ul> <li>Fully managed</li> <li>Minimal operational overhead</li> <li>Consistent performance</li> </ul> <p>Cons</p> <ul> <li>Less control over internals</li> <li>Cost can scale quickly</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#33-metadata-filtering","title":"3.3 Metadata Filtering","text":"<p>Metadata filtering restricts retrieval to relevant subsets.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_6","title":"Pros","text":"<ul> <li>Improves precision</li> <li>Enforces access control</li> <li>Reduces irrelevant context</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_6","title":"Cons","text":"<ul> <li>Over filtering can reduce recall</li> <li>Poor filter design can hide relevant documents</li> </ul> <p>Filtering can be applied before or after vector search, each with different tradeoffs.</p>"},{"location":"RAG/indexing_and_vector_database/#4-retrieval-strategies","title":"4. Retrieval Strategies","text":""},{"location":"RAG/indexing_and_vector_database/#41-dense-retrieval","title":"4.1 Dense Retrieval","text":"<p>Dense retrieval embeds queries and documents into low-dimensional dense vectors using neural encoders.</p> <p>Common models:</p> <ul> <li>Sentence Transformers</li> <li>Contriever</li> <li>E5</li> <li>GTR</li> </ul> <p>Retrieval is performed using approximate nearest neighbor search.</p>"},{"location":"RAG/indexing_and_vector_database/#pros_7","title":"Pros","text":"<ul> <li>Captures semantic similarity</li> <li>Robust to paraphrasing</li> <li>Domain adaptable via fine tuning</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_7","title":"Cons","text":"<ul> <li>Weak for exact values</li> <li>Sensitive to embedding quality</li> <li>Harder to interpret scores</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#42-sparse-retrieval","title":"4.2 Sparse Retrieval","text":"<p>Sparse retrieval represents documents and queries using high-dimensional sparse vectors based on term statistics.</p>"},{"location":"RAG/indexing_and_vector_database/#common-methods","title":"Common methods:","text":"<ul> <li>TF-IDF</li> <li>BM25</li> <li>Inverted Indexes</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#pros_8","title":"Pros","text":"<ul> <li>Excellent for exact matches</li> <li>Strong for structured identifiers</li> <li>Interpretable results</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_8","title":"Cons","text":"<ul> <li>Poor semantic recall</li> <li>Fails on natural language variation</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#43-hybrid-retrieval","title":"4.3 Hybrid Retrieval","text":"<p>Hybrid retrieval combines sparse and dense retrieval signals.</p> <p>Common strategies:</p> <ul> <li>Score fusion</li> <li>Result union followed by reranking</li> <li>Weighted linear combination</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#pros_9","title":"Pros","text":"<ul> <li>Best overall retrieval quality</li> <li>Handles diverse query types</li> <li>Reduces failure modes of single retrievers</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_9","title":"Cons","text":"<ul> <li>Higher system complexity</li> <li>Increased latency and cost</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#44-multi-stage-retrieval","title":"4.4 Multi Stage Retrieval","text":""},{"location":"RAG/indexing_and_vector_database/#441-two-stage-retrieval","title":"4.4.1 Two-Stage Retrieval","text":"<p>Typical pipeline:</p> <ol> <li>Fast retriever retrieves top K candidates</li> <li>Reranker refines the list</li> </ol> <p>First stage:</p> <ul> <li>BM25 or dense ANN search</li> </ul> <p>Second stage:</p> <ul> <li>Cross-encoder</li> <li>LLM-based reranker</li> </ul> <p>Reasoning</p> <ul> <li>First stage optimizes recall</li> <li>Second stage optimizes precision</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#442-cross-encoder-reranking","title":"4.4.2 Cross-Encoder Reranking","text":"<p>Cross-encoders jointly encode query and document.</p> <p>Advantages</p> <ul> <li>Rich token-level interactions</li> <li>Higher ranking accuracy</li> </ul> <p>Limitations</p> <ul> <li>Computationally expensive</li> <li>Cannot be applied to large corpora directly</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#45-query-aware-retrieval-techniques","title":"4.5 Query-aware Retrieval Techniques","text":""},{"location":"RAG/indexing_and_vector_database/#451-query-expansion","title":"4.5.1 Query Expansion","text":"<p>Techniques:</p> <ul> <li>Synonym expansion</li> <li>LLM-generated query reformulations</li> <li>Multi-query retrieval</li> </ul> <p>Why it helps</p> <ul> <li>Improves recall</li> <li>Handles ambiguous or underspecified queries</li> </ul> <p>Risk</p> <ul> <li>Query drift</li> <li>Increased noise</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#452-hypothetical-document-embeddings-hyde","title":"4.5.2 Hypothetical Document Embeddings (HyDE)","text":"<p>HyDE generates a hypothetical answer and embeds it for retrieval.</p> <p>Reasoning</p> <ul> <li>The generated answer is closer to relevant documents than the original query</li> <li>Improves semantic alignment</li> </ul> <p>Failure mode</p> <ul> <li>Model hallucination propagates into retrieval</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#5-re-ranking","title":"5. Re Ranking","text":""},{"location":"RAG/indexing_and_vector_database/#51-cross-encoder-re-ranking","title":"5.1 Cross Encoder Re Ranking","text":""},{"location":"RAG/indexing_and_vector_database/#pros_10","title":"Pros","text":"<ul> <li>Very high precision</li> <li>Strong relevance modeling</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_10","title":"Cons","text":"<ul> <li>Computationally expensive</li> <li>Limited to small candidate sets</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#52-llm-based-re-ranking","title":"5.2 LLM Based Re Ranking","text":""},{"location":"RAG/indexing_and_vector_database/#pros_11","title":"Pros","text":"<ul> <li>Flexible reasoning</li> <li>Can incorporate task specific instructions</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#cons_11","title":"Cons","text":"<ul> <li>Expensive</li> <li>Non deterministic</li> <li>Prompt sensitive</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#6-retrieval-evaluation-metrics","title":"6. Retrieval Evaluation Metrics","text":"<p>Common metrics:</p> <ul> <li>Recall@K</li> <li>Precision@K</li> <li>MRR</li> <li>nDCG</li> </ul> <p>Key reasoning</p> <p>High Recall@K is often more important than precision because the LLM can filter irrelevant context better than it can invent missing information.</p>"},{"location":"RAG/indexing_and_vector_database/#7-system-design-tradeoffs","title":"7. System Design Tradeoffs","text":"<ul> <li>Latency vs Recall: Deeper search improves recall but increases response time.</li> <li>Memory vs Accuracy: Compression saves memory at the cost of precision.</li> <li>Static vs Dynamic Data: Frequent updates favor graph based indexes.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#8-common-failure-modes","title":"8. Common Failure Modes","text":"<ul> <li>Poor chunking leading to partial context</li> <li>Embedding mismatch between query and corpus</li> <li>Over aggressive filtering</li> <li>High recall but low precision hurting generation</li> <li>Untuned index parameters</li> </ul>"},{"location":"RAG/supporting_topics/bm25/","title":"BM25","text":""},{"location":"RAG/supporting_topics/bm25/#1-overview","title":"1. Overview","text":"<p>BM25 (Best Matching 25) is a probabilistic ranking function used in information retrieval to score and rank documents given a query. It can be seen as a principled improvement over TF IDF that better handles term frequency saturation and document length normalization. BM25 is widely used in search engines and remains a strong lexical baseline in modern retrieval systems.</p>"},{"location":"RAG/supporting_topics/bm25/#2-intuition-behind-bm25","title":"2. Intuition Behind BM25","text":"<p>TF-IDF assumes that term importance grows linearly with frequency and applies only coarse normalization for document length. In practice, these assumptions are often suboptimal.</p> <p>BM25 improves on this by: - Saturating the contribution of repeated terms - Explicitly normalizing for document length - Grounding the scoring function in probabilistic retrieval theory</p> <p>The result is a ranking function that aligns better with human relevance judgments.</p>"},{"location":"RAG/supporting_topics/bm25/#3-bm25-scoring-formula","title":"3. BM25 Scoring Formula","text":"<p>The BM25 score for a document d and query q is:</p> \\[ \\text{BM25}(d, q) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{     tf(t, d) \\cdot (k_1 + 1) }{     tf(t, d)     + k_1 \\cdot     \\left(         1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}}     \\right) } \\] <p>Where:</p> <ul> <li>\\(t\\) is a query term  </li> <li>\\(tf(t, d)\\) is the term frequency of \\(t\\) in document \\(d\\) </li> <li>\\(|d|\\) is the length of document \\(d\\) </li> <li>\\(\\text{avgdl}\\) is the average document length in the corpus  </li> <li>\\(k_1\\) controls term frequency saturation  </li> <li>\\(b\\) controls document length normalization  </li> <li>\\(\\text{IDF}(t)\\) is the inverse document frequency of term \\(t\\)</li> </ul>"},{"location":"RAG/supporting_topics/bm25/#31-key-components-explained","title":"3.1 Key Components Explained","text":""},{"location":"RAG/supporting_topics/bm25/#term-frequency-saturation","title":"Term Frequency Saturation","text":"<p>Unlike TF IDF, BM25 assumes diminishing returns for repeated terms. The first few occurrences of a term matter most, while additional repetitions contribute progressively less to the score.</p> <p>The parameter \\(k_1\\) controls how quickly this saturation happens.</p>"},{"location":"RAG/supporting_topics/bm25/#document-length-normalization","title":"Document Length Normalization","text":"<p>Longer documents naturally contain more terms and higher raw term counts. BM25 explicitly normalizes scores based on document length to avoid favoring longer documents unfairly.</p> <p>The parameter b controls how strongly document length affects the score: - b = 0 disables length normalization - b = 1 applies full normalization</p>"},{"location":"RAG/supporting_topics/bm25/#inverse-document-frequency","title":"Inverse Document Frequency","text":"<p>BM25 uses a slightly modified IDF formulation that is more robust for rare and common terms:</p> \\[IDF(t) = log((N \u2212 df(t) + 0.5) / (df(t) + 0.5))\\] <p>This formulation stabilizes scores and improves ranking quality.</p>"},{"location":"RAG/supporting_topics/bm25/#4-where-bm25-is-used","title":"4. Where BM25 is Used","text":"<p>BM25 is commonly used in:</p> <ul> <li>Search engines</li> <li>Document retrieval systems</li> <li>Question answering pipelines</li> <li>First stage retrievers in RAG systems</li> <li>Enterprise and web scale search applications</li> </ul> <p>It is often the default choice when dense embeddings are not available or too expensive.</p>"},{"location":"RAG/supporting_topics/bm25/#5-strengths-of-bm25","title":"5. Strengths of BM25","text":"<ul> <li>Strong lexical matching performance</li> <li>Better handling of term frequency than TF IDF</li> <li>Explicit document length normalization</li> <li>No training required</li> <li>Robust across different domains</li> </ul> <p>BM25 often outperforms TF IDF with minimal tuning.</p>"},{"location":"RAG/supporting_topics/bm25/#6-limitations-and-caveats","title":"6. Limitations and Caveats","text":"<ul> <li>Cannot capture semantic similarity</li> <li>Still relies on exact term overlap</li> <li>Sensitive to tokenization and preprocessing</li> <li>Query term independence assumption</li> <li>Requires tuning of k1 and b for optimal performance</li> </ul> <p>BM25 remains a lexical method and inherits the fundamental limitations of bag of words models.</p>"},{"location":"RAG/supporting_topics/bm25/#7-practical-considerations","title":"7. Practical Considerations","text":"<ul> <li>Typical values: k1 in [1.2, 2.0], b around 0.75</li> <li>Stopword handling significantly affects results</li> <li>Works best with cosine or BM25 specific scoring, not raw dot products</li> <li>Often paired with inverted indexes for efficient retrieval</li> </ul> <p>BM25 is computationally efficient and scales well to large corpora.</p>"},{"location":"RAG/supporting_topics/bm25/#8-bm25-vs-tf-idf","title":"8. BM25 vs TF IDF","text":"Aspect TF IDF BM25 Term frequency Linear Saturated Length normalization Weak Explicit Tuning parameters None k1, b Ranking quality Good Better Complexity Low Moderate <p>BM25 can be seen as a more refined and robust version of TF IDF.</p>"},{"location":"RAG/supporting_topics/bm25/#81-an-example","title":"8.1 An Example","text":"<p>Consider a small corpus of three documents and a simple query.</p>"},{"location":"RAG/supporting_topics/bm25/#corpus","title":"Corpus","text":"<ul> <li>D1: \"deep learning deep learning deep learning tutorial\"</li> <li>D2: \"deep learning tutorial\"</li> <li>D3: \"deep learning introduction overview\"</li> </ul>"},{"location":"RAG/supporting_topics/bm25/#query-deep-learning-tutorial","title":"Query : deep learning tutorial","text":""},{"location":"RAG/supporting_topics/bm25/#how-tf-idf-scores-these-documents","title":"How TF IDF Scores These Documents","text":"<pre><code>Key Observation\n\nTF IDF increases linearly with term frequency. Repeating the same term multiple times keeps increasing the score.\n\n### Approximate Behavior\n\n- D1\n    - Very high TF for \"deep\" and \"learning\"\n    - High TF IDF score due to repetition\n- D2\n    - Moderate TF for all query terms\n    - Lower score than D1\n- D3\n    - Missing \"tutorial\"\n    - Lowest score\n\nTF IDF Ranking: D1 &gt; D2 &gt; D3\n\nIssue\nD1 is ranked highest mainly because it repeats \"deep learning\" many times, even though it is not necessarily more relevant to the query than D2.\n\nTF IDF assumes that more repetitions always mean higher relevance.\n</code></pre>"},{"location":"RAG/supporting_topics/bm25/#how-bm25-scores-these-documents","title":"How BM25 Scores These Documents","text":"<pre><code>Key Observation\n\nBM25 applies **term frequency saturation** and **length normalization**.\n\n- The first few occurrences of a term matter most\n- Extra repetitions contribute less\n- Longer documents are penalized\n\nApproximate Behavior\n\n- D1\n    - TF contribution saturates quickly\n    - Longer document length reduces score\n- D2\n    - Balanced term coverage\n    - Shorter document length boosts score\n- D3\n    - Still missing \"tutorial\"\n    - Lowest score\n\nBM25 Ranking: D2 &gt; D1 &gt; D3\n</code></pre>"},{"location":"RAG/supporting_topics/bm25/#82-why-bm25-is-better-than-tf-idf-in-this-example","title":"8.2 Why BM25 Is Better Than TF IDF in This Example","text":""},{"location":"RAG/supporting_topics/bm25/#821-term-frequency-saturation","title":"8.2.1 Term Frequency Saturation","text":"<p>TF IDF: Repeats always increase score</p> <p>BM25: Repeats beyond a point add little value</p> <p>This prevents keyword stuffing from dominating rankings.</p>"},{"location":"RAG/supporting_topics/bm25/#822-document-length-normalization","title":"8.2.2 Document Length Normalization","text":"<p>TF IDF: Weak or implicit normalization</p> <p>BM25: Explicitly penalizes longer documents</p> <p>This avoids favoring verbose documents.</p>"},{"location":"RAG/supporting_topics/bm25/#823-ranking-quality","title":"8.2.3 Ranking Quality","text":"<p>BM25 aligns better with human intuition:</p> <ul> <li>D2 is more concise and directly matches the query</li> <li>D1 is repetitive but not necessarily more informative</li> </ul> <p>Note: BM25 improves over TF IDF by introducing realistic assumptions about term importance. It limits the impact of repeated terms and corrects for document length bias, leading to more reliable and intuitive document rankings in retrieval systems.</p>"},{"location":"RAG/supporting_topics/bm25/#9-bm25-in-modern-retrieval-systems","title":"9. BM25 in Modern Retrieval Systems","text":"<p>BM25 is frequently used as:</p> <ul> <li>A fast first stage retriever</li> <li>A baseline for evaluating dense retrievers</li> <li>A component in hybrid retrieval systems</li> <li>A fallback when embeddings fail</li> </ul> <p>Many state of the art systems combine BM25 with dense retrieval to balance recall and precision.</p>"},{"location":"RAG/supporting_topics/bm25/#10-interview-perspective","title":"10. Interview Perspective","text":"<p>For interviews, emphasize:</p> <ul> <li>Why BM25 improves over TF IDF</li> <li>The role of term frequency saturation</li> <li>The importance of document length normalization</li> <li>Typical parameter choices and their effects</li> <li>How BM25 fits into RAG and hybrid retrieval pipelines</li> </ul> <p>This demonstrates both theoretical understanding and practical retrieval intuition.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/","title":"Sentence Transformer","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#1-overview","title":"1. Overview","text":"<p>Sentence Transformers are neural models designed to convert sentences or short text passages into fixed size dense vectors that capture semantic meaning. They enable efficient semantic similarity, clustering, and retrieval by embedding text into a continuous vector space.</p> <p>They are a foundational component of modern semantic search and RAG systems.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#2-core-idea-behind-sentence-transformers","title":"2. Core Idea Behind Sentence Transformers","text":"<p>Traditional models like BERT produce contextual token embeddings but are not directly suitable for sentence level similarity. Sentence Transformers adapt transformer encoders to produce meaningful sentence level embeddings.</p> <p>The key idea is: - Similar sentences should have embeddings that are close in vector space - Dissimilar sentences should be far apart</p> <p>This is achieved through task specific training objectives.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#3-how-sentence-transformers-work-step-by-step","title":"3. How Sentence Transformers Work: Step by Step","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#step-1-input-text","title":"Step 1: Input Text","text":"<p>Sentence Transformers operate on short texts such as: - Sentences - Paragraphs - Queries and documents</p> <p>Example: \"Neural retrieval improves search quality\"</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-2-transformer-encoding","title":"Step 2: Transformer Encoding","text":"<p>The input text is passed through a transformer encoder such as BERT, RoBERTa, or MiniLM.</p> <p>The model outputs contextualized embeddings for each token in the input.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-3-pooling-to-sentence-embedding","title":"Step 3: Pooling to Sentence Embedding","text":"<p>Token embeddings are aggregated into a single fixed size vector.</p> <p>Common pooling strategies:</p> <ul> <li>Mean pooling across token embeddings</li> <li>CLS token pooling</li> <li>Max pooling</li> </ul> <p>Mean pooling is most commonly used because it is stable and performs well empirically.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-4-normalization","title":"Step 4: Normalization","text":"<p>The resulting sentence embedding is often L2 normalized.</p> <p>This allows cosine similarity to be computed efficiently using dot product.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-5-training-objective","title":"Step 5: Training Objective","text":"<p>Sentence Transformers are trained using contrastive or similarity based losses.</p> <p>Common objectives:</p> <ul> <li>Cosine similarity loss</li> <li>Triplet loss</li> <li>Multiple negative ranking loss</li> </ul> <p>Training encourages semantically similar sentences to have high cosine similarity.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-6-dense-vector-representation","title":"Step 6: Dense Vector Representation","text":"<p>Each sentence is represented as a dense vector.</p> <p>Example: \"Neural retrieval improves search quality\" \u2192 [0.12, -0.03, 0.88, ...]</p> <p>These embeddings capture semantic meaning beyond exact word overlap.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-7-indexing-and-retrieval","title":"Step 7: Indexing and Retrieval","text":"<p>Embeddings are indexed using approximate nearest neighbor methods such as:</p> <ul> <li>FAISS</li> <li>HNSW</li> <li>ScaNN</li> </ul> <p>At query time:</p> <ul> <li>The query is embedded</li> <li>Nearest neighbors are retrieved using cosine similarity or dot product</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#4-why-sentence-transformers-work-well","title":"4. Why Sentence Transformers Work Well","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#41-semantic-matching","title":"4.1 Semantic Matching","text":"<p>They can match paraphrases and synonyms: \"car repair\" \u2248 \"automobile maintenance\"</p> <p>Even without shared tokens.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#42-dense-representations","title":"4.2 Dense Representations","text":"<p>Dense vectors:</p> <ul> <li>Are compact</li> <li>Capture continuous semantic relationships</li> <li>Enable fast similarity search with ANN indexes</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#5-where-sentence-transformers-are-used","title":"5. Where Sentence Transformers Are Used","text":"<ul> <li>Semantic search</li> <li>Dense retrieval for RAG</li> <li>Document clustering</li> <li>Duplicate detection</li> <li>Question answering</li> <li>Reranking and retrieval augmentation</li> </ul> <p>They are widely adopted due to strong performance and ease of use.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#6-strengths-of-sentence-transformers","title":"6. Strengths of Sentence Transformers","text":"<ul> <li>Strong semantic understanding</li> <li>Compact representations</li> <li>Effective for paraphrase matching</li> <li>Pretrained models available</li> <li>Easy integration with vector databases</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#7-limitations-and-caveats","title":"7. Limitations and Caveats","text":"<ul> <li>Poor at exact lexical matching</li> <li>Sensitive to domain shift</li> <li>Require ANN infrastructure</li> <li>Harder to interpret than sparse methods</li> <li>Retrieval errors are harder to debug</li> </ul> <p>Dense retrievers can miss rare but important keywords.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#8-practical-considerations","title":"8. Practical Considerations","text":"<ul> <li>Model choice affects latency and quality</li> <li>Embedding dimensionality impacts storage and speed</li> <li>Fine tuning improves domain specific performance</li> <li>Hybrid retrieval often outperforms pure dense retrieval</li> <li>Reranking improves precision</li> </ul> <p>Sentence Transformers are often used alongside sparse retrievers.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#9-sentence-transformers-vs-sparse-retrieval","title":"9. Sentence Transformers vs Sparse Retrieval","text":"Aspect Sparse (BM25, SPLADE) Sentence Transformers Representation Sparse Dense Semantic matching Limited Strong Interpretability High Low Exact match Strong Weak Infrastructure Inverted index Vector index <p>Hybrid approaches combine both to get the best of both worlds.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#interview-perspective","title":"Interview Perspective","text":"<p>For interviews, focus on:</p> <ul> <li>How pooling creates sentence level embeddings</li> <li>Why contrastive learning is used</li> <li>Tradeoffs between dense and sparse retrieval</li> <li>Failure modes of dense retrievers</li> <li>Their role in RAG and hybrid retrieval systems</li> </ul> <p>This demonstrates practical understanding of modern retrieval architectures.</p>"},{"location":"RAG/supporting_topics/splade/","title":"Splade","text":""},{"location":"RAG/supporting_topics/splade/#1-overview","title":"1. Overview","text":"<p>SPLADE (Sparse Lexical and Expansion Model) is a sparse neural retrieval model that combines the strengths of traditional lexical methods like BM25 with the semantic generalization of neural models. It produces sparse, interpretable representations while enabling semantic term expansion using pretrained language models.</p> <p>SPLADE is especially popular in modern retrieval and RAG systems as a drop in replacement or complement to BM25.</p>"},{"location":"RAG/supporting_topics/splade/#2-core-idea-behind-splade","title":"2. Core Idea Behind SPLADE","text":"<p>Classical sparse retrievers rely on exact term overlap. Dense retrievers capture semantics but lose interpretability and efficiency of inverted indexes.</p> <p>SPLADE bridges this gap by:</p> <ul> <li>Using a transformer encoder to generate sparse vectors</li> <li>Expanding queries and documents with semantically related terms</li> <li>Keeping representations compatible with inverted indexes</li> </ul> <p>Each document and query is represented as a weighted bag of vocabulary terms, where weights are learned rather than hand engineered.</p>"},{"location":"RAG/supporting_topics/splade/#3-how-splade-works","title":"3. How SPLADE Works","text":""},{"location":"RAG/supporting_topics/splade/#step-1-input-text","title":"Step 1: Input Text","text":"<p>SPLADE processes text in the same way as standard transformer models.</p> <p>Example document: \"Neural retrieval models improve search quality\"</p> <p>Example query: \"better search models\"</p> <p>Both documents and queries go through the same pipeline.</p>"},{"location":"RAG/supporting_topics/splade/#step-2-transformer-encoding","title":"Step 2: Transformer Encoding","text":"<p>The input text is passed through a masked language model such as BERT.</p> <p>The model outputs contextualized embeddings for each token. Unlike dense retrieval, SPLADE does not pool these embeddings into a single dense vector.</p> <p>Instead, it projects them into the vocabulary space.</p>"},{"location":"RAG/supporting_topics/splade/#step-3-vocabulary-level-scoring","title":"Step 3: Vocabulary Level Scoring","text":"<p>For each token position, the model produces a score for every term in the vocabulary.</p> <p>Conceptually, the model answers:</p> <p>Which vocabulary terms are relevant to this text, even if they do not appear explicitly?</p> <p>Example high scoring terms for the document might be: neural, retrieval, search, information, ranking, models</p> <p>This is where semantic expansion happens.</p>"},{"location":"RAG/supporting_topics/splade/#step-4-non-linearity-and-sparsification","title":"Step 4: Non Linearity and Sparsification","text":"<p>Raw scores are transformed using:</p> <ul> <li>ReLU to remove negative values</li> <li>Log scaling to control large activations</li> </ul> <p>Then, max pooling across token positions is applied.</p> <p>This results in: - One score per vocabulary term - Many terms having zero weight</p> <p>Only the most relevant terms remain active.</p>"},{"location":"RAG/supporting_topics/splade/#step-5-sparse-vector-representation","title":"Step 5: Sparse Vector Representation","text":"<p>The final output is a sparse vector over the vocabulary.</p> <p>Example representation: { search: 2.1, retrieval: 1.8, neural: 1.5, ranking: 0.9 }</p> <p>This representation:</p> <ul> <li>Is sparse like BM25</li> <li>Encodes semantics like neural models</li> <li>Is interpretable as weighted terms</li> </ul>"},{"location":"RAG/supporting_topics/splade/#step-6-indexing-with-inverted-index","title":"Step 6: Indexing with Inverted Index","text":"<p>Each non-zero term is added to an inverted index.</p> <p>For each term, the index stores:</p> <ul> <li>Document IDs</li> <li>Term weights</li> </ul> <p>This allows fast retrieval using standard sparse indexing infrastructure.</p>"},{"location":"RAG/supporting_topics/splade/#step-7-query-processing","title":"Step 7: Query Processing","text":"<p>Queries go through the exact same steps:</p> <ul> <li>Transformer encoding</li> <li>Vocabulary scoring</li> <li>Sparsification</li> <li>Sparse vector creation</li> </ul> <p>Example query expansion:</p> <p>\"better search models\" \u2192 {search, retrieval, ranking, information}</p>"},{"location":"RAG/supporting_topics/splade/#step-8-retrieval-and-scoring","title":"Step 8: Retrieval and Scoring","text":"<p>Document relevance is computed using a dot product between sparse query and document vectors.</p> <p>Only overlapping non zero terms contribute to the score, making retrieval efficient.</p> <p>Documents with strong lexical or semantic overlap score highest.</p>"},{"location":"RAG/supporting_topics/splade/#why-this-works-better-than-bm25","title":"Why This Works Better Than BM25","text":""},{"location":"RAG/supporting_topics/splade/#learned-term-weights","title":"Learned Term Weights","text":"<p>BM25 uses hand crafted formulas. SPLADE learns which terms matter directly from data.</p>"},{"location":"RAG/supporting_topics/splade/#semantic-expansion-without-dense-vectors","title":"Semantic Expansion Without Dense Vectors","text":"<p>SPLADE can match:</p> <p>query: \"car repair\" document: \"automobile maintenance\"</p> <p>Even without exact word overlap.</p>"},{"location":"RAG/supporting_topics/splade/#sparsity-preserves-efficiency","title":"Sparsity Preserves Efficiency","text":"<p>Despite using transformers:</p> <ul> <li>Storage remains sparse</li> <li>Retrieval uses inverted indexes</li> <li>Latency stays manageable</li> </ul>"},{"location":"RAG/supporting_topics/splade/#mental-model-for-interviews","title":"Mental Model for Interviews","text":"<p>Think of SPLADE as:</p> <p>BM25 where the term weights and expansions are learned by a language model instead of being manually designed.</p>"},{"location":"RAG/supporting_topics/splade/#4-splade-scoring-function","title":"4. SPLADE Scoring Function","text":"<p>For a document or query, SPLADE computes:</p> <p>Score(t) = max over positions of log(1 + ReLU(logit(t)))</p> <p>Key properties:</p> <ul> <li>ReLU enforces non negativity</li> <li>Log scaling controls large activations</li> <li>Max pooling encourages sparse activation</li> </ul> <p>Only a small subset of vocabulary terms receive non-zero weights.</p>"},{"location":"RAG/supporting_topics/splade/#5-learned-term-expansion","title":"5. Learned Term Expansion","text":"<p>Unlike BM25 or TF IDF, SPLADE can assign weight to terms that do not explicitly appear in the text.</p> <p>Example:</p> <ul> <li>Query: \"car repair\"</li> <li>Expanded terms: \"automobile\", \"mechanic\", \"engine\"</li> </ul> <p>This improves recall while preserving lexical matching behavior.</p>"},{"location":"RAG/supporting_topics/splade/#6-where-splade-is-used","title":"6. Where SPLADE Is Used","text":"<p>SPLADE is commonly used in:</p> <ul> <li>Neural information retrieval</li> <li>Hybrid search systems</li> <li>RAG first stage retrieval</li> <li>Enterprise and domain specific search</li> <li>Large scale retrieval where dense vectors are costly</li> </ul> <p>It is especially useful when inverted index compatibility is required.</p>"},{"location":"RAG/supporting_topics/splade/#7-strengths-of-splade","title":"7. Strengths of SPLADE","text":"<ul> <li>Sparse and interpretable representations</li> <li>Semantic expansion improves recall</li> <li>Compatible with inverted indexes</li> <li>Strong performance compared to BM25</li> <li>Lower storage cost than dense embeddings</li> </ul> <p>SPLADE often outperforms BM25 without sacrificing efficiency.</p>"},{"location":"RAG/supporting_topics/splade/#8-limitations-and-caveats","title":"8. Limitations and Caveats","text":"<ul> <li>Requires pretrained transformer models</li> <li>Higher indexing cost than BM25</li> <li>Vocabulary size affects memory usage</li> <li>Inference is slower than classical sparse methods</li> <li>Requires careful regularization to control sparsity</li> </ul> <p>Despite sparsity, SPLADE is still more expensive than purely lexical methods.</p>"},{"location":"RAG/supporting_topics/splade/#9-practical-considerations","title":"9. Practical Considerations","text":"<ul> <li>Regularization is critical to maintain sparsity</li> <li>Index size depends on vocabulary and pruning thresholds</li> <li>Query time expansion increases recall but adds latency</li> <li>Often combined with BM25 or dense retrievers</li> <li>Fine-tuning on domain data improves performance</li> </ul> <p>SPLADE is typically deployed where quality gains justify added complexity.</p>"},{"location":"RAG/supporting_topics/splade/#10-splade-vs-bm25","title":"10. SPLADE vs BM25","text":"Aspect BM25 SPLADE Term weighting Hand crafted Learned Semantic expansion No Yes Sparsity Sparse Sparse Interpretability High Moderate Retrieval quality Strong Stronger <p>SPLADE can be viewed as a neural generalization of BM25.</p>"},{"location":"RAG/supporting_topics/splade/#11-splade-in-modern-rag-pipelines","title":"11. SPLADE in Modern RAG Pipelines","text":"<p>SPLADE is often used as:</p> <ul> <li>A semantic sparse retriever</li> <li>A complement to dense embeddings</li> <li>A first stage retriever before reranking</li> <li>A fallback when dense retrieval misses lexical matches</li> </ul> <p>Hybrid systems frequently combine SPLADE, BM25, and dense retrieval.</p>"},{"location":"RAG/supporting_topics/splade/#12-interview-perspective","title":"12. Interview Perspective","text":"<p>For interviews, emphasize:</p> <ul> <li>Why SPLADE is still sparse despite being neural</li> <li>How learned term expansion improves recall</li> <li>Compatibility with inverted indexes</li> <li>Tradeoffs between BM25, SPLADE, and dense retrieval</li> <li>Its role in modern RAG architectures</li> </ul>"},{"location":"RAG/supporting_topics/tf_idf/","title":"TF_IDF","text":""},{"location":"RAG/supporting_topics/tf_idf/#1-overview","title":"1. Overview","text":"<p>TF IDF (Term Frequency\u2013Inverse Document Frequency) is a classical text representation technique used to convert unstructured text into numerical features. It is widely used in information retrieval and traditional NLP pipelines because of its simplicity, interpretability, and strong lexical matching performance.</p> <p>At a high level, TF IDF assigns higher importance to words that are frequent in a document but rare across the entire corpus. This helps highlight terms that best characterize a document while suppressing common, non informative words.</p>"},{"location":"RAG/supporting_topics/tf_idf/#2-intuition-behind-tf-idf","title":"2. Intuition Behind TF IDF","text":"<p>Not all words in a document are equally useful. Words like \"the\", \"is\", or \"and\" appear frequently but contribute little to meaning. In contrast, domain specific terms often appear repeatedly within a document but infrequently across the corpus.</p> <p>TF IDF captures this intuition by combining two signals: - How important a word is within a document - How unique that word is across documents</p> <p>Only terms that satisfy both conditions receive high scores.</p>"},{"location":"RAG/supporting_topics/tf_idf/#3-term-frequency-tf","title":"3. Term Frequency (TF)","text":"<p>Term Frequency measures how often a term appears in a document.</p> <p>A common formulation is:</p> <p>TF(t, d) = count(t in d) / total number of terms in d</p> <p>TF increases with repeated occurrence of a term in the same document, reflecting its local importance. Variants such as logarithmic TF are sometimes used to reduce the impact of very frequent terms.</p>"},{"location":"RAG/supporting_topics/tf_idf/#4-inverse-document-frequency-idf","title":"4. Inverse Document Frequency (IDF)","text":"<p>Inverse Document Frequency measures how rare a term is across the corpus.</p> <p>A common formulation is:</p> \\[IDF(t) = log(N / (1 + df(t)))\\] <p>Where:</p> <ul> <li>\\(N\\) is the total number of documents</li> <li>\\(df(t)\\) is the number of documents containing the term</li> </ul> <p>Rare terms receive higher IDF values, while common terms receive lower values. Smoothing is often applied to avoid division by zero.</p>"},{"location":"RAG/supporting_topics/tf_idf/#5-tf-idf-score","title":"5. TF-IDF Score","text":"<p>The final TF-IDF score is the product of TF and IDF:</p> <p>TF IDF(t, d) = TF(t, d) \u00d7 IDF(t)</p> <p>A high TF IDF score indicates a term that is both important to the document and discriminative across the corpus.</p>"},{"location":"RAG/supporting_topics/tf_idf/#6-how-tf-idf-is-used","title":"6. How TF-IDF is Used","text":"<p>TF IDF is commonly used in:</p> <ul> <li>Search engines and information retrieval systems</li> <li>Document similarity and clustering using cosine similarity</li> <li>Keyword extraction</li> <li>Text classification as a baseline feature representation</li> <li>Sparse retrieval components in RAG pipelines</li> </ul> <p>Despite the rise of dense embeddings, TF IDF remains a strong baseline and is often combined with neural retrievers.</p>"},{"location":"RAG/supporting_topics/tf_idf/#7-strengths-of-tf-idf","title":"7. Strengths of TF IDF","text":"<ul> <li>Simple and easy to implement</li> <li>Does not require labeled data or training</li> <li>Computationally efficient</li> <li>Produces interpretable feature weights</li> <li>Works well for exact and partial lexical matches</li> </ul> <p>These properties make TF IDF attractive for fast retrieval and baseline systems.</p>"},{"location":"RAG/supporting_topics/tf_idf/#8-limitations-and-caveats","title":"8. Limitations and Caveats","text":"<p>TF IDF has several important limitations:</p> <ul> <li>It ignores word order and syntactic structure</li> <li>It cannot capture semantic similarity or paraphrases</li> <li>Vocabulary size can grow very large</li> <li>Performance depends heavily on preprocessing choices</li> <li>Sparse high dimensional vectors increase memory usage</li> </ul> <p>Because of these limitations, TF IDF performs poorly when semantic understanding is required.</p>"},{"location":"RAG/supporting_topics/tf_idf/#9-practical-considerations","title":"9. Practical Considerations","text":"<ul> <li>Stopword removal and normalization significantly affect performance</li> <li>Cosine similarity is preferred over raw dot product</li> <li>IDF smoothing improves robustness</li> <li>Stemming or lemmatization can reduce sparsity</li> <li>TF-IDF is sensitive to spelling and tokenization errors</li> </ul> <p>Careful preprocessing is often more important than the exact TF-IDF formula.</p>"},{"location":"RAG/supporting_topics/tf_idf/#10-tf-idf-in-modern-systems","title":"10. TF IDF in Modern Systems","text":"<p>In modern retrieval systems, TF IDF is often used as:</p> <ul> <li>A fast first stage retriever</li> <li>A lexical complement to dense embeddings</li> <li>A baseline for evaluating neural retrievers</li> </ul> <p>Hybrid retrieval systems frequently combine TF IDF or BM25 with dense vector search to balance precision and recall.</p>"},{"location":"RAG/supporting_topics/tf_idf/#interview-perspective","title":"Interview Perspective","text":"<p>For interviews, focus on:</p> <ul> <li>The intuition behind IDF and why it matters</li> <li>Why cosine similarity is commonly used</li> <li>When TF-IDF works well and when it fails</li> <li>How it compares to neural embeddings</li> <li>Its role in retrieval augmented generation systems</li> </ul>"},{"location":"context_engineering/ce/","title":"Context Engineering","text":""},{"location":"context_engineering/ce/#paged-attention","title":"Paged Attention","text":""}]}