{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAG &amp; Context Engineering concepts","text":""},{"location":"references/","title":"RAG &amp; Context Engineering concepts","text":""},{"location":"RAG/advanced_rag/","title":"Advanced RAG","text":""},{"location":"RAG/advanced_rag/#1-overview","title":"1. Overview","text":"<p>Vanilla RAG fails on complex queries, multi-hop reasoning, and ambiguous questions. Advanced techniques address these limitations by improving the query before retrieval, the retrieval process itself, or how context is used during generation.</p>"},{"location":"RAG/advanced_rag/#2-query-transformation","title":"2. Query Transformation","text":""},{"location":"RAG/advanced_rag/#query-rewriting","title":"Query Rewriting","text":"<p>Use an LLM to rephrase the original query before retrieval. Useful when the user's question is ambiguous, too short, or uses different vocabulary than the corpus.</p> <p>Example:</p> <ul> <li>Original: \"What's the deal with transformer attention?\"</li> <li>Rewritten: \"How does the self-attention mechanism work in transformer neural networks?\"</li> </ul> <p>The rewritten query is typically longer, more specific, and uses terminology that better matches document text.</p>"},{"location":"RAG/advanced_rag/#multi-query-retrieval","title":"Multi-Query Retrieval","text":"<p>Generate multiple reformulations of the query, retrieve documents for each, and merge the result sets (deduplicated). Improves recall by covering different phrasings and sub-questions within the original query.</p> <p>Example sub-queries from \"How does RAG reduce hallucinations?\":</p> <ul> <li>\"What causes hallucinations in large language models?\"</li> <li>\"How does grounding in retrieved documents improve factual accuracy?\"</li> <li>\"What is the role of retrieval in RAG systems?\"</li> </ul> <p>Each sub-query may retrieve different relevant documents, increasing total coverage.</p>"},{"location":"RAG/advanced_rag/#hyde-hypothetical-document-embeddings","title":"HyDE (Hypothetical Document Embeddings)","text":"<p>Use an LLM to generate a hypothetical ideal answer to the query, embed that answer, and retrieve real documents similar to it.</p> <p>Intuition: A generated answer lives in the same vector space as actual documents (dense, fluent text), bridging the vocabulary gap between a short query and longer document text. Short queries and long documents often have low cosine similarity even when semantically matched.</p> <p>Workflow:</p> <pre><code>Query: \"How does attention mechanism work?\"\n  \u2502\n  \u25bc\nLLM generates hypothetical answer:\n\"The attention mechanism allows each token to attend to all other tokens\n in the sequence by computing query, key, and value matrices...\"\n  \u2502\n  \u25bc\nEmbed hypothetical answer \u2192 retrieve real documents most similar to it\n</code></pre> <ul> <li>Use when: Standard retrieval misses relevant documents because query phrasing differs from document text.</li> <li>Risk: If the LLM hallucinates in the hypothetical answer, those errors are encoded into the retrieval vector and may retrieve wrong or misleading documents.</li> </ul>"},{"location":"RAG/advanced_rag/#step-back-prompting","title":"Step-Back Prompting","text":"<p>For specific, detailed queries, ask the LLM to first generate a more general \"step-back\" question, retrieve documents for the general question, then answer the specific question using that broader context.</p> <p>Example:</p> <ul> <li>Specific: \"What was the GDP growth rate of Germany in Q3 2023?\"</li> <li>Step-back: \"What factors drive GDP growth in European economies?\"</li> </ul> <p>Retrieving for the broader question surfaces background context the specific question might not directly match.</p>"},{"location":"RAG/advanced_rag/#3-advanced-retrieval-architectures","title":"3. Advanced Retrieval Architectures","text":""},{"location":"RAG/advanced_rag/#parent-document-retrieval","title":"Parent-Document Retrieval","text":"<p>Index small chunks for high-precision retrieval, but when a small chunk is retrieved, expand the context by also returning its parent chunk (or surrounding window).</p> <p>Why it helps: Resolves the tension between retrieval precision (small chunks retrieve exactly the right passage) and context completeness (the LLM needs surrounding text to fully understand that passage).</p> <p>Implementation:</p> <ol> <li>Index small chunks (100\u2013200 tokens) for retrieval.</li> <li>Store a mapping from each small chunk to its parent chunk (400\u2013800 tokens).</li> <li>At retrieval time, retrieve small chunks, then substitute their parent chunks in the prompt.</li> </ol>"},{"location":"RAG/advanced_rag/#iterative-recursive-retrieval","title":"Iterative / Recursive Retrieval","text":"<p>Perform multiple rounds of retrieval, where the LLM's intermediate reasoning at each step guides the next query. Particularly useful for multi-hop questions that require chaining information across multiple documents.</p> <p>Example for \"Who was the mentor of the founder of DeepMind?\":</p> <ol> <li>Retrieve: \"founder of DeepMind\" \u2192 learn it's Demis Hassabis.</li> <li>Retrieve: \"Demis Hassabis mentor\" \u2192 learn the answer.</li> </ol> <p>Vanilla RAG would try to answer this in one shot and likely fail.</p>"},{"location":"RAG/advanced_rag/#self-rag","title":"Self-RAG","text":"<p>Train the LLM to decide dynamically whether retrieval is needed, evaluate the relevance of retrieved documents, and assess whether the generated output is supported by evidence \u2014 all using special reflection tokens generated inline.</p> <p>Reflection tokens:</p> <ul> <li><code>[Retrieve]</code> \u2014 should retrieval happen?</li> <li><code>[IsRel]</code> \u2014 is the retrieved document relevant?</li> <li><code>[IsSup]</code> \u2014 is the generated text supported by retrieved evidence?</li> <li><code>[IsUse]</code> \u2014 is the overall response useful?</li> </ul> <p>This produces more selective and grounded outputs than standard RAG at the cost of more complex training.</p>"},{"location":"RAG/advanced_rag/#rag-fusion","title":"RAG Fusion","text":"<p>Combines multi-query generation with hybrid retrieval and fusion:</p> <ol> <li>Generate N reformulations of the query.</li> <li>Run dense + sparse retrieval for each reformulation.</li> <li>Fuse all result sets with Reciprocal Rank Fusion.</li> <li>Pass the merged, deduplicated document set to the LLM.</li> </ol> <p>Addresses both query ambiguity (via multi-query) and retrieval coverage (via hybrid search) simultaneously.</p>"},{"location":"RAG/advanced_rag/#graph-rag","title":"Graph RAG","text":"<p>Build a knowledge graph from documents (entities and relations), then retrieve by traversing the graph rather than pure vector similarity.</p> <ul> <li>Strength: Excellent for multi-hop reasoning and questions about relationships between entities.</li> <li>Weakness: Expensive to build and maintain; requires entity extraction and relation parsing.</li> </ul>"},{"location":"RAG/advanced_rag/#4-context-window-management","title":"4. Context Window Management","text":"<p>Even after good retrieval, how you pack documents into the context window matters significantly.</p>"},{"location":"RAG/advanced_rag/#reorder-for-primacy-recency","title":"Reorder for Primacy / Recency","text":"<p>Place the most relevant documents at the beginning and end of the context window, not the middle \u2014 to mitigate the lost-in-the-middle effect. Less relevant documents go in the middle.</p>"},{"location":"RAG/advanced_rag/#context-compression","title":"Context Compression","text":"<p>Use an LLM to extract only the sentences directly relevant to the query from each retrieved document before packing them into the prompt. Reduces noise and context window usage significantly.</p> <p>Example tools: LLMLingua, Selective Context, RECOMP.</p>"},{"location":"RAG/advanced_rag/#citation-constrained-generation","title":"Citation-Constrained Generation","text":"<p>Instruct the LLM to cite specific retrieved passages in its answer:</p> <pre><code>\"Answer the question using only the provided context. \n For each claim, cite the source chunk by number [1], [2], etc.\n If the answer cannot be found in the context, say 'I don't know'.\"\n</code></pre> <p>Forces grounding and makes hallucinations detectable.</p>"},{"location":"RAG/advanced_rag/#prompt-structure","title":"Prompt Structure","text":"<ul> <li>Place the system instruction first.</li> <li>Number each retrieved chunk for easy citation.</li> <li>Place the query last (most recent in the context window).</li> </ul>"},{"location":"RAG/advanced_rag/#5-the-gold-standard-production-pipeline","title":"5. The \"Gold Standard\" Production Pipeline","text":"<p>Combining the best techniques from across this guide:</p> <pre><code>1. Query rewriting / expansion\n   \u2514\u2500 LLM rewrites the query; optionally generates a HyDE response.\n\n2. Hybrid retrieval (parallel)\n   \u251c\u2500 Dense vector search (bi-encoder + ANN index)\n   \u2514\u2500 Sparse search (BM25 or SPLADE + inverted index)\n\n3. Score fusion\n   \u2514\u2500 Reciprocal Rank Fusion (RRF) merges ranked lists\n\n4. Reranking\n   \u2514\u2500 Cross-encoder re-scores top 50\u2013100 candidates\n\n5. Context selection &amp; ordering\n   \u2514\u2500 Top 5\u201310 documents, ordered most-relevant first/last\n\n6. Citation-constrained generation\n   \u2514\u2500 LLM instructed to ground answer in retrieved text and cite sources\n</code></pre>"},{"location":"RAG/advanced_rag/#6-interview-questions","title":"6. Interview Questions","text":"<p>Q: What is the difference between vanilla RAG and Self-RAG?</p> <p>A: In vanilla RAG, retrieval always happens and the LLM always uses the retrieved context regardless of quality. Self-RAG trains the LLM to decide dynamically whether retrieval is needed, to evaluate whether retrieved documents are relevant, and to assess whether its own output is supported by evidence. This produces more selective, higher-quality responses at the cost of more complex training and a specialised model.</p> <p>Q: How would you handle a multi-hop question like \"Who was the mentor of the person who founded company X?\"</p> <p>A: This requires reasoning across at least two documents. Approaches: (1) iterative retrieval \u2014 retrieve for \"founder of X\", extract the name, then retrieve for \"mentor of [name]\"; (2) query decomposition \u2014 break the question into sub-queries and chain their answers; (3) Graph RAG \u2014 build a knowledge graph and traverse it. Vanilla RAG typically fails on these without explicit multi-hop support, as the single-shot query doesn't match any single document.</p> <p>Q: What is RAG Fusion and how does it differ from simple hybrid retrieval?</p> <p>A: RAG Fusion generates multiple reformulations of the query (not just uses multiple retrieval methods), retrieves documents for each reformulation separately, and fuses all result sets with RRF. Simple hybrid retrieval uses multiple methods (dense + sparse) on the same query. RAG Fusion additionally handles query ambiguity and vocabulary mismatch by diversifying the queries themselves.</p> <p>Q: How does HyDE differ from query rewriting?</p> <p>A: Query rewriting produces a better-phrased version of the user's original question \u2014 still a question. HyDE generates a full hypothetical answer \u2014 a dense passage of text that looks like the kind of document you want to retrieve. The key insight is that a passage is much closer in vector space to other passages than a short question is, even if they're about the same topic.</p> <p>Q: When would context compression be worth the extra latency?</p> <p>A: When retrieved documents are long and noisy, and the LLM's context window is a bottleneck. For example, if retrieving 10 chunks of 500 tokens each fills most of a 4k context window, compressing each chunk to 100 tokens of truly relevant content gives you room to include more documents and reduces the chance the model gets confused by irrelevant text. The added LLM call for compression typically costs less than the degradation in answer quality from noisy context.</p>"},{"location":"RAG/chunking/","title":"Chunking","text":""},{"location":"RAG/chunking/#1-overview","title":"1. Overview","text":"<p>Chunking splits documents into smaller units before embedding and indexing. It is a critical design choice because it directly determines retrieval granularity, context relevance, latency, and cost. Poor chunking is one of the easiest ways to silently break a RAG system.</p>"},{"location":"RAG/chunking/#2-why-chunks-must-be-sized-carefully","title":"2 Why Chunks Must Be Sized Carefully","text":"<p>Embedding models have fixed input limits (typically 512 to 8192 tokens), so documents must be split. But chunk size affects quality in both directions:</p> <ul> <li>Too small: Chunks lose semantic meaning; retrieval returns fragments that don't fully answer the question.</li> <li>Too large: Chunks contain multiple unrelated topics; embedding quality degrades; context window fills with noise.</li> </ul>"},{"location":"RAG/chunking/#3-chunking-strategies","title":"3. Chunking Strategies","text":""},{"location":"RAG/chunking/#fixed-size-chunking","title":"Fixed-Size Chunking","text":"<p>Documents are split into consecutive windows of N tokens with optional overlap. Simple and fast, but ignores semantic boundaries.</p> <p>Algorithm: 1. Tokenise the document. 2. Split into consecutive windows of size N. 3. Optionally overlap adjacent windows by M tokens.</p> <ul> <li>Pros: Simple to implement; fast and scalable; good baseline.</li> <li>Cons: Ignores semantic boundaries; may split sentences mid-thought.</li> <li>Use when: Baseline systems; uniform document formats; large-scale indexing where simplicity matters.</li> </ul>"},{"location":"RAG/chunking/#sentence-based-chunking","title":"Sentence-Based Chunking","text":"<p>Documents are split at sentence boundaries, accumulating sentences until a token threshold is reached.</p> <ul> <li>Pros: Preserves sentence semantics; reduces mid-sentence splits.</li> <li>Cons: Sentence lengths vary; ignores higher-level document structure.</li> <li>Use when: Narrative text; QA over articles or reports.</li> </ul>"},{"location":"RAG/chunking/#paragraph-based-chunking","title":"Paragraph-Based Chunking","text":"<p>Chunks are formed at paragraph boundaries and merged if small; large paragraphs are split further if needed.</p> <ul> <li>Pros: Preserves local topical coherence; aligns with human-written structure.</li> <li>Cons: Paragraph length is highly inconsistent; formatting noise can affect quality.</li> <li>Use when: Well-structured documentation; markdown or HTML content.</li> </ul>"},{"location":"RAG/chunking/#recursive-chunking","title":"Recursive Chunking","text":"<p>Applies a hierarchy of split rules \u2014 sections \u2192 paragraphs \u2192 sentences \u2192 fixed-size fallback \u2014 only falling back to finer splits when the chunk exceeds the size limit.</p> <ul> <li>Pros: Preserves document structure; produces semantically meaningful chunks; handles diverse formats.</li> <li>Cons: More complex to implement; requires reliable document parsing.</li> <li>Use when: Enterprise documents; PDFs with headings; mixed-format content. This is the most common production approach.</li> </ul>"},{"location":"RAG/chunking/#semantic-context-aware-chunking","title":"Semantic / Context-Aware Chunking","text":"<p>Adjacent text units are grouped based on embedding similarity rather than fixed boundaries.</p> <ul> <li>Pros: High semantic coherence; reduces context fragmentation.</li> <li>Cons: Computationally expensive \u2014 requires embedding during preprocessing; sensitive to similarity thresholds.</li> <li>Use when: High-precision RAG; smaller corpora where quality matters most.</li> </ul>"},{"location":"RAG/chunking/#sliding-window-chunking","title":"Sliding Window Chunking","text":"<p>Overlapping windows slide across the document (e.g., 512-token window, 256-token stride).</p> <ul> <li>Pros: Preserves cross-boundary context; reduces information loss at chunk edges.</li> <li>Cons: Doubles or more the index size; higher storage and retrieval cost; redundant embeddings.</li> <li>Use when: Long-form documents; multi-hop reasoning tasks; cases where boundary loss is critical.</li> </ul>"},{"location":"RAG/chunking/#4-chunk-size-and-top-k-are-coupled","title":"4 Chunk Size and Top-k Are Coupled","text":"<p>Changing chunk size almost always requires adjusting top-k. They must be tuned jointly.</p> Chunk Size Typical Top-k Behaviour Small (100\u2013300 tokens) High (10\u201320) High recall, lower precision \u2014 many fragments retrieved Medium (300\u2013700 tokens) Medium (4\u20138) Balanced \u2014 good default starting point Large (700\u20131500 tokens) Low (1\u20133) High precision, risk of missing relevant info <p>Common failure patterns:</p> <ul> <li>Small chunks + low top-k \u2192 missing required information</li> <li>Large chunks + high top-k \u2192 context overload and noise</li> <li>Large chunks + low top-k \u2192 partial coverage</li> </ul>"},{"location":"RAG/chunking/#5-chunk-overlap","title":"5 Chunk Overlap","text":"<p>Overlap (sharing tokens between adjacent chunks) prevents information loss at chunk boundaries.</p> <p>Typical settings:</p> <ul> <li>Fixed-size chunking: 10\u201320% overlap</li> <li>Sliding window: stride equals 50% of window size</li> <li>Recursive chunking: overlap often unnecessary</li> </ul> <p>Benefits: Improved recall; reduced boundary effects. Costs: Larger index; higher storage and retrieval cost; redundant embeddings.</p> <p>Overlap is a mitigation strategy, not a substitute for good chunking design.</p>"},{"location":"RAG/chunking/#6-chunk-metadata-and-filtering","title":"6 Chunk Metadata and Filtering","text":"<p>Attaching structured metadata to each chunk enables filtering before or after similarity search \u2014 one of the highest-ROI improvements in a vanilla RAG system.</p> <p>Common metadata fields: document ID, section heading, timestamp/version, author, content type, access permissions.</p> <p>How it's used:</p> <ul> <li>Pre-filter by document type, date, or access permission before ANN search (faster, but risks reducing recall if over-filtered).</li> <li>Post-filter after ANN search (preserves recall, wastes compute on irrelevant candidates).</li> </ul> <p>Example: Retrieve only chunks from documents created after a certain date, or from a specific product version.</p>"},{"location":"RAG/chunking/#7-special-cases-tables-and-code","title":"7 Special Cases: Tables and Code","text":"<p>Text-centric chunking destroys the structure of tables and source code.</p> <p>Tables:</p> <ul> <li>Never split a table row across chunks.</li> <li>Attach the table schema and column headers as metadata to every row-chunk.</li> <li>Consider serialising rows to natural language for embedding.</li> </ul> <p>Code:</p> <ul> <li>Chunk at function or class boundaries \u2014 never split a function across chunks.</li> <li>File-level chunking for small files is acceptable.</li> <li>Long-range dependencies mean that smaller granularity (line-level) loses context.</li> </ul>"},{"location":"RAG/chunking/#8-adaptive-chunking","title":"8 Adaptive Chunking","text":"<p>Different queries require different granularity. A single static chunking strategy cannot optimally serve all query types.</p> Query Type Preferred Chunking Fact lookup Small chunks Concept explanation Medium chunks Procedural steps Large chunks Multi-hop reasoning Overlapping or sliding window <p>Adaptive approach: Maintain multiple indexes with different chunk sizes and select based on query classification. Higher accuracy, but more system complexity.</p>"},{"location":"RAG/chunking/#9-interview-questions","title":"9 Interview Questions","text":"<p>Q: How do you choose the right chunk size?</p> <p>A: Consider three factors: (1) the embedding model's effective range \u2014 some models degrade with very long inputs; (2) the user's query style \u2014 short factual queries need small chunks, summary queries need larger ones; (3) the \"lost in the middle\" phenomenon \u2014 LLMs often miss information buried in the middle of long context windows. Start with medium chunks (400\u2013600 tokens) and evaluate retrieval recall before optimising.</p> <p>Q: What is the \"lost in the middle\" problem?</p> <p>A: LLMs tend to pay more attention to content at the beginning and end of their context window, and less to content in the middle. This means that if you inject many large chunks into the prompt, relevant information buried in the middle may be ignored. Solutions include re-ordering retrieved chunks (most relevant first/last) and using smaller chunks with higher top-k.</p> <p>Q: Why is overlap used, and what are its costs?</p> <p>A: Overlap ensures that information near chunk boundaries appears in at least one chunk in its full context. The cost is index bloat: overlapping chunks produce redundant embeddings, increasing storage and retrieval compute. Overlap is a safety net, not a primary strategy.</p> <p>Q: How would you chunk a large PDF with tables and diagrams?</p> <p>A: Parse the PDF with a structure-aware tool to extract text, tables, and metadata separately. Chunk the text recursively. Serialise tables (e.g., to CSV or natural language rows) and attach the table schema as metadata. For diagrams, use a vision model to generate textual descriptions, then chunk those. Tag each chunk with its source section and content type for filtered retrieval.</p> <p>Q: What happens if you use the wrong chunk size for your embedding model?</p> <p>A: If chunks exceed the model's effective input window, the encoder either truncates them (losing information) or the quality of the embedding degrades because the model cannot attend properly to all tokens. If chunks are much smaller than what the model is trained on, you lose context and the embedding may not capture full meaning. Always check your embedding model's recommended input range.</p>"},{"location":"RAG/embedding/","title":"Embedding","text":""},{"location":"RAG/embedding/#1-overview","title":"1. Overview","text":"<p>Embeddings are fixed-length dense vectors that encode the semantic meaning of text. In RAG, embeddings are generated for both document chunks (at index time) and user queries (at retrieval time), and similarity between those vectors determines what gets retrieved.</p>"},{"location":"RAG/embedding/#2-dense-sparse-and-hybrid-embeddings","title":"2. Dense, Sparse, and Hybrid Embeddings","text":""},{"location":"RAG/embedding/#dense-embeddings","title":"Dense Embeddings","text":"<p>Text is mapped into a continuous low-dimensional vector space (typically 256\u20134096 dimensions) using a neural encoder. Similar texts end up near each other regardless of exact word overlap.</p> <ul> <li>Pros: Captures paraphrases and semantic equivalence; efficient ANN search; handles natural language queries well.</li> <li>Cons: Weak at exact keyword matching; sensitive to domain shift; less interpretable.</li> <li>Examples: Sentence-BERT, E5, GTE, OpenAI text-embedding models.</li> </ul>"},{"location":"RAG/embedding/#sparse-embeddings","title":"Sparse Embeddings","text":"<p>Text is represented as a high-dimensional sparse vector over a vocabulary (most dimensions are zero). Non-zero weights correspond to terms that appear in the document or query.</p> <ul> <li>Pros: Excellent exact-match recall for keywords, IDs, product codes; interpretable; robust for rare terms.</li> <li>Cons: No semantic generalisation; fails on paraphrases; vocabulary-dependent.</li> <li>Examples: TF-IDF, BM25, SPLADE.</li> </ul>"},{"location":"RAG/embedding/#hybrid-embeddings","title":"Hybrid Embeddings","text":"<p>Hybrid approaches combine both signals. Three common strategies:</p> <ul> <li>Late fusion (RRF): Run dense and sparse search independently, then merge ranked lists using Reciprocal Rank Fusion. Called \"late\" because merging happens after both retrievals complete.</li> <li>Two-stage retrieval: Sparse search narrows millions of documents to a few hundred candidates; a dense model re-scores that shortlist. Balances efficiency and accuracy.</li> <li>Joint sparse-dense (SPLADE-style): A single model produces vectors containing both semantic signals and lexical importance weights \u2014 one unified search.</li> </ul>"},{"location":"RAG/embedding/#3-bi-encoders-vs-cross-encoders","title":"3. Bi-Encoders vs. Cross-Encoders","text":"<p>This is the most important architectural distinction in embedding-based retrieval and a near-universal interview topic.</p>"},{"location":"RAG/embedding/#bi-encoders-dual-encoders","title":"Bi-Encoders (Dual Encoders)","text":"<p>Query and document are encoded independently into vectors. Similarity is a dot product or cosine distance. Because document embeddings are pre-computed and stored, retrieval is O(log N) with ANN indexes.</p> <pre><code>s(q, d) = \u27e8f(q), g(d)\u27e9\n</code></pre> <ul> <li>Strengths: Extremely fast at scale; indexable; supports millions of documents.</li> <li>Weakness: No cross-attention between query and document \u2014 limited relevance modelling.</li> <li>Used for: First-stage retrieval (recall-optimised).</li> </ul>"},{"location":"RAG/embedding/#cross-encoders","title":"Cross-Encoders","text":"<p>Query and document are concatenated and passed through a transformer together. Full self-attention across both texts allows every query token to attend to every document token.</p> <pre><code>s(q, d) = h([q ; d])\n</code></pre> <ul> <li>Strengths: Rich token-level interactions; substantially more accurate relevance scoring.</li> <li>Weakness: Cannot be pre-computed or indexed \u2014 one full forward pass per query-document pair. Too slow for large corpora.</li> <li>Used for: Second-stage reranking over a small candidate set (precision-optimised).</li> </ul> <p>Bi-encoders maximise recall at scale. Cross-encoders maximise precision on a shortlist. Production systems use both in sequence.</p>"},{"location":"RAG/embedding/#late-interaction-colbert","title":"Late Interaction \u2014 ColBERT","text":"<p>A middle ground between bi- and cross-encoders:</p> <ul> <li>Stores a vector per token in each document (more memory than bi-encoders).</li> <li>Uses a MaxSim operation at retrieval: the score for a query token is the maximum similarity across all document token vectors.</li> <li>Achieves near cross-encoder accuracy at bi-encoder speed.</li> </ul>"},{"location":"RAG/embedding/#4-embedding-training-objectives","title":"4. Embedding Training Objectives","text":""},{"location":"RAG/embedding/#contrastive-learning-infonce-mnr-loss","title":"Contrastive Learning (InfoNCE / MNR Loss)","text":"<p>Positive pairs (query + relevant document) are pulled closer in vector space; negative examples (other documents in the batch) are pushed apart.</p> <p>InfoNCE Loss:</p> <pre><code>L = -log [ exp(sim(q, d+)) / (exp(sim(q, d+)) + \u03a3 exp(sim(q, d-))) ]\n</code></pre> <ul> <li>Temperature \u03c4: Scales similarity scores before softmax. Low \u03c4 \u2192 model focuses heavily on hardest negatives. High \u03c4 \u2192 smooths the distribution.</li> </ul> <p>Multiple Negatives Ranking (MNR) Loss: The standard in Sentence-Transformers. In a batch of K pairs, each document serves as a negative for all other queries in the batch \u2014 providing K\u20131 negatives per query \"for free\" without manual labelling.</p>"},{"location":"RAG/embedding/#supervised-retrieval-fine-tuning","title":"Supervised Retrieval Fine-tuning","text":"<p>Models are fine-tuned on human-labelled query-document relevance datasets.</p> <ul> <li>MS MARCO: The gold standard \u2014 real Bing queries paired with human-judged relevant passages. Most production embedding models (BGE, GTE, E5) are trained on MS MARCO.</li> <li>Pairwise loss: Model is given (q, d+, d-) and penalised if d- scores higher than d+.</li> <li>Listwise loss: Model optimises the entire ranked list at once. More accurate for ranking (directly optimises nDCG) but more expensive to train.</li> </ul>"},{"location":"RAG/embedding/#matryoshka-representation-learning-mrl","title":"Matryoshka Representation Learning (MRL)","text":"<p>Training embeds information hierarchically so the first N dimensions contain the most important features. This enables vector truncation \u2014 you can store 1536-dimensional vectors but query only the first 256 dimensions to save storage and compute with minimal accuracy loss.</p>"},{"location":"RAG/embedding/#instruction-tuned-embeddings","title":"Instruction-Tuned Embeddings","text":"<p>Models like Instructor and BGE accept a natural-language task prefix:</p> <p>\"Represent this query for retrieving legal documents\"</p> <p>This lets a single model adapt its embedding space to different tasks \u2014 retrieval vs. clustering vs. classification \u2014 without separate models.</p>"},{"location":"RAG/embedding/#5-domain-adaptation-the-cold-start-problem","title":"5. Domain Adaptation (The \"Cold Start\" Problem)","text":"<p>Generic embeddings underperform on specialist domains (medical, legal, code). Adaptation options in increasing cost order:</p> <ol> <li>Continued pre-training: Run Masked Language Modeling (MLM) on your private corpus to teach the model domain vocabulary.</li> <li>Contrastive fine-tuning: Use domain-specific query-document pairs with contrastive loss to pull relevant items closer.</li> <li>Generative Pseudo-Labeling (GPL): Use an LLM to generate synthetic questions for each unlabeled document, then train the embedding model on these synthetic pairs. Effective with zero labeled data.</li> <li>Adapter-based tuning: Insert lightweight adapter layers and fine-tune only those, leaving base model weights frozen.</li> </ol> <p>Critical: If you change the embedding model, you must re-index the entire corpus. Vectors from different models are not comparable.</p>"},{"location":"RAG/embedding/#6-distance-metrics","title":"6. Distance Metrics","text":"Metric What It Measures Note Cosine similarity Angle between vectors (ignores magnitude) Most common for text; equivalent to dot product on L2-normalised vectors Dot product Projection of one vector onto another Faster; rewards both direction and magnitude Euclidean distance Straight-line distance in vector space Sensitive to vector magnitude; less common for text <p>Note: Cosine similarity with L2-normalised vectors is equivalent to dot product. L2 normalisation stabilises similarity scores and improves ANN search behaviour by projecting all vectors onto a unit hypersphere.</p>"},{"location":"RAG/embedding/#7-common-failure-modes","title":"7. Common Failure Modes","text":"Failure Mode Root Cause Mitigation Semantic drift Retrieved chunks are topically related but not relevant Add cross-encoder reranker Out-of-vocabulary Search for product IDs or rare part numbers fails Add hybrid search (BM25 or SPLADE) Intent mismatch Procedural query retrieves descriptive content Use HyDE (hypothetical document embeddings) Domain mismatch Generic model underperforms on specialist text Fine-tune or domain-adapt the embedding model Lost in the middle LLM ignores context deep in the prompt Parent-Document Retrieval; reorder chunks"},{"location":"RAG/embedding/#8-evaluation-of-embeddings","title":"8. Evaluation of Embeddings","text":"<p>Offline (retrieval-level):</p> <ul> <li>Recall@k \u2014 is the relevant document in the top k?</li> <li>MRR \u2014 how early does the first relevant document appear?</li> <li>nDCG \u2014 quality of the full ranked list using graded relevance.</li> </ul> <p>End-to-end:</p> <ul> <li>Answer correctness, faithfulness, latency, and cost.</li> </ul> <p>Better retrieval does not always lead to better generation without proper prompting and context selection.</p>"},{"location":"RAG/embedding/#9-interview-questions","title":"9. Interview Questions","text":"<p>Q: Why can't you use an LLM directly for retrieval over millions of documents?</p> <p>A: Two reasons: (1) LLMs have a fixed context window and cannot \"read\" millions of documents in one pass; (2) attention is O(N\u00b2) in sequence length, making it computationally infeasible. Embeddings provide O(log N) search via ANN indexes.</p> <p>Q: What is HyDE and when would you use it?</p> <p>A: Hypothetical Document Embeddings \u2014 use an LLM to generate a \"fake\" ideal answer to the query, embed that answer, and retrieve real documents similar to it. This bridges the vocabulary gap between a short query and longer document text. Use it when standard retrieval misses relevant documents because the query uses different wording than the documents. Risk: if the LLM hallucinates, the hallucinated text is embedded and may retrieve wrong documents.</p> <p>Q: When do dense embeddings outperform BM25, and when does BM25 win?</p> <p>A: Dense embeddings win on semantic search, paraphrase matching, and natural language queries where exact document wording differs from the query. BM25 wins when queries contain rare terms, product codes, identifiers, or exact strings where lexical matching is what matters. Hybrid systems combine both to cover both failure modes.</p> <p>Q: Does increasing embedding dimensionality always improve performance?</p> <p>A: No. Higher dimensions increase memory, index size, and ANN search latency. Beyond a point, the \"curse of dimensionality\" makes distance metrics less meaningful. MRL allows trading off dimensionality versus accuracy dynamically at query time.</p> <p>Q: When should you re-index your vector database?</p> <p>A: Whenever you change the embedding model \u2014 you cannot compare vectors generated by Model A with those from Model B. Also re-index when the corpus changes significantly, when chunk size or preprocessing changes, or when switching to an index type that requires a full rebuild (like IVF).</p> <p>Q: How do bi-encoders and cross-encoders differ in how they handle relevance?</p> <p>A: Bi-encoders encode query and document independently so they cannot model interaction between the two at encoding time \u2014 they rely on global semantic similarity. Cross-encoders concatenate query and document and process them jointly, so every token in the query can attend to every token in the document. This produces much richer relevance signals but cannot be pre-computed, making it unsuitable for large-scale first-stage retrieval.</p>"},{"location":"RAG/evaluation/","title":"Evaluation","text":""},{"location":"RAG/evaluation/#1-overview","title":"1. Overview","text":"<p>Evaluation is one of the hardest aspects of RAG. The system has multiple interacting components, no single ground-truth output, and can fail silently \u2014 generating fluent but incorrect answers. Effective evaluation requires multiple layers covering retrieval quality, generation quality, and faithfulness.</p>"},{"location":"RAG/evaluation/#2-why-rag-evaluation-is-hard","title":"2. Why RAG Evaluation Is Hard","text":"<p>Unlike traditional NLP tasks, RAG systems:</p> <ul> <li>Do not have a single ground-truth output (multiple valid answers may exist).</li> <li>Depend on external knowledge sources that can be wrong, stale, or irrelevant.</li> <li>Can fail silently \u2014 generating fluent but factually wrong answers.</li> <li>Have multiple interacting components: a failure in retrieval causes a failure in generation, but this is hard to detect end-to-end.</li> </ul> <p>No single metric is sufficient. Effective evaluation requires layered coverage.</p>"},{"location":"RAG/evaluation/#3-component-vs-end-to-end-evaluation","title":"3. Component vs. End-to-End Evaluation","text":""},{"location":"RAG/evaluation/#component-level-evaluation","title":"Component-Level Evaluation","text":"<p>Each module is tested independently.</p> <ul> <li>Retrieval: Are relevant documents retrieved? Are they ranked correctly?</li> <li>Generation: Given perfect context, can the model answer correctly?</li> </ul> Pros Easier to debug failures; clear error attribution; fast offline iteration Cons Does not capture compounding errors; may overestimate real-world performance"},{"location":"RAG/evaluation/#end-to-end-evaluation","title":"End-to-End Evaluation","text":"<p>The full pipeline is tested from user query to final answer.</p> Pros Reflects real user experience; captures interaction effects between components Cons Hard to diagnose root causes; more expensive and noisy <p>Strong systems use both \u2014 component evaluation during development, end-to-end evaluation before deployment.</p>"},{"location":"RAG/evaluation/#4-retrieval-metrics","title":"4. Retrieval Metrics","text":""},{"location":"RAG/evaluation/#recallk","title":"Recall@k","text":"<p>Fraction of queries for which at least one relevant document appears in the top-k retrieved results.</p> <ul> <li>Why it matters: If recall is low, generation cannot recover. Especially critical for factual QA.</li> <li>Limitation: Binary notion of relevance; does not consider ranking quality within top-k.</li> </ul>"},{"location":"RAG/evaluation/#mean-reciprocal-rank-mrr","title":"Mean Reciprocal Rank (MRR)","text":"<p>Measures how early the first relevant document appears in the ranked list. Score = average of 1/rank across queries.</p> <ul> <li>Why it matters: Rewards systems that rank relevant documents earlier; useful when only one document is needed.</li> <li>Limitation: Ignores multiple relevant documents.</li> </ul>"},{"location":"RAG/evaluation/#ndcg-normalised-discounted-cumulative-gain","title":"nDCG (Normalised Discounted Cumulative Gain)","text":"<p>Measures quality of the full ranked list using graded relevance, penalising relevant documents that appear lower.</p> <ul> <li>Why it matters: More realistic for multi-document relevance; handles graded (not just binary) relevance labels.</li> <li>Limitation: Requires graded relevance annotations; more complex to compute and interpret.</li> </ul>"},{"location":"RAG/evaluation/#precisionk","title":"Precision@k","text":"<p>Fraction of top-k retrieved documents that are relevant.</p> <ul> <li>Why it matters: Complements recall \u2014 high precision means less noisy context for the LLM.</li> <li>Limitation: Must be considered alongside recall; a system can have high precision@3 with low recall.</li> </ul> <p>High Recall@k is often more critical than precision in RAG retrieval, because the LLM can filter irrelevant context \u2014 but it cannot invent missing information.</p>"},{"location":"RAG/evaluation/#5-generation-quality-metrics","title":"5. Generation Quality Metrics","text":"Metric What It Measures Limitation Exact Match (EM) Exact string match with reference answer Too strict for NL generation; penalises valid paraphrasing F1 Score (token overlap) Token-level overlap with reference Surface-level; misses semantic equivalence BLEU / ROUGE N-gram overlap with reference text Poorly correlates with factual correctness; rewards fluency BERTScore Semantic similarity via contextual embeddings Better than n-gram but still not factuality-aware <p>These metrics primarily measure fluency and surface similarity, not truthfulness. A hallucinated answer can score well if it is fluent and partially overlaps with the reference.</p>"},{"location":"RAG/evaluation/#6-faithfulness-and-groundedness","title":"6. Faithfulness and Groundedness","text":"<p>The most critical RAG-specific evaluation dimension. A system can score well on generation metrics while still hallucinating \u2014 generating correct-sounding text not supported by the retrieved documents.</p>"},{"location":"RAG/evaluation/#faithfulness","title":"Faithfulness","text":"<p>Question: Is every claim in the answer supported by the retrieved context?</p> <ul> <li>Measured by sentence-level entailment checks or LLM-as-a-judge prompting.</li> <li>Failure mode: Correct-sounding claims that are not in any retrieved document.</li> </ul>"},{"location":"RAG/evaluation/#groundedness","title":"Groundedness","text":"<p>Question: Does every specific claim in the answer trace back to a retrieved source?</p> <ul> <li>Measured by claim extraction followed by source matching or citation validation.</li> <li>Failure mode: Answers that are correct (by coincidence or parametric knowledge) but unsupported by retrieved text.</li> </ul>"},{"location":"RAG/evaluation/#answer-relevance","title":"Answer Relevance","text":"<p>Question: Does the answer actually address the original question?</p> <ul> <li>Measured by LLM-as-a-judge or semantic similarity between answer and query.</li> </ul>"},{"location":"RAG/evaluation/#context-relevance","title":"Context Relevance","text":"<p>Question: Were the retrieved documents actually relevant to the query?</p> <ul> <li>Measured by LLM-based relevance scoring per retrieved chunk.</li> </ul>"},{"location":"RAG/evaluation/#7-llm-as-a-judge-ragas-framework","title":"7. LLM-as-a-Judge (RAGAS Framework)","text":"<p>Using an LLM to evaluate RAG outputs is increasingly the standard approach. RAGAS is a widely-used framework evaluating four dimensions without requiring ground-truth labels for generation:</p> Dimension Question Answered Faithfulness Are all claims in the answer supported by retrieved context? Answer Relevance Is the answer on-topic for the original query? Context Recall Does the retrieved context contain what's needed to answer? (Needs reference answer) Context Precision Are retrieved documents relevant, or is there noise? <p>Benefits: Scales without human labelling; captures semantic nuance beyond lexical overlap.</p> <p>Risks:</p> <ul> <li>Bias towards fluent answers \u2014 LLM judges may reward well-written hallucinations.</li> <li>Sensitivity to prompt design \u2014 scoring rubrics matter significantly.</li> <li>Self-preference bias \u2014 models tend to rate outputs from similar architectures more favourably.</li> </ul> <p>Best practice: Validate LLM-as-a-judge scores against human judgments on a held-out set before trusting them for production decisions.</p>"},{"location":"RAG/evaluation/#8-human-evaluation-protocols","title":"8. Human Evaluation Protocols","text":"<p>Human evaluation remains the gold standard for RAG systems.</p> <p>Common criteria: Correctness, completeness, faithfulness, clarity, usefulness.</p> <p>Protocol design:</p> <ul> <li>Blind evaluation (evaluators don't know which system produced which answer).</li> <li>Multiple annotators per example (typically 3).</li> <li>Measure inter-annotator agreement (Cohen's Kappa).</li> </ul> <p>Tradeoffs: High cost; low scalability; slow iteration \u2014 but irreplaceable for final validation.</p>"},{"location":"RAG/evaluation/#9-common-rag-failure-types-and-root-causes","title":"9. Common RAG Failure Types and Root Causes","text":"Failure Type Diagnosis Fix Relevant doc not retrieved Low Recall@k Improve embeddings; use hybrid retrieval; adjust chunk size Retrieved but not used by LLM Good recall, low faithfulness Improve prompt; citation constraints; rerank more aggressively Partial hallucinations Mixed faithful + invented claims Faithfulness scoring; instruction-tune with grounding Over-reliance on parametric knowledge Model ignores retrieved context Explicit grounding instructions; Self-RAG Stale or contradictory sources Corpus not updated Add timestamps; filter by recency; update index"},{"location":"RAG/evaluation/#10-building-an-evaluation-dataset","title":"10. Building an Evaluation Dataset","text":"<p>Challenge: Creating reliable ground-truth labels for RAG is expensive.</p> <p>Approaches:</p> <ol> <li>Synthetic generation: Use an LLM to generate questions from your document corpus, creating question\u2013context\u2013answer triples. Fast and free, but synthetic questions may not match real user queries.</li> <li>Production logging: Sample real user queries and have humans label relevant documents and correct answers. Most realistic, but requires live traffic.</li> <li>Expert annotation: For high-stakes domains (medical, legal), pay subject matter experts to create a gold-standard test set. Expensive but highest quality.</li> </ol>"},{"location":"RAG/evaluation/#11-interview-questions","title":"11. Interview Questions","text":"<p>Q: Why is no single metric sufficient to evaluate a RAG system?</p> <p>A: RAG has multiple failure modes that different metrics capture: retrieval metrics (recall, nDCG) measure whether the right documents are found; generation metrics (EM, F1) measure answer quality given perfect context; faithfulness metrics measure whether the answer is grounded in retrieved text; end-to-end metrics capture the combined effect. A system can have high recall but low faithfulness (retrieves well, then hallucinates). You need all layers to diagnose root causes.</p> <p>Q: What is the difference between faithfulness and answer correctness?</p> <p>A: Faithfulness measures whether the answer is supported by retrieved context \u2014 an answer can be wrong if the retrieved context is wrong, but still be \"faithful\". Answer correctness measures whether the answer matches a known ground truth \u2014 an answer can be correct but use parametric knowledge rather than retrieved context. Ideally you want both: correct and grounded in retrieved text.</p> <p>Q: How would you build an evaluation set for a new RAG system with no existing labelled data?</p> <p>A: Three approaches: (1) Synthetic generation \u2014 use an LLM to generate questions from your corpus, creating question-context-answer triples; (2) production logging \u2014 sample real user queries and have humans label relevant documents and correct answers; (3) expert annotation \u2014 for high-stakes domains, pay subject matter experts to create a gold-standard test set. Start with synthetic data for fast iteration and refine with human annotations before deployment.</p> <p>Q: What are the risks of using LLM-as-a-judge for evaluation?</p> <p>A: Three main risks: (1) Fluency bias \u2014 LLMs tend to prefer well-written answers even if they are less factually accurate; (2) Prompt sensitivity \u2014 the scoring rubric significantly affects scores and must be carefully designed; (3) Self-preference \u2014 when the same model family is used for generation and evaluation, it tends to rate its own outputs more favourably. Mitigate by using a different model family for judging than for generation, and always validate against human judgments on a held-out set.</p> <p>Q: How do you diagnose whether a RAG quality problem is in retrieval or generation?</p> <p>A: Run an oracle experiment: manually find the correct document and inject it directly into the prompt, bypassing retrieval entirely. If the model produces a correct answer with the oracle document, the problem is in retrieval. If the model still fails with the correct document in context, the problem is in generation (prompting, model reasoning, or context integration). This isolates the failure mode clearly.</p>"},{"location":"RAG/fundamentals/","title":"Fundamentals","text":""},{"location":"RAG/fundamentals/#1-overview-what-is-rag-and-why-does-it-exist","title":"1. Overview \u2014 What Is RAG and Why Does It Exist?","text":"<p>Large Language Models are trained on a fixed snapshot of data; knowledge is baked into model weights and cannot change without retraining. This creates three hard problems: a knowledge cutoff (no access to post-training events), hallucinations (the model generates plausible-sounding but wrong answers when it lacks a fact), and poor domain specificity for proprietary or niche corpora.</p> <p>Retrieval-Augmented Generation (RAG) decouples knowledge storage from language generation. Instead of forcing the model to memorise facts, it retrieves relevant external documents at inference time and conditions generation on that retrieved context.</p>"},{"location":"RAG/fundamentals/#2-the-core-rag-loop","title":"2. The Core RAG Loop","text":"<ol> <li>User submits a query.</li> <li>A retriever searches an external knowledge base and returns the top-k most relevant chunks.</li> <li>Retrieved chunks are injected into the prompt as context.</li> <li>The LLM generates an answer grounded in that context.</li> </ol>"},{"location":"RAG/fundamentals/#3-rag-vs-fine-tuning-choosing-the-right-tool","title":"3. RAG vs. Fine-Tuning \u2014 Choosing the Right Tool","text":"<p>This is one of the most common conceptual interview questions. The short answer: use RAG when the problem is about knowledge access; use fine-tuning when the problem is about model behaviour.</p> Dimension RAG Fine-Tuning Knowledge type Dynamic, large, frequently changing Stable, compact Primary goal Access external facts at inference time Change reasoning style or output format Cost to update Re-index documents (cheap) Retrain or fine-tune model (expensive) Explainability Citations traceable to source chunks Opaque \u2014 knowledge in weights Hallucination risk Reduced (grounded in retrieved text) Not directly addressed Common use case Enterprise Q&amp;A, support bots, doc search Instruction following, code style, tone <p>In practice RAG and fine-tuning are complementary. A model may be fine-tuned for instruction following, while RAG supplies factual grounding at runtime.</p>"},{"location":"RAG/fundamentals/#4-high-level-rag-pipeline","title":"4. High-Level RAG Pipeline","text":"<p>A standard RAG system has four stages:</p> <ol> <li>Indexing \u2014 documents are chunked, embedded, and stored in a vector index.</li> <li>Retrieval \u2014 the user query is embedded and the most similar chunks are fetched.</li> <li>Augmentation \u2014 retrieved chunks are injected into the prompt alongside the query.</li> <li>Generation \u2014 the LLM generates a grounded answer conditioned on query + context.</li> </ol>"},{"location":"RAG/fundamentals/#5-key-failure-modes-of-vanilla-rag","title":"5. Key Failure Modes of Vanilla RAG","text":"Failure Mode Description Poor recall Relevant documents exist but are not retrieved Poor precision Retrieved documents are irrelevant or noisy Chunking errors Semantic meaning is fragmented across chunk boundaries Context overflow Retrieved context exceeds the model's context window Model ignores context LLM falls back on parametric knowledge despite good retrieval No verification System produces fluent but wrong answers with no detection <p>Most real-world RAG systems extend vanilla RAG to explicitly address these failure modes by adding reranking, query rewriting, verification, and feedback loops.</p>"},{"location":"RAG/fundamentals/#6-interview-questions","title":"6. Interview Questions","text":"<p>Q: What problem does RAG solve that fine-tuning does not?</p> <p>A: RAG solves the knowledge access problem \u2014 it lets the model use information that wasn't in the training set, or that changes frequently. Fine-tuning changes how the model behaves (style, reasoning, format) but does not give it a way to look things up. A fine-tuned model still hallucinates when asked about facts outside its training data.</p> <p>Q: Can you use RAG and fine-tuning together?</p> <p>A: Yes, and this is common in production. A model might be fine-tuned for instruction following or a specific output format, and RAG provides the factual grounding at query time. Fine-tuning handles \"how to respond\"; RAG handles \"what to respond with\".</p> <p>Q: What are the main failure modes in a vanilla RAG system?</p> <p>A: The biggest failure is retrieval failure \u2014 the right document exists but isn't retrieved (poor recall) or irrelevant documents are retrieved (poor precision). Other failures include chunking errors that fragment context, context window overflow, the model ignoring or hallucinating over retrieved text, and the lack of any verification step.</p> <p>Q: Formally, how does RAG change the generation objective?</p> <p>A: Vanilla LLM generation is P(y | q) \u2014 the answer depends only on the query. RAG conditions generation on both the query and retrieved documents: P(y | q, d\u2081:k). The model reasons over provided evidence rather than relying solely on its internal parameters.</p>"},{"location":"RAG/indexing_and_vector_database/","title":"Indexing Strategies, Vector Databases, and Retrieval Systems for RAG","text":""},{"location":"RAG/indexing_and_vector_database/#1-overview","title":"1. Overview","text":"<p>Indexing defines how embeddings are organised for fast similarity search at scale. The right indexing strategy depends on corpus size, latency requirements, update frequency, and memory constraints. If the retriever is the bottleneck for RAG quality, the index is what makes the retriever fast enough to use in production.</p>"},{"location":"RAG/indexing_and_vector_database/#2-flat-index-exact-search","title":"2. Flat Index (Exact Search)","text":"<p>Computes similarity against every vector in the database. Recall is perfect by definition \u2014 no approximation.</p> <ul> <li>Pros: Exact results; simple implementation; deterministic.</li> <li>Cons: Linear search time \u2014 O(N) per query; unusable in production for large corpora.</li> <li>Use when: Small datasets (up to ~100k vectors); offline ground-truth benchmarking; evaluating other indexes.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#3-approximate-nearest-neighbor-ann-indexes","title":"3 Approximate Nearest Neighbor (ANN) Indexes","text":"<p>ANN indexes trade a small accuracy loss for large performance gains. The two dominant approaches are HNSW and IVF.</p>"},{"location":"RAG/indexing_and_vector_database/#hnsw-hierarchical-navigable-small-world","title":"HNSW (Hierarchical Navigable Small World)","text":"<p>Builds a multi-layer graph where each node connects to similar vectors. Search starts at higher (coarser) layers and progressively refines through lower (finer) layers. The \"navigable small world\" property ensures short paths between any two nodes in the graph.</p> <p>How it works:</p> <ul> <li>During indexing, each new vector is inserted into multiple layers, with connections to its nearest neighbours at each layer.</li> <li>During search, the algorithm enters at the top layer (fewest nodes), greedily navigates to the nearest centroid, then descends layer by layer for increasing refinement.</li> </ul> Pros Very high recall at low latency; supports dynamic insertion without full rebuild; tunable recall/speed tradeoff Cons High memory overhead from graph edges (typically 2\u20138x raw vector storage); slow build on very large datasets Best for Latency-sensitive RAG; medium-to-large corpora; frequently updated data <p>Key parameters:</p> <ul> <li><code>M</code> \u2014 number of connections per node. Higher M \u2192 better recall, more memory.</li> <li><code>ef_construction</code> \u2014 search width during index build. Higher \u2192 better index quality, slower build.</li> <li><code>ef_search</code> \u2014 search width at query time. Higher \u2192 better recall, slower queries.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#ivf-inverted-file-index","title":"IVF (Inverted File Index)","text":"<p>Clusters all vectors into <code>n_list</code> centroid clusters at build time. At query time, only the <code>n_probe</code> closest cluster centroids are searched.</p> <p>Analogy to classical inverted index: Instead of <code>term \u2192 documents</code>, IVF uses <code>centroid ID \u2192 vectors assigned to that centroid</code>. Only vectors in the probed clusters are evaluated.</p> Pros Lower memory than HNSW; faster to build; disk-backed search feasible Cons Lower recall than HNSW if relevant vectors fall outside probed clusters; sensitive to clustering quality Best for Very large datasets; cost-constrained or memory-constrained systems <p>Key parameters:</p> <ul> <li><code>n_list</code> \u2014 number of clusters. More clusters \u2192 higher precision but longer build time.</li> <li><code>n_probe</code> \u2014 number of clusters searched at query time. More probes \u2192 higher recall, slower queries.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#product-quantization-pq","title":"Product Quantization (PQ)","text":"<p>Compresses high-dimensional vectors into compact codes by splitting each vector into sub-vectors and quantising each sub-vector independently using a trained codebook.</p> <ul> <li>Pros: Massive memory reduction \u2014 enables storage of billions of vectors; lower I/O cost.</li> <li>Cons: Lossy compression \u2014 recall drops due to quantisation errors; harder to debug.</li> <li>Typically combined with: IVF+PQ for extreme-scale search (e.g., web-scale retrieval).</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#ann-index-comparison","title":"ANN Index Comparison","text":"Index Recall Query Speed Memory Update Support Best For Flat (exact) Perfect Slow (linear) Low Easy Ground truth, small datasets HNSW Very high Very fast High Easy (dynamic) Production RAG, latency-sensitive IVF High Fast Medium Requires rebuild Large scale, memory-constrained IVF+PQ Moderate Very fast Very low Requires rebuild Billion-scale search Sparse (BM25) High (lexical) Very fast Low Easy Keyword search, hybrid RAG"},{"location":"RAG/indexing_and_vector_database/#4-sparse-indexes","title":"4. Sparse Indexes","text":"<p>Sparse indexes use term-based inverted indexes mapping <code>term \u2192 posting list of documents</code>. Standard infrastructure for BM25 and SPLADE. Implemented in Elasticsearch, OpenSearch, and Lucene.</p> <ul> <li>Excellent for: Lexical retrieval; exact keyword matches; rare terms and identifiers.</li> <li>Cannot do: Semantic similarity; paraphrase matching.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#5-hybrid-indexing","title":"5. Hybrid Indexing","text":"<p>Hybrid systems maintain both a dense vector index and a sparse inverted index. Retrieval runs in parallel across both, and results are merged (typically with Reciprocal Rank Fusion).</p> <ul> <li>Pros: Improved recall and precision; robust to diverse query types; production-proven.</li> <li>Cons: Increased system complexity; higher latency (two retrieval paths); requires score fusion tuning.</li> </ul>"},{"location":"RAG/indexing_and_vector_database/#6-vector-databases","title":"6. Vector Databases","text":"<p>Vector databases manage embedding storage, indexing, ANN search, metadata filtering, and scaling in a unified system. Key selection criteria: index type support, metadata filtering capabilities, update model, latency guarantees, and operational overhead.</p> Database Key Strength Consideration FAISS (Meta) Extremely flexible, high-performance, research standard Not a full DB \u2014 needs extra engineering for production Milvus Distributed, scalable, multiple index types High operational complexity Qdrant Strong metadata filtering, RAG-optimised, simple to operate Less ecosystem than FAISS Pinecone Fully managed, zero ops overhead, consistent performance Limited internal control; cost scales quickly Weaviate Strong hybrid search (dense + BM25 built-in) More complex query interface pgvector Postgres extension \u2014 no new infrastructure needed Lower performance at large scale"},{"location":"RAG/indexing_and_vector_database/#7-metadata-filtering","title":"7. Metadata Filtering","text":"<p>Metadata filtering restricts retrieval to relevant subsets before (pre-filtering) or after (post-filtering) vector search.</p> <p>Pre-filtering (filter first, then search the smaller set):</p> <ul> <li>Faster \u2014 ANN search runs on a smaller index.</li> <li>Risk: over-filtering can hurt recall if filters are too strict.</li> </ul> <p>Post-filtering (search first, then discard irrelevant results):</p> <ul> <li>Better recall \u2014 the full index is searched.</li> <li>Wastes compute on candidates that will be filtered out.</li> </ul> <p>Common metadata filters:</p> <ul> <li>Document type or source</li> <li>Timestamp / version (retrieve only recent documents)</li> <li>Author or department</li> <li>Access permissions / tenant ID (multi-tenant RAG)</li> <li>Content type (prose vs. table vs. code)</li> </ul> <p>Metadata filtering is one of the highest-ROI improvements in a vanilla RAG system \u2014 it narrows the search space without changing the embedding model or retraining anything.</p>"},{"location":"RAG/indexing_and_vector_database/#8-multi-tenant-rag-and-access-control","title":"8. Multi-Tenant RAG and Access Control","text":"<p>In enterprise systems, different users should only retrieve from their permitted document subset. Two main approaches:</p> <ol> <li>Namespace / collection isolation: Each tenant's documents live in a separate index namespace. Cleanest isolation but higher infrastructure cost.</li> <li>Metadata-based filtering: All documents share one index; retrieval filters on a <code>tenant_id</code> metadata field. More efficient but relies on the vector DB correctly enforcing filters.</li> </ol>"},{"location":"RAG/indexing_and_vector_database/#9-interview-questions","title":"9. Interview Questions","text":"<p>Q: What is the key trade-off between HNSW and IVF?</p> <p>A: HNSW gives higher recall and faster queries but uses significantly more memory (graph edges) and is slower to build. IVF uses less memory and builds faster but can miss relevant vectors that fall outside the probed clusters, giving lower recall. For latency-sensitive RAG with frequent updates, HNSW is usually preferred. For very large datasets where memory is a constraint, IVF (often with PQ compression) is used.</p> <p>Q: When would you re-index your vector database?</p> <p>A: You must re-index whenever you change the embedding model \u2014 vectors from different models live in different vector spaces and cannot be compared. You should also re-index when the corpus changes significantly (documents added/deleted), when chunk size or preprocessing changes, or when switching to an index type that requires a full rebuild (like IVF).</p> <p>Q: How does metadata filtering interact with ANN search?</p> <p>A: Pre-filtering (filter before ANN search) is faster because you search a smaller set, but can hurt recall if filters are too aggressive. Post-filtering (run ANN then filter results) preserves recall but wastes compute. Some vector databases (like Qdrant) support segment-level filtering that approximates pre-filtering without the recall penalty.</p> <p>Q: What are the main operational differences between FAISS and a managed vector DB like Pinecone?</p> <p>A: FAISS is a library \u2014 extremely flexible and high-performance, but you are responsible for persistence, serving, scaling, replication, and monitoring. Pinecone is a fully managed service with consistent performance, automatic scaling, and no operational overhead, but you have less control over internals (index type, tuning parameters) and costs scale quickly with data volume. For research or highly custom pipelines, FAISS is better. For product teams that want to ship quickly, managed databases like Pinecone or Qdrant are preferable.</p> <p>Q: How would you design a RAG system for a multi-tenant SaaS application where each customer should only retrieve their own documents?</p> <p>A: Two main options: (1) Namespace isolation \u2014 each customer gets a separate collection/namespace in the vector DB. Strong security guarantees, but higher cost at large customer counts. (2) Metadata filtering \u2014 store all documents in one index with a <code>tenant_id</code> field, and always filter on tenant_id at query time. More efficient, but you must trust the vector DB's filter enforcement and ensure no metadata leakage. For high-security requirements (e.g., healthcare, finance), namespace isolation is safer.</p>"},{"location":"RAG/quick_reference/","title":"Quick Reference","text":""},{"location":"RAG/quick_reference/#1-when-to-use-each-retrieval-method","title":"1. When to Use Each Retrieval Method","text":"Method Use When BM25 Exact keyword matches matter; no training budget; queries contain IDs, codes, or rare terms; strong baseline TF-IDF Very simple baseline; interpretability is critical; no BM25 available SPLADE Want BM25 efficiency + semantic expansion; existing inverted-index infra; diverse query vocabulary Dense bi-encoder Natural language queries; paraphrase matching; fine-grained semantic similarity Hybrid (dense + sparse) Production systems; diverse query types; best overall recall and precision Cross-encoder (reranker) Second-stage precision boost over 50\u2013100 candidates; latency is acceptable"},{"location":"RAG/quick_reference/#2-chunking-strategy-decision-guide","title":"2. Chunking Strategy Decision Guide","text":"Document Type Recommended Strategy Uniform text (articles, reports) Sentence-based or paragraph-based Structured docs with headings (PDFs, wikis) Recursive chunking Long documents, multi-hop queries Sliding window or recursive High-precision, smaller corpus Semantic/context-aware Tables Row-based with schema metadata Source code Function-level or class-level"},{"location":"RAG/quick_reference/#3-chunk-size-and-top-k-reference","title":"3. Chunk Size and Top-k Reference","text":"Chunk Size Top-k (retrieval) Top-m (after reranker) Tradeoff Small (100\u2013300 tokens) 20\u201350 5\u201310 High recall, more context noise Medium (300\u2013700 tokens) 10\u201320 5\u201310 Balanced \u2014 good default Large (700\u20131500 tokens) 3\u20138 3\u20135 High precision, risk of missing info"},{"location":"RAG/quick_reference/#4-retrieval-failure-diagnostic-flow","title":"4. Retrieval Failure Diagnostic Flow","text":"<pre><code>Low answer quality?\n  \u2502\n  \u251c\u2500 Check Recall@k\n  \u2502     Is the relevant document in top-k?\n  \u2502     NO \u2192 Fix the retriever:\n  \u2502           - Better embedding model\n  \u2502           - Add hybrid search (BM25 + dense)\n  \u2502           - Add query expansion or HyDE\n  \u2502           - Adjust chunk size\n  \u2502           - Add metadata filters\n  \u2502\n  \u251c\u2500 Check Context Precision\n  \u2502     Are the retrieved documents actually relevant?\n  \u2502     NO \u2192 Add cross-encoder reranker\n  \u2502           Improve chunking to reduce noise\n  \u2502\n  \u251c\u2500 Check Faithfulness\n  \u2502     Is the answer grounded in retrieved text?\n  \u2502     NO \u2192 Improve prompt (citation constraints)\n  \u2502           Add explicit grounding instructions\n  \u2502           Check for context window overflow\n  \u2502\n  \u2514\u2500 Check Answer Correctness\n        Is the grounded answer actually right?\n        NO \u2192 Retrieved documents may be wrong, outdated, or insufficient\n              Update corpus; validate sources; improve coverage\n</code></pre>"},{"location":"RAG/quick_reference/#5-key-parameter-defaults","title":"5. Key Parameter Defaults","text":"Parameter Default / Starting Point Notes Chunk size 400\u2013600 tokens Tune based on query type and embedding model Chunk overlap 10\u201315% of chunk size Mitigates boundary loss; increases index size Top-k (first-stage retriever) 50\u2013100 Higher for hybrid; lower in resource-constrained systems Top-m (after reranker) 5\u201310 What the LLM actually sees BM25 k\u2081 1.5 Controls TF saturation speed BM25 b 0.75 Controls length normalisation strength RRF constant k 60 Robustly combines ranked lists; rarely needs tuning HNSW M 16\u201364 Higher = better recall, more memory HNSW ef_search 100\u2013200 Higher = better recall, slower queries"},{"location":"RAG/quick_reference/#6-architecture-decision-matrix","title":"6. Architecture Decision Matrix","text":"System Size Latency Req. Update Freq. Recommended Architecture Small (&lt;100k docs) Any Any Flat index or HNSW + BM25 hybrid Medium (100k\u201310M docs) Low Frequent HNSW + sparse, with cross-encoder reranker Large (&gt;10M docs) Low Infrequent IVF+PQ + sparse, with reranker Very large (&gt;1B docs) Very low Infrequent IVF+PQ, distributed (Milvus), managed (Pinecone)"},{"location":"RAG/quick_reference/#7-bi-encoder-vs-cross-encoder-quick-reference","title":"7. Bi-Encoder vs. Cross-Encoder Quick Reference","text":"Bi-Encoder Cross-Encoder Encoding Query and doc separately Query + doc concatenated together Pre-computation Doc embeddings stored at index time Must run at query time per candidate Scalability Millions of docs ~50\u2013100 candidates Recall / Precision High recall High precision Speed Very fast (ANN search) Slow (full forward pass per pair) Used in First-stage retrieval Reranking"},{"location":"RAG/quick_reference/#8-evaluation-metric-summary","title":"8. Evaluation Metric Summary","text":"Metric Stage What It Measures Recall@k Retrieval Is relevant doc in top k? MRR Retrieval How early is first relevant doc? nDCG Retrieval Full ranked list quality (graded relevance) Precision@k Retrieval What fraction of top-k is relevant? Exact Match (EM) Generation Exact string match with reference F1 (token) Generation Token overlap with reference BERTScore Generation Semantic similarity to reference Faithfulness End-to-end Claims supported by retrieved context? Answer Relevance End-to-end Does answer address the query? Context Precision End-to-end Are retrieved docs relevant? Context Recall End-to-end Does context contain required info?"},{"location":"RAG/quick_reference/#9-the-production-rag-checklist","title":"9. The Production RAG Checklist","text":"<ul> <li> Define evaluation metrics before building (Recall@k, faithfulness, answer correctness).</li> <li> Choose chunk size and strategy; evaluate retrieval recall before touching the LLM.</li> <li> Start with hybrid retrieval (BM25 + dense) as the first-stage retriever.</li> <li> Add a cross-encoder reranker; measure Precision@5 improvement.</li> <li> Attach metadata to all chunks; implement filtered retrieval.</li> <li> Implement faithfulness and groundedness checks before shipping.</li> <li> Run the oracle experiment to confirm failures are in retrieval, not generation.</li> <li> Log queries, retrieved context, and answers in production for continuous evaluation.</li> <li> Re-index when changing embedding models.</li> <li> Build a held-out evaluation set; validate LLM-as-a-judge scores against humans.</li> </ul>"},{"location":"RAG/quick_reference/#10-top-interview-topics-by-frequency","title":"10. Top Interview Topics by Frequency","text":"Topic Frequency Key Point to Know RAG vs. fine-tuning Very high RAG = knowledge access; fine-tuning = behaviour change Bi-encoder vs. cross-encoder Very high Bi = fast/recall; cross = slow/precision; use both in sequence Chunking strategy choice High Recursive is the production default; tune size + top-k jointly BM25 vs. dense retrieval High Complementary; BM25 = exact match; dense = semantics Hybrid retrieval + RRF High Best overall quality; RRF is robust without weight tuning Faithfulness vs. correctness High Faithfulness = grounded in context; correctness = matches truth HyDE Medium Embed hypothetical answer instead of query to bridge vocab gap Lost in the middle Medium LLMs miss context in the middle; put best docs first/last HNSW vs. IVF Medium HNSW = recall/speed; IVF = memory efficiency RAGAS / LLM-as-judge Medium Standard eval framework; validate against human labels"},{"location":"RAG/reranking/","title":"Re-Ranking","text":""},{"location":"RAG/reranking/#1-overview","title":"1. Overview","text":"<p>Reranking is a second-stage step that re-scores a small candidate set returned by the first-stage retriever. The goal is to shift from recall-optimised to precision-optimised \u2014 ensuring the documents that actually enter the LLM prompt are the most relevant, not just the most similar embeddings.</p>"},{"location":"RAG/reranking/#2-why-reranking-is-necessary","title":"2. Why Reranking Is Necessary","text":"<p>First-stage retrievers (bi-encoders, BM25) are optimised for speed and recall. They encode query and document independently, which means they cannot model fine-grained relevance \u2014 the nuanced relationship between a specific question and a specific passage. A document that is semantically similar to a query may not actually answer it.</p> <p>Example: The query \"What are the side effects of aspirin in elderly patients?\" will semantically match many documents about aspirin. A bi-encoder may rank a general \"aspirin overview\" above a document that specifically discusses geriatric dosing risks \u2014 because the general document has higher semantic overlap. A cross-encoder reranker, reading both query and document together, would correctly identify the more specific document as more relevant.</p> <p>Rerankers fix this by evaluating each candidate in full context of the query.</p>"},{"location":"RAG/reranking/#3-cross-encoder-reranking","title":"3. Cross-Encoder Reranking","text":"<p>The standard reranking approach. The query and each candidate document are concatenated and fed through a transformer together. Full self-attention across both texts allows every query token to attend to every document token.</p> <p>How it works:</p> <ol> <li>First-stage retriever returns top-k candidates (typically 50\u2013100).</li> <li>For each candidate, concatenate <code>[CLS] query [SEP] document [SEP]</code> and run through the cross-encoder.</li> <li>The model outputs a single relevance score per candidate.</li> <li>Re-sort candidates by these scores; pass top-m (typically 5\u201310) to the LLM.</li> </ol> Pros Substantially higher precision than bi-encoders; captures exact question-answer relevance; rich token-level interactions Cons Cannot be pre-computed or indexed; one full forward pass per candidate \u2014 O(k) per query; too slow for first-stage retrieval on large corpora"},{"location":"RAG/reranking/#4-the-two-stage-recall-precision-pipeline","title":"4. The Two-Stage Recall \u2192 Precision Pipeline","text":"<p>The canonical production pattern combines a fast first-stage retriever with a slower but accurate reranker:</p> <pre><code>Query\n  \u2502\n  \u25bc\nStage 1: First-stage retriever (bi-encoder or BM25)\n         \u2192 Retrieve top 50\u2013100 candidates\n         \u2192 Fast, recall-optimised (O(log N) search)\n  \u2502\n  \u25bc\nStage 2: Reranker (cross-encoder)\n         \u2192 Re-score all 50\u2013100 candidates\n         \u2192 Slow, precision-optimised (O(k) forward passes)\n         \u2192 Return top 5\u201310\n  \u2502\n  \u25bc\nStage 3: LLM generation\n         \u2192 Receives only top 5\u201310 reranked documents as context\n</code></pre> <p>Adding a cross-encoder reranker is one of the highest-ROI improvements in a RAG system. The first retriever casts a wide net; the reranker ensures only the most relevant content reaches the LLM.</p>"},{"location":"RAG/reranking/#5-llm-based-reranking","title":"5 LLM-Based Reranking","text":"<p>An LLM is prompted to score or rank candidates rather than using a dedicated cross-encoder.</p> <p>Strategies:</p> <ul> <li>Pointwise: Score each document independently on a scale (e.g., 0\u201310 relevance).</li> <li>Pairwise: Compare two documents at a time and identify the more relevant one.</li> <li>Listwise: Ask the LLM to sort the entire candidate list and return the ranked order.</li> </ul> Pros Flexible \u2014 can use task-specific instructions; can reason about relevance semantically Cons Most expensive (LLM API cost per reranking); non-deterministic; prompt-sensitive <p>When to use: When cross-encoder models underperform on your domain; when you need explainable relevance reasoning; when latency constraints are loose.</p>"},{"location":"RAG/reranking/#6-reranker-models","title":"6. Reranker Models","text":"Model Notes <code>ms-marco-MiniLM-L-6-v2</code> Fast, small cross-encoder fine-tuned on MS MARCO \u2014 good default <code>bge-reranker-large</code> Strong performance; part of the BGE family <code>cross-encoder/ms-marco-electra-base</code> Stronger than MiniLM, still fast Cohere Rerank API Managed API; good multilingual support GPT-4 / Claude (listwise) Most accurate but highest latency and cost"},{"location":"RAG/reranking/#7-reranking-in-hybrid-systems","title":"7. Reranking in Hybrid Systems","text":"<p>In hybrid retrieval pipelines (dense + sparse), reranking typically happens after Reciprocal Rank Fusion merges the two result sets:</p> <pre><code>Dense retrieval \u2192 top 100\n                        \u2198\n                         RRF merge \u2192 top 100 fused \u2192 Cross-encoder reranker \u2192 top 10 \u2192 LLM\n                        \u2197\nSparse retrieval \u2192 top 100\n</code></pre> <p>This architecture maximises both recall (hybrid retrieval) and precision (cross-encoder reranking).</p>"},{"location":"RAG/reranking/#8-interview-questions","title":"8. Interview Questions","text":"<p>Q: Why use a bi-encoder for retrieval instead of a cross-encoder from the start?</p> <p>A: Cross-encoders cannot be pre-computed \u2014 every query-document pair requires a full model pass, so you cannot index documents ahead of time. A bi-encoder pre-computes all document embeddings once and stores them. At query time you only encode the query once and find nearest neighbours with fast math. For a corpus of 1 million documents, a cross-encoder would require 1 million forward passes per query \u2014 completely infeasible. Bi-encoders make retrieval tractable; cross-encoders then improve precision on a small shortlist.</p> <p>Q: What is a good top-k for the first-stage retriever?</p> <p>A: Typically 50\u2013100. Large enough to have high recall (not miss relevant documents), small enough to keep reranking latency manageable. The exact number depends on the reranker's latency budget and how many documents will ultimately be passed to the LLM (usually 5\u201310). In hybrid systems, each retriever returns 50\u2013100 and RRF merges them before reranking.</p> <p>Q: What happens if the first-stage retriever misses the relevant document entirely?</p> <p>A: The reranker cannot help \u2014 it can only reorder what was already retrieved. This is why first-stage recall is so critical. If Recall@100 is low, invest in improving the retriever (better embedding model, hybrid retrieval, query expansion) rather than in a better reranker. The reranker is a precision tool, not a recall tool.</p> <p>Q: How do you evaluate whether a reranker is actually helping?</p> <p>A: Compare Precision@5 (or Precision@10) before and after reranking on a labelled evaluation set. Also measure MRR \u2014 whether the most relevant document is moving to a higher rank. End-to-end, measure faithfulness and answer correctness before/after adding the reranker. Watch latency as a cost: cross-encoder reranking adds 50\u2013200ms per query depending on model size and number of candidates.</p> <p>Q: What is the difference between reranking and retrieval?</p> <p>A: Retrieval selects candidates from the full corpus \u2014 it must be fast and prioritises recall. Reranking re-scores a small shortlist \u2014 it can be slower and prioritises precision. Retrieval uses approximate methods (ANN, BM25) that scale to millions of documents. Reranking uses exact, expensive methods (cross-encoders, LLMs) that only work on tens to hundreds of candidates. They serve complementary roles in the pipeline.</p>"},{"location":"RAG/retrieval_methods/","title":"Retrieval Methods","text":""},{"location":"RAG/retrieval_methods/#1-overview","title":"1. Overview","text":"<p>Retrieval is the single most important factor in RAG quality. If the right document is not retrieved, the LLM cannot recover. This section covers the main retrieval algorithms from classical to neural, and how they compare.</p>"},{"location":"RAG/retrieval_methods/#2-tf-idf","title":"2. TF-IDF","text":"<p>TF-IDF (Term Frequency\u2013Inverse Document Frequency) is the foundational lexical retrieval method. It scores documents by combining two signals.</p>"},{"location":"RAG/retrieval_methods/#term-frequency-tf","title":"Term Frequency (TF)","text":"<p>How often a term appears in the document. Higher frequency \u2192 higher score.</p> <pre><code>TF(t, d) = count(t in d) / total terms in d\n</code></pre> <p>Variants like logarithmic TF reduce the impact of very frequent terms.</p>"},{"location":"RAG/retrieval_methods/#inverse-document-frequency-idf","title":"Inverse Document Frequency (IDF)","text":"<p>How rare a term is across the corpus. Rare terms score higher; common words (\"the\", \"is\") score near zero.</p> <pre><code>IDF(t) = log(N / (1 + df(t)))\n</code></pre> <p>Where N is the total number of documents and df(t) is the number containing term t.</p>"},{"location":"RAG/retrieval_methods/#tf-idf-score","title":"TF-IDF Score","text":"<pre><code>TF-IDF(t, d) = TF(t, d) \u00d7 IDF(t)\n</code></pre> <p>A high score indicates a term that is both locally frequent and globally distinctive.</p> <p>Strengths</p> <ul> <li>Simple, no training required, interpretable, fast.</li> <li>Works well for exact and partial lexical matches.</li> </ul> <p>Weaknesses</p> <ul> <li>No semantic understanding \u2014 requires exact word overlap.</li> <li>Assumes term independence (bag-of-words).</li> <li>Linear TF with no saturation \u2014 keyword stuffing inflates scores.</li> <li>Ignores word order and syntax.</li> </ul>"},{"location":"RAG/retrieval_methods/#3-bm25-best-matching-25","title":"3. BM25 (Best Matching 25)","text":"<p>BM25 is the go-to lexical retrieval baseline and a direct improvement over TF-IDF. It is probabilistically motivated and addresses TF-IDF's two main weaknesses: term frequency saturation and document length normalisation.</p>"},{"location":"RAG/retrieval_methods/#bm25-formula","title":"BM25 Formula","text":"<pre><code>BM25(d, q) = \u03a3 IDF(t) \u00d7 [tf(t,d) \u00d7 (k\u2081 + 1)] / [tf(t,d) + k\u2081 \u00d7 (1 - b + b \u00d7 |d|/avgdl)]\n</code></pre> <p>Where:</p> <ul> <li><code>tf(t, d)</code> \u2014 term frequency of t in document d</li> <li><code>|d|</code> \u2014 length of document d</li> <li><code>avgdl</code> \u2014 average document length in the corpus</li> <li><code>k\u2081</code> \u2014 controls term frequency saturation (typically 1.2\u20132.0)</li> <li><code>b</code> \u2014 controls length normalisation strength (typically 0.75)</li> </ul>"},{"location":"RAG/retrieval_methods/#key-improvements-over-tf-idf","title":"Key Improvements Over TF-IDF","text":"<p>Term Frequency Saturation: BM25 assumes diminishing returns for repeated terms. The first few occurrences matter most; additional repetitions contribute progressively less. This prevents keyword stuffing from dominating rankings.</p> <p>Document Length Normalisation: Longer documents are explicitly penalised via the <code>b</code> parameter. Setting b=0 disables normalisation; b=1 applies full normalisation. TF-IDF only applies weak or implicit normalisation.</p> <p>Modified IDF:</p> <p><pre><code>IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5))\n</code></pre> More robust for rare and common terms than the basic IDF formula.</p>"},{"location":"RAG/retrieval_methods/#bm25-vs-tf-idf","title":"BM25 vs. TF-IDF","text":"Aspect TF-IDF BM25 Term frequency Linear \u2014 repeats always increase score Saturated \u2014 diminishing returns past threshold Length normalisation Weak or implicit Explicit, tunable via b Tuning parameters None k\u2081 (saturation) and b (length norm) Ranking quality Good baseline Better; closer to human judgment Requires training No No"},{"location":"RAG/retrieval_methods/#example-why-bm25-ranks-better","title":"Example: Why BM25 Ranks Better","text":"<p>Corpus:</p> <ul> <li>D1: \"deep learning deep learning deep learning tutorial\"</li> <li>D2: \"deep learning tutorial\"</li> <li>D3: \"deep learning introduction overview\"</li> </ul> <p>Query: \"deep learning tutorial\"</p> <ul> <li>TF-IDF ranks: D1 &gt; D2 &gt; D3 \u2014 D1 wins purely due to repetition of \"deep learning\".</li> <li>BM25 ranks: D2 &gt; D1 &gt; D3 \u2014 D1's score saturates; D2's shorter length gives it a boost. D2 is more concise and directly on-topic.</li> </ul> <p>BM25 aligns with human intuition: repeating the same phrase doesn't make a document more relevant.</p>"},{"location":"RAG/retrieval_methods/#when-to-use-bm25","title":"When to Use BM25","text":"<ul> <li>Exact keyword matches matter (names, IDs, error codes, product codes).</li> <li>No training budget or labelled data.</li> <li>Fast, no-GPU, interpretable baseline.</li> <li>As the sparse component of a hybrid retrieval system.</li> </ul>"},{"location":"RAG/retrieval_methods/#4-splade","title":"4. SPLADE","text":"<p>SPLADE (Sparse Lexical and Expansion Model) is a neural sparse retrieval model that bridges classical lexical retrieval (BM25) and dense semantic retrieval. It uses a pretrained transformer to produce sparse vector representations, and critically, it performs learned term expansion.</p>"},{"location":"RAG/retrieval_methods/#core-idea","title":"Core Idea","text":"<p>BM25 relies on exact term overlap. SPLADE trains a transformer to assign weights to vocabulary terms \u2014 including terms not present in the input \u2014 based on semantic relevance. Despite using a neural model, the output is a sparse vector compatible with standard inverted-index infrastructure.</p>"},{"location":"RAG/retrieval_methods/#how-splade-works-step-by-step","title":"How SPLADE Works (Step by Step)","text":"<ol> <li>Input text is passed through a masked language model (e.g., BERT).</li> <li>Vocabulary scoring: For each token position, the model produces a score for every term in the vocabulary. The model answers: \"Which vocabulary terms are relevant to this text, even if not explicitly present?\"</li> <li>Non-linearity and sparsification: Raw scores are transformed using ReLU (removes negatives) + log scaling (controls large activations). Max pooling across token positions produces a single sparse vector.</li> <li>Result: One score per vocabulary term; most are zero. Only the most relevant terms remain active.</li> <li>Inverted index: Non-zero terms are stored in a standard inverted index \u2014 identical infrastructure to BM25.</li> </ol> <p>Example expansion: - Input: \"car repair\" - Expanded active terms: <code>{car: 2.1, repair: 1.9, automobile: 1.4, mechanic: 1.1, engine: 0.8}</code></p> <p>This allows matching a document about \"automobile maintenance\" even though neither query word appears in it.</p>"},{"location":"RAG/retrieval_methods/#splade-scoring-function","title":"SPLADE Scoring Function","text":"<pre><code>Score(t) = max over positions of log(1 + ReLU(logit_t))\n</code></pre> <ul> <li>ReLU enforces non-negativity</li> <li>Log scaling controls large activations</li> <li>Max pooling encourages sparse activation</li> </ul>"},{"location":"RAG/retrieval_methods/#splade-vs-bm25","title":"SPLADE vs. BM25","text":"Aspect BM25 SPLADE Term weighting Hand-crafted formula (TF \u00d7 IDF) Learned by transformer Semantic expansion None \u2014 exact words only Yes \u2014 expands to related terms Index compatibility Inverted index Inverted index (same infrastructure) Interpretability High Moderate Requires training No Yes Retrieval quality Strong baseline Stronger; approaches dense retrieval <p>Think of SPLADE as \"BM25 where the term weights and expansions are learned by a language model rather than hand-crafted.\"</p>"},{"location":"RAG/retrieval_methods/#when-to-use-splade","title":"When to Use SPLADE","text":"<ul> <li>Want BM25 efficiency with semantic expansion.</li> <li>Existing inverted-index infrastructure that cannot be replaced.</li> <li>Queries use different vocabulary than documents.</li> <li>As the sparse component in hybrid retrieval alongside dense models.</li> </ul>"},{"location":"RAG/retrieval_methods/#5-sentence-transformers-dense-retrieval","title":"5. Sentence Transformers (Dense Retrieval)","text":"<p>Sentence Transformers are neural models that convert text into dense fixed-size vectors encoding semantic meaning. They are the backbone of dense retrieval in most production RAG systems.</p>"},{"location":"RAG/retrieval_methods/#how-they-work-step-by-step","title":"How They Work (Step by Step)","text":"<ol> <li>Input text is passed through a transformer encoder (BERT, RoBERTa, MiniLM, etc.).</li> <li>Transformer encoding produces one contextualised embedding per token.</li> <li>Pooling: Token embeddings are aggregated into a single fixed-size vector. Mean pooling is most common \u2014 it is stable and empirically strong.</li> <li>L2 normalisation: The vector is normalised so cosine similarity equals dot product.</li> <li>Indexing: Vectors are stored in a vector database for ANN search.</li> </ol>"},{"location":"RAG/retrieval_methods/#why-mean-pooling","title":"Why Mean Pooling?","text":"<p>Mean pooling averages all token embeddings, including padding tokens (which are typically masked). It is simple, stable, and performs better than CLS-token pooling in most retrieval benchmarks.</p>"},{"location":"RAG/retrieval_methods/#training","title":"Training","text":"<p>Sentence Transformers are trained with contrastive objectives:</p> <ul> <li> <p>Multiple Negatives Ranking (MNR) Loss: In a batch of K query-document pairs, each document serves as a negative for all other queries in the batch. This provides K\u20131 negatives per query for free \u2014 no manual negative labelling needed. This is the standard training approach for models like BGE, E5, and GTE.</p> </li> <li> <p>Triplet Loss: (anchor, positive, negative) \u2014 the model is penalised when the negative is closer to the anchor than the positive.</p> </li> </ul>"},{"location":"RAG/retrieval_methods/#sentence-transformers-vs-sparse-retrieval","title":"Sentence Transformers vs. Sparse Retrieval","text":"Aspect Sparse (BM25 / SPLADE) Dense (Sentence Transformers) Semantic matching Limited / partial Strong \u2014 handles paraphrases natively Exact match Strong Weak \u2014 misses rare keywords and IDs Interpretability High Low \u2014 black box Infrastructure Inverted index Vector database with ANN index Training required No (BM25) / Yes (SPLADE) Yes Domain adaptation Not needed (BM25) May require fine-tuning"},{"location":"RAG/retrieval_methods/#when-to-use","title":"When to Use","text":"<ul> <li>Dense retrieval: Natural language queries; paraphrase matching; fine-grained semantic similarity matters.</li> <li>Sparse retrieval: Queries with specific identifiers; corpus has precise technical terminology.</li> <li>Hybrid (recommended for production): Most systems combine both to cover complementary failure modes.</li> </ul>"},{"location":"RAG/retrieval_methods/#6-retrieval-method-comparison","title":"6. Retrieval Method Comparison","text":"Method Semantic Matching Exact Match Training Needed Infrastructure Best Use Case TF-IDF None Good No Inverted index Simple baseline BM25 None Strong No Inverted index Keyword-heavy queries; default lexical baseline SPLADE Via expansion Strong Yes Inverted index Neural sparse; efficient + semantic Dense bi-encoder Strong Weak Yes Vector DB + ANN Natural language queries; semantic similarity Hybrid (dense + sparse) Strong Strong Yes (dense part) Both Production RAG; best overall quality"},{"location":"RAG/retrieval_methods/#7-interview-questions","title":"7. Interview Questions","text":"<p>Q: Why is BM25 still widely used despite neural models being more powerful?</p> <p>A: BM25 requires no training, has no GPU inference cost, is fully interpretable, and handles exact keyword matching better than dense models. It's also very fast on inverted indexes. In hybrid systems, BM25 and dense retrieval complement each other \u2014 BM25 catches cases the dense model misses (exact terms, IDs) and dense models catch paraphrases BM25 misses.</p> <p>Q: What is the difference between SPLADE and a dense bi-encoder?</p> <p>A: SPLADE produces a sparse vector over the vocabulary and is indexed in an inverted index \u2014 fast, memory-efficient, and interpretable. A dense bi-encoder produces a continuous low-dimensional vector indexed in a vector DB with ANN search. SPLADE uses a neural model for semantic expansion but retains sparse representation. In practice SPLADE often outperforms BM25 and approaches dense retrieval quality, while remaining compatible with existing inverted-index infrastructure.</p> <p>Q: What is Reciprocal Rank Fusion (RRF) and why is it preferred over score interpolation?</p> <p>A: RRF combines ranked lists from multiple retrievers by summing 1/(k + rank) for each document across all retrievers (k is typically 60). It is preferred over linear score interpolation because it is robust to score scale differences between retrievers \u2014 BM25 scores and cosine similarities live on completely different scales. RRF requires no weight tuning and typically matches or outperforms tuned linear combinations.</p> <p>Q: Why might a dense retriever miss a relevant document that BM25 finds?</p> <p>A: Dense retrievers encode the entire meaning of text into a single vector \u2014 rare or highly specific terms may get diluted in the embedding. If a document is only relevant because it contains a specific product ID, error code, or proper noun, a dense embedding may not preserve that specificity. BM25's exact-match scoring handles these cases naturally. This is why hybrid retrieval consistently outperforms either method alone.</p> <p>Q: What preprocessing decisions most affect BM25 performance?</p> <p>A: Stopword removal, stemming/lemmatisation, and tokenisation. Removing stopwords (words like \"the\", \"is\") reduces noise. Stemming conflates \"running\" and \"run\", improving recall but potentially harming precision. BM25's k\u2081 and b parameters also have a significant effect and should be tuned on held-out validation data rather than left at defaults.</p>"},{"location":"RAG/supporting_topics/bm25/","title":"BM25","text":""},{"location":"RAG/supporting_topics/bm25/#1-overview","title":"1. Overview","text":"<p>BM25 (Best Matching 25) is a probabilistic ranking function used in information retrieval to score and rank documents given a query. It can be seen as a principled improvement over TF IDF that better handles term frequency saturation and document length normalization. BM25 is widely used in search engines and remains a strong lexical baseline in modern retrieval systems.</p>"},{"location":"RAG/supporting_topics/bm25/#2-intuition-behind-bm25","title":"2. Intuition Behind BM25","text":"<p>TF-IDF assumes that term importance grows linearly with frequency and applies only coarse normalization for document length. In practice, these assumptions are often suboptimal.</p> <p>BM25 improves on this by: - Saturating the contribution of repeated terms - Explicitly normalizing for document length - Grounding the scoring function in probabilistic retrieval theory</p> <p>The result is a ranking function that aligns better with human relevance judgments.</p>"},{"location":"RAG/supporting_topics/bm25/#3-bm25-scoring-formula","title":"3. BM25 Scoring Formula","text":"<p>The BM25 score for a document d and query q is:</p> \\[ \\text{BM25}(d, q) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{     tf(t, d) \\cdot (k_1 + 1) }{     tf(t, d)     + k_1 \\cdot     \\left(         1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}}     \\right) } \\] <p>Where:</p> <ul> <li>\\(t\\) is a query term  </li> <li>\\(tf(t, d)\\) is the term frequency of \\(t\\) in document \\(d\\) </li> <li>\\(|d|\\) is the length of document \\(d\\) </li> <li>\\(\\text{avgdl}\\) is the average document length in the corpus  </li> <li>\\(k_1\\) controls term frequency saturation  </li> <li>\\(b\\) controls document length normalization  </li> <li>\\(\\text{IDF}(t)\\) is the inverse document frequency of term \\(t\\)</li> </ul>"},{"location":"RAG/supporting_topics/bm25/#31-key-components-explained","title":"3.1 Key Components Explained","text":""},{"location":"RAG/supporting_topics/bm25/#term-frequency-saturation","title":"Term Frequency Saturation","text":"<p>Unlike TF IDF, BM25 assumes diminishing returns for repeated terms. The first few occurrences of a term matter most, while additional repetitions contribute progressively less to the score.</p> <p>The parameter \\(k_1\\) controls how quickly this saturation happens.</p>"},{"location":"RAG/supporting_topics/bm25/#document-length-normalization","title":"Document Length Normalization","text":"<p>Longer documents naturally contain more terms and higher raw term counts. BM25 explicitly normalizes scores based on document length to avoid favoring longer documents unfairly.</p> <p>The parameter b controls how strongly document length affects the score: - b = 0 disables length normalization - b = 1 applies full normalization</p>"},{"location":"RAG/supporting_topics/bm25/#inverse-document-frequency","title":"Inverse Document Frequency","text":"<p>BM25 uses a slightly modified IDF formulation that is more robust for rare and common terms:</p> \\[IDF(t) = log((N \u2212 df(t) + 0.5) / (df(t) + 0.5))\\] <p>This formulation stabilizes scores and improves ranking quality.</p>"},{"location":"RAG/supporting_topics/bm25/#4-where-bm25-is-used","title":"4. Where BM25 is Used","text":"<p>BM25 is commonly used in:</p> <ul> <li>Search engines</li> <li>Document retrieval systems</li> <li>Question answering pipelines</li> <li>First stage retrievers in RAG systems</li> <li>Enterprise and web scale search applications</li> </ul> <p>It is often the default choice when dense embeddings are not available or too expensive.</p>"},{"location":"RAG/supporting_topics/bm25/#5-strengths-of-bm25","title":"5. Strengths of BM25","text":"<ul> <li>Strong lexical matching performance</li> <li>Better handling of term frequency than TF IDF</li> <li>Explicit document length normalization</li> <li>No training required</li> <li>Robust across different domains</li> </ul> <p>BM25 often outperforms TF IDF with minimal tuning.</p>"},{"location":"RAG/supporting_topics/bm25/#6-limitations-and-caveats","title":"6. Limitations and Caveats","text":"<ul> <li>Cannot capture semantic similarity</li> <li>Still relies on exact term overlap</li> <li>Sensitive to tokenization and preprocessing</li> <li>Query term independence assumption</li> <li>Requires tuning of k1 and b for optimal performance</li> </ul> <p>BM25 remains a lexical method and inherits the fundamental limitations of bag of words models.</p>"},{"location":"RAG/supporting_topics/bm25/#7-practical-considerations","title":"7. Practical Considerations","text":"<ul> <li>Typical values: k1 in [1.2, 2.0], b around 0.75</li> <li>Stopword handling significantly affects results</li> <li>Works best with cosine or BM25 specific scoring, not raw dot products</li> <li>Often paired with inverted indexes for efficient retrieval</li> </ul> <p>BM25 is computationally efficient and scales well to large corpora.</p>"},{"location":"RAG/supporting_topics/bm25/#8-bm25-vs-tf-idf","title":"8. BM25 vs TF IDF","text":"Aspect TF IDF BM25 Term frequency Linear Saturated Length normalization Weak Explicit Tuning parameters None k1, b Ranking quality Good Better Complexity Low Moderate <p>BM25 can be seen as a more refined and robust version of TF IDF.</p>"},{"location":"RAG/supporting_topics/bm25/#81-an-example","title":"8.1 An Example","text":"<p>Consider a small corpus of three documents and a simple query.</p>"},{"location":"RAG/supporting_topics/bm25/#corpus","title":"Corpus","text":"<ul> <li>D1: \"deep learning deep learning deep learning tutorial\"</li> <li>D2: \"deep learning tutorial\"</li> <li>D3: \"deep learning introduction overview\"</li> </ul>"},{"location":"RAG/supporting_topics/bm25/#query-deep-learning-tutorial","title":"Query : deep learning tutorial","text":""},{"location":"RAG/supporting_topics/bm25/#how-tf-idf-scores-these-documents","title":"How TF IDF Scores These Documents","text":"<pre><code>Key Observation\n\nTF IDF increases linearly with term frequency. Repeating the same term multiple times keeps increasing the score.\n\n### Approximate Behavior\n\n- D1\n    - Very high TF for \"deep\" and \"learning\"\n    - High TF IDF score due to repetition\n- D2\n    - Moderate TF for all query terms\n    - Lower score than D1\n- D3\n    - Missing \"tutorial\"\n    - Lowest score\n\nTF IDF Ranking: D1 &gt; D2 &gt; D3\n\nIssue\nD1 is ranked highest mainly because it repeats \"deep learning\" many times, even though it is not necessarily more relevant to the query than D2.\n\nTF IDF assumes that more repetitions always mean higher relevance.\n</code></pre>"},{"location":"RAG/supporting_topics/bm25/#how-bm25-scores-these-documents","title":"How BM25 Scores These Documents","text":"<pre><code>Key Observation\n\nBM25 applies **term frequency saturation** and **length normalization**.\n\n- The first few occurrences of a term matter most\n- Extra repetitions contribute less\n- Longer documents are penalized\n\nApproximate Behavior\n\n- D1\n    - TF contribution saturates quickly\n    - Longer document length reduces score\n- D2\n    - Balanced term coverage\n    - Shorter document length boosts score\n- D3\n    - Still missing \"tutorial\"\n    - Lowest score\n\nBM25 Ranking: D2 &gt; D1 &gt; D3\n</code></pre>"},{"location":"RAG/supporting_topics/bm25/#82-why-bm25-is-better-than-tf-idf-in-this-example","title":"8.2 Why BM25 Is Better Than TF IDF in This Example","text":""},{"location":"RAG/supporting_topics/bm25/#821-term-frequency-saturation","title":"8.2.1 Term Frequency Saturation","text":"<p>TF IDF: Repeats always increase score</p> <p>BM25: Repeats beyond a point add little value</p> <p>This prevents keyword stuffing from dominating rankings.</p>"},{"location":"RAG/supporting_topics/bm25/#822-document-length-normalization","title":"8.2.2 Document Length Normalization","text":"<p>TF IDF: Weak or implicit normalization</p> <p>BM25: Explicitly penalizes longer documents</p> <p>This avoids favoring verbose documents.</p>"},{"location":"RAG/supporting_topics/bm25/#823-ranking-quality","title":"8.2.3 Ranking Quality","text":"<p>BM25 aligns better with human intuition:</p> <ul> <li>D2 is more concise and directly matches the query</li> <li>D1 is repetitive but not necessarily more informative</li> </ul> <p>Note: BM25 improves over TF IDF by introducing realistic assumptions about term importance. It limits the impact of repeated terms and corrects for document length bias, leading to more reliable and intuitive document rankings in retrieval systems.</p>"},{"location":"RAG/supporting_topics/bm25/#9-bm25-in-modern-retrieval-systems","title":"9. BM25 in Modern Retrieval Systems","text":"<p>BM25 is frequently used as:</p> <ul> <li>A fast first stage retriever</li> <li>A baseline for evaluating dense retrievers</li> <li>A component in hybrid retrieval systems</li> <li>A fallback when embeddings fail</li> </ul> <p>Many state of the art systems combine BM25 with dense retrieval to balance recall and precision.</p>"},{"location":"RAG/supporting_topics/bm25/#10-interview-perspective","title":"10. Interview Perspective","text":"<p>For interviews, emphasize:</p> <ul> <li>Why BM25 improves over TF IDF</li> <li>The role of term frequency saturation</li> <li>The importance of document length normalization</li> <li>Typical parameter choices and their effects</li> <li>How BM25 fits into RAG and hybrid retrieval pipelines</li> </ul> <p>This demonstrates both theoretical understanding and practical retrieval intuition.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/","title":"Sentence Transformer","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#1-overview","title":"1. Overview","text":"<p>Sentence Transformers are neural models designed to convert sentences or short text passages into fixed size dense vectors that capture semantic meaning. They enable efficient semantic similarity, clustering, and retrieval by embedding text into a continuous vector space.</p> <p>They are a foundational component of modern semantic search and RAG systems.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#2-core-idea-behind-sentence-transformers","title":"2. Core Idea Behind Sentence Transformers","text":"<p>Traditional models like BERT produce contextual token embeddings but are not directly suitable for sentence level similarity. Sentence Transformers adapt transformer encoders to produce meaningful sentence level embeddings.</p> <p>The key idea is: - Similar sentences should have embeddings that are close in vector space - Dissimilar sentences should be far apart</p> <p>This is achieved through task specific training objectives.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#3-how-sentence-transformers-work-step-by-step","title":"3. How Sentence Transformers Work: Step by Step","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#step-1-input-text","title":"Step 1: Input Text","text":"<p>Sentence Transformers operate on short texts such as: - Sentences - Paragraphs - Queries and documents</p> <p>Example: \"Neural retrieval improves search quality\"</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-2-transformer-encoding","title":"Step 2: Transformer Encoding","text":"<p>The input text is passed through a transformer encoder such as BERT, RoBERTa, or MiniLM.</p> <p>The model outputs contextualized embeddings for each token in the input.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-3-pooling-to-sentence-embedding","title":"Step 3: Pooling to Sentence Embedding","text":"<p>Token embeddings are aggregated into a single fixed size vector.</p> <p>Common pooling strategies:</p> <ul> <li>Mean pooling across token embeddings</li> <li>CLS token pooling</li> <li>Max pooling</li> </ul> <p>Mean pooling is most commonly used because it is stable and performs well empirically.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-4-normalization","title":"Step 4: Normalization","text":"<p>The resulting sentence embedding is often L2 normalized.</p> <p>This allows cosine similarity to be computed efficiently using dot product.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-5-training-objective","title":"Step 5: Training Objective","text":"<p>Sentence Transformers are trained using contrastive or similarity based losses.</p> <p>Common objectives:</p> <ul> <li>Cosine similarity loss</li> <li>Triplet loss</li> <li>Multiple negative ranking loss</li> </ul> <p>Training encourages semantically similar sentences to have high cosine similarity.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-6-dense-vector-representation","title":"Step 6: Dense Vector Representation","text":"<p>Each sentence is represented as a dense vector.</p> <p>Example: \"Neural retrieval improves search quality\" \u2192 [0.12, -0.03, 0.88, ...]</p> <p>These embeddings capture semantic meaning beyond exact word overlap.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#step-7-indexing-and-retrieval","title":"Step 7: Indexing and Retrieval","text":"<p>Embeddings are indexed using approximate nearest neighbor methods such as:</p> <ul> <li>FAISS</li> <li>HNSW</li> <li>ScaNN</li> </ul> <p>At query time:</p> <ul> <li>The query is embedded</li> <li>Nearest neighbors are retrieved using cosine similarity or dot product</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#4-why-sentence-transformers-work-well","title":"4. Why Sentence Transformers Work Well","text":""},{"location":"RAG/supporting_topics/sentence_transformers/#41-semantic-matching","title":"4.1 Semantic Matching","text":"<p>They can match paraphrases and synonyms: \"car repair\" \u2248 \"automobile maintenance\"</p> <p>Even without shared tokens.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#42-dense-representations","title":"4.2 Dense Representations","text":"<p>Dense vectors:</p> <ul> <li>Are compact</li> <li>Capture continuous semantic relationships</li> <li>Enable fast similarity search with ANN indexes</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#5-where-sentence-transformers-are-used","title":"5. Where Sentence Transformers Are Used","text":"<ul> <li>Semantic search</li> <li>Dense retrieval for RAG</li> <li>Document clustering</li> <li>Duplicate detection</li> <li>Question answering</li> <li>Reranking and retrieval augmentation</li> </ul> <p>They are widely adopted due to strong performance and ease of use.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#6-strengths-of-sentence-transformers","title":"6. Strengths of Sentence Transformers","text":"<ul> <li>Strong semantic understanding</li> <li>Compact representations</li> <li>Effective for paraphrase matching</li> <li>Pretrained models available</li> <li>Easy integration with vector databases</li> </ul>"},{"location":"RAG/supporting_topics/sentence_transformers/#7-limitations-and-caveats","title":"7. Limitations and Caveats","text":"<ul> <li>Poor at exact lexical matching</li> <li>Sensitive to domain shift</li> <li>Require ANN infrastructure</li> <li>Harder to interpret than sparse methods</li> <li>Retrieval errors are harder to debug</li> </ul> <p>Dense retrievers can miss rare but important keywords.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#8-practical-considerations","title":"8. Practical Considerations","text":"<ul> <li>Model choice affects latency and quality</li> <li>Embedding dimensionality impacts storage and speed</li> <li>Fine tuning improves domain specific performance</li> <li>Hybrid retrieval often outperforms pure dense retrieval</li> <li>Reranking improves precision</li> </ul> <p>Sentence Transformers are often used alongside sparse retrievers.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#9-sentence-transformers-vs-sparse-retrieval","title":"9. Sentence Transformers vs Sparse Retrieval","text":"Aspect Sparse (BM25, SPLADE) Sentence Transformers Representation Sparse Dense Semantic matching Limited Strong Interpretability High Low Exact match Strong Weak Infrastructure Inverted index Vector index <p>Hybrid approaches combine both to get the best of both worlds.</p>"},{"location":"RAG/supporting_topics/sentence_transformers/#interview-perspective","title":"Interview Perspective","text":"<p>For interviews, focus on:</p> <ul> <li>How pooling creates sentence level embeddings</li> <li>Why contrastive learning is used</li> <li>Tradeoffs between dense and sparse retrieval</li> <li>Failure modes of dense retrievers</li> <li>Their role in RAG and hybrid retrieval systems</li> </ul> <p>This demonstrates practical understanding of modern retrieval architectures.</p>"},{"location":"RAG/supporting_topics/splade/","title":"Splade","text":""},{"location":"RAG/supporting_topics/splade/#1-overview","title":"1. Overview","text":"<p>SPLADE (Sparse Lexical and Expansion Model) is a sparse neural retrieval model that combines the strengths of traditional lexical methods like BM25 with the semantic generalization of neural models. It produces sparse, interpretable representations while enabling semantic term expansion using pretrained language models.</p> <p>SPLADE is especially popular in modern retrieval and RAG systems as a drop in replacement or complement to BM25.</p>"},{"location":"RAG/supporting_topics/splade/#2-core-idea-behind-splade","title":"2. Core Idea Behind SPLADE","text":"<p>Classical sparse retrievers rely on exact term overlap. Dense retrievers capture semantics but lose interpretability and efficiency of inverted indexes.</p> <p>SPLADE bridges this gap by:</p> <ul> <li>Using a transformer encoder to generate sparse vectors</li> <li>Expanding queries and documents with semantically related terms</li> <li>Keeping representations compatible with inverted indexes</li> </ul> <p>Each document and query is represented as a weighted bag of vocabulary terms, where weights are learned rather than hand engineered.</p>"},{"location":"RAG/supporting_topics/splade/#3-how-splade-works","title":"3. How SPLADE Works","text":""},{"location":"RAG/supporting_topics/splade/#step-1-input-text","title":"Step 1: Input Text","text":"<p>SPLADE processes text in the same way as standard transformer models.</p> <p>Example document: \"Neural retrieval models improve search quality\"</p> <p>Example query: \"better search models\"</p> <p>Both documents and queries go through the same pipeline.</p>"},{"location":"RAG/supporting_topics/splade/#step-2-transformer-encoding","title":"Step 2: Transformer Encoding","text":"<p>The input text is passed through a masked language model such as BERT.</p> <p>The model outputs contextualized embeddings for each token. Unlike dense retrieval, SPLADE does not pool these embeddings into a single dense vector.</p> <p>Instead, it projects them into the vocabulary space.</p>"},{"location":"RAG/supporting_topics/splade/#step-3-vocabulary-level-scoring","title":"Step 3: Vocabulary Level Scoring","text":"<p>For each token position, the model produces a score for every term in the vocabulary.</p> <p>Conceptually, the model answers:</p> <p>Which vocabulary terms are relevant to this text, even if they do not appear explicitly?</p> <p>Example high scoring terms for the document might be: neural, retrieval, search, information, ranking, models</p> <p>This is where semantic expansion happens.</p>"},{"location":"RAG/supporting_topics/splade/#step-4-non-linearity-and-sparsification","title":"Step 4: Non Linearity and Sparsification","text":"<p>Raw scores are transformed using:</p> <ul> <li>ReLU to remove negative values</li> <li>Log scaling to control large activations</li> </ul> <p>Then, max pooling across token positions is applied.</p> <p>This results in: - One score per vocabulary term - Many terms having zero weight</p> <p>Only the most relevant terms remain active.</p>"},{"location":"RAG/supporting_topics/splade/#step-5-sparse-vector-representation","title":"Step 5: Sparse Vector Representation","text":"<p>The final output is a sparse vector over the vocabulary.</p> <p>Example representation: { search: 2.1, retrieval: 1.8, neural: 1.5, ranking: 0.9 }</p> <p>This representation:</p> <ul> <li>Is sparse like BM25</li> <li>Encodes semantics like neural models</li> <li>Is interpretable as weighted terms</li> </ul>"},{"location":"RAG/supporting_topics/splade/#step-6-indexing-with-inverted-index","title":"Step 6: Indexing with Inverted Index","text":"<p>Each non-zero term is added to an inverted index.</p> <p>For each term, the index stores:</p> <ul> <li>Document IDs</li> <li>Term weights</li> </ul> <p>This allows fast retrieval using standard sparse indexing infrastructure.</p>"},{"location":"RAG/supporting_topics/splade/#step-7-query-processing","title":"Step 7: Query Processing","text":"<p>Queries go through the exact same steps:</p> <ul> <li>Transformer encoding</li> <li>Vocabulary scoring</li> <li>Sparsification</li> <li>Sparse vector creation</li> </ul> <p>Example query expansion:</p> <p>\"better search models\" \u2192 {search, retrieval, ranking, information}</p>"},{"location":"RAG/supporting_topics/splade/#step-8-retrieval-and-scoring","title":"Step 8: Retrieval and Scoring","text":"<p>Document relevance is computed using a dot product between sparse query and document vectors.</p> <p>Only overlapping non zero terms contribute to the score, making retrieval efficient.</p> <p>Documents with strong lexical or semantic overlap score highest.</p>"},{"location":"RAG/supporting_topics/splade/#why-this-works-better-than-bm25","title":"Why This Works Better Than BM25","text":""},{"location":"RAG/supporting_topics/splade/#learned-term-weights","title":"Learned Term Weights","text":"<p>BM25 uses hand crafted formulas. SPLADE learns which terms matter directly from data.</p>"},{"location":"RAG/supporting_topics/splade/#semantic-expansion-without-dense-vectors","title":"Semantic Expansion Without Dense Vectors","text":"<p>SPLADE can match:</p> <p>query: \"car repair\" document: \"automobile maintenance\"</p> <p>Even without exact word overlap.</p>"},{"location":"RAG/supporting_topics/splade/#sparsity-preserves-efficiency","title":"Sparsity Preserves Efficiency","text":"<p>Despite using transformers:</p> <ul> <li>Storage remains sparse</li> <li>Retrieval uses inverted indexes</li> <li>Latency stays manageable</li> </ul>"},{"location":"RAG/supporting_topics/splade/#mental-model-for-interviews","title":"Mental Model for Interviews","text":"<p>Think of SPLADE as:</p> <p>BM25 where the term weights and expansions are learned by a language model instead of being manually designed.</p>"},{"location":"RAG/supporting_topics/splade/#4-splade-scoring-function","title":"4. SPLADE Scoring Function","text":"<p>For a document or query, SPLADE computes:</p> <p>Score(t) = max over positions of log(1 + ReLU(logit(t)))</p> <p>Key properties:</p> <ul> <li>ReLU enforces non negativity</li> <li>Log scaling controls large activations</li> <li>Max pooling encourages sparse activation</li> </ul> <p>Only a small subset of vocabulary terms receive non-zero weights.</p>"},{"location":"RAG/supporting_topics/splade/#5-learned-term-expansion","title":"5. Learned Term Expansion","text":"<p>Unlike BM25 or TF IDF, SPLADE can assign weight to terms that do not explicitly appear in the text.</p> <p>Example:</p> <ul> <li>Query: \"car repair\"</li> <li>Expanded terms: \"automobile\", \"mechanic\", \"engine\"</li> </ul> <p>This improves recall while preserving lexical matching behavior.</p>"},{"location":"RAG/supporting_topics/splade/#6-where-splade-is-used","title":"6. Where SPLADE Is Used","text":"<p>SPLADE is commonly used in:</p> <ul> <li>Neural information retrieval</li> <li>Hybrid search systems</li> <li>RAG first stage retrieval</li> <li>Enterprise and domain specific search</li> <li>Large scale retrieval where dense vectors are costly</li> </ul> <p>It is especially useful when inverted index compatibility is required.</p>"},{"location":"RAG/supporting_topics/splade/#7-strengths-of-splade","title":"7. Strengths of SPLADE","text":"<ul> <li>Sparse and interpretable representations</li> <li>Semantic expansion improves recall</li> <li>Compatible with inverted indexes</li> <li>Strong performance compared to BM25</li> <li>Lower storage cost than dense embeddings</li> </ul> <p>SPLADE often outperforms BM25 without sacrificing efficiency.</p>"},{"location":"RAG/supporting_topics/splade/#8-limitations-and-caveats","title":"8. Limitations and Caveats","text":"<ul> <li>Requires pretrained transformer models</li> <li>Higher indexing cost than BM25</li> <li>Vocabulary size affects memory usage</li> <li>Inference is slower than classical sparse methods</li> <li>Requires careful regularization to control sparsity</li> </ul> <p>Despite sparsity, SPLADE is still more expensive than purely lexical methods.</p>"},{"location":"RAG/supporting_topics/splade/#9-practical-considerations","title":"9. Practical Considerations","text":"<ul> <li>Regularization is critical to maintain sparsity</li> <li>Index size depends on vocabulary and pruning thresholds</li> <li>Query time expansion increases recall but adds latency</li> <li>Often combined with BM25 or dense retrievers</li> <li>Fine-tuning on domain data improves performance</li> </ul> <p>SPLADE is typically deployed where quality gains justify added complexity.</p>"},{"location":"RAG/supporting_topics/splade/#10-splade-vs-bm25","title":"10. SPLADE vs BM25","text":"Aspect BM25 SPLADE Term weighting Hand crafted Learned Semantic expansion No Yes Sparsity Sparse Sparse Interpretability High Moderate Retrieval quality Strong Stronger <p>SPLADE can be viewed as a neural generalization of BM25.</p>"},{"location":"RAG/supporting_topics/splade/#11-splade-in-modern-rag-pipelines","title":"11. SPLADE in Modern RAG Pipelines","text":"<p>SPLADE is often used as:</p> <ul> <li>A semantic sparse retriever</li> <li>A complement to dense embeddings</li> <li>A first stage retriever before reranking</li> <li>A fallback when dense retrieval misses lexical matches</li> </ul> <p>Hybrid systems frequently combine SPLADE, BM25, and dense retrieval.</p>"},{"location":"RAG/supporting_topics/splade/#12-interview-perspective","title":"12. Interview Perspective","text":"<p>For interviews, emphasize:</p> <ul> <li>Why SPLADE is still sparse despite being neural</li> <li>How learned term expansion improves recall</li> <li>Compatibility with inverted indexes</li> <li>Tradeoffs between BM25, SPLADE, and dense retrieval</li> <li>Its role in modern RAG architectures</li> </ul>"},{"location":"RAG/supporting_topics/tf_idf/","title":"TF_IDF","text":""},{"location":"RAG/supporting_topics/tf_idf/#1-overview","title":"1. Overview","text":"<p>TF IDF (Term Frequency\u2013Inverse Document Frequency) is a classical text representation technique used to convert unstructured text into numerical features. It is widely used in information retrieval and traditional NLP pipelines because of its simplicity, interpretability, and strong lexical matching performance.</p> <p>At a high level, TF IDF assigns higher importance to words that are frequent in a document but rare across the entire corpus. This helps highlight terms that best characterize a document while suppressing common, non informative words.</p>"},{"location":"RAG/supporting_topics/tf_idf/#2-intuition-behind-tf-idf","title":"2. Intuition Behind TF IDF","text":"<p>Not all words in a document are equally useful. Words like \"the\", \"is\", or \"and\" appear frequently but contribute little to meaning. In contrast, domain specific terms often appear repeatedly within a document but infrequently across the corpus.</p> <p>TF IDF captures this intuition by combining two signals: - How important a word is within a document - How unique that word is across documents</p> <p>Only terms that satisfy both conditions receive high scores.</p>"},{"location":"RAG/supporting_topics/tf_idf/#3-term-frequency-tf","title":"3. Term Frequency (TF)","text":"<p>Term Frequency measures how often a term appears in a document.</p> <p>A common formulation is:</p> <p>TF(t, d) = count(t in d) / total number of terms in d</p> <p>TF increases with repeated occurrence of a term in the same document, reflecting its local importance. Variants such as logarithmic TF are sometimes used to reduce the impact of very frequent terms.</p>"},{"location":"RAG/supporting_topics/tf_idf/#4-inverse-document-frequency-idf","title":"4. Inverse Document Frequency (IDF)","text":"<p>Inverse Document Frequency measures how rare a term is across the corpus.</p> <p>A common formulation is:</p> \\[IDF(t) = log(N / (1 + df(t)))\\] <p>Where:</p> <ul> <li>\\(N\\) is the total number of documents</li> <li>\\(df(t)\\) is the number of documents containing the term</li> </ul> <p>Rare terms receive higher IDF values, while common terms receive lower values. Smoothing is often applied to avoid division by zero.</p>"},{"location":"RAG/supporting_topics/tf_idf/#5-tf-idf-score","title":"5. TF-IDF Score","text":"<p>The final TF-IDF score is the product of TF and IDF:</p> <p>TF IDF(t, d) = TF(t, d) \u00d7 IDF(t)</p> <p>A high TF IDF score indicates a term that is both important to the document and discriminative across the corpus.</p>"},{"location":"RAG/supporting_topics/tf_idf/#6-how-tf-idf-is-used","title":"6. How TF-IDF is Used","text":"<p>TF IDF is commonly used in:</p> <ul> <li>Search engines and information retrieval systems</li> <li>Document similarity and clustering using cosine similarity</li> <li>Keyword extraction</li> <li>Text classification as a baseline feature representation</li> <li>Sparse retrieval components in RAG pipelines</li> </ul> <p>Despite the rise of dense embeddings, TF IDF remains a strong baseline and is often combined with neural retrievers.</p>"},{"location":"RAG/supporting_topics/tf_idf/#7-strengths-of-tf-idf","title":"7. Strengths of TF IDF","text":"<ul> <li>Simple and easy to implement</li> <li>Does not require labeled data or training</li> <li>Computationally efficient</li> <li>Produces interpretable feature weights</li> <li>Works well for exact and partial lexical matches</li> </ul> <p>These properties make TF IDF attractive for fast retrieval and baseline systems.</p>"},{"location":"RAG/supporting_topics/tf_idf/#8-limitations-and-caveats","title":"8. Limitations and Caveats","text":"<p>TF IDF has several important limitations:</p> <ul> <li>It ignores word order and syntactic structure</li> <li>It cannot capture semantic similarity or paraphrases</li> <li>Vocabulary size can grow very large</li> <li>Performance depends heavily on preprocessing choices</li> <li>Sparse high dimensional vectors increase memory usage</li> </ul> <p>Because of these limitations, TF IDF performs poorly when semantic understanding is required.</p>"},{"location":"RAG/supporting_topics/tf_idf/#9-practical-considerations","title":"9. Practical Considerations","text":"<ul> <li>Stopword removal and normalization significantly affect performance</li> <li>Cosine similarity is preferred over raw dot product</li> <li>IDF smoothing improves robustness</li> <li>Stemming or lemmatization can reduce sparsity</li> <li>TF-IDF is sensitive to spelling and tokenization errors</li> </ul> <p>Careful preprocessing is often more important than the exact TF-IDF formula.</p>"},{"location":"RAG/supporting_topics/tf_idf/#10-tf-idf-in-modern-systems","title":"10. TF IDF in Modern Systems","text":"<p>In modern retrieval systems, TF IDF is often used as:</p> <ul> <li>A fast first stage retriever</li> <li>A lexical complement to dense embeddings</li> <li>A baseline for evaluating neural retrievers</li> </ul> <p>Hybrid retrieval systems frequently combine TF IDF or BM25 with dense vector search to balance precision and recall.</p>"},{"location":"RAG/supporting_topics/tf_idf/#interview-perspective","title":"Interview Perspective","text":"<p>For interviews, focus on:</p> <ul> <li>The intuition behind IDF and why it matters</li> <li>Why cosine similarity is commonly used</li> <li>When TF-IDF works well and when it fails</li> <li>How it compares to neural embeddings</li> <li>Its role in retrieval augmented generation systems</li> </ul>"},{"location":"context_engineering/ce/","title":"Ce","text":""},{"location":"context_engineering/ce/#paged-attention","title":"Paged Attention","text":""}]}